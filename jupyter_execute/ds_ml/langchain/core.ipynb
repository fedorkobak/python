{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b6ae9c",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "The `langchain_core` package provides tools that are used through the entire LangChain ecosystem. This section considers some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac43b4",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "There are several classes that represent different aspects of prompting with LangChain.\n",
    "\n",
    "| Class Name    | Role           | General Description                                                                                                                                      |\n",
    "|---------------|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| SystemMessage | System         | Provides instructions or context to \"prime\" the model's behavior. It sets the persona, tone, or rules for the entire conversation. Typically the first message in a list. |\n",
    "| HumanMessage  | Human          | Represents the user's input. This is the message that a human sends to the model to ask a question or provide a command.                                  |\n",
    "| AIMessage     | AI (Assistant) | Represents the response from the language model. This is the output you get after invoking a model. It can contain text, tool calls, or other data.       |\n",
    "| ToolMessage   | Tool           | Represents the output or result of a tool function that was invoked by the AI. This is used to pass the outcome of a tool call back to the model for further processing. |\n",
    "\n",
    "The primary design of LangChain is to pass a list of objects to the model. It returns an output of type `AIMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a41f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "All LangChain messages are children of the `langchain_core.messages.BaseMessage` class. The  follwing cell shows the relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbb65fa",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "\n",
    "(\n",
    "    issubclass(HumanMessage, BaseMessage),\n",
    "    issubclass(SystemMessage, BaseMessage),\n",
    "    issubclass(AIMessage, BaseMessage),\n",
    "    issubclass(ToolMessage, BaseMessage)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc158e87-7eae-4fbc-b83a-d929dad253de",
   "metadata": {},
   "source": [
    "### Pretty print\n",
    "\n",
    "The messages have a `pretty_print` method that prints them in a special format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8384e-c5cb-4d09-9589-86399f7f905f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the outputs of the `pretty_print` method for a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328b9f24-0bfb-4aac-bb84-1d6bd532a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(\"What is the weather in SF\")\n",
    "human_message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fee39e-25bb-4d4b-bfc3-bf69e20a7074",
   "metadata": {},
   "source": [
    "However, its true potential is in using the pretty print for the lists of messages from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef1b69b-da91-437d-a68b-becc45280a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "However, I'm a large language model, I don't have real-time access to current weather conditions\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "messages = [human_message]\n",
    "model = ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    "messages.append(model.invoke(messages))\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297feacd",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "In the LangChain paradigm, a prompt is a structured input for a model. It can include a system message, user input, or messaging history. The `lang_chain` package provides various tools for prompt templating. The following cell lists the most popular classes used for templating and their descriptions.\n",
    "\n",
    "| Class / Function                        | Description                                                                 |\n",
    "|----------------------------------------|-----------------------------------------------------------------------------|\n",
    "| **BasePromptTemplate**                 | Abstract base class for all prompt templates.                               |\n",
    "| **StringPromptTemplate**               | Base class for string-based templates (like f-string).                      |\n",
    "| **PromptTemplate**                     | Core template class for generating prompts with variables. Supports methods like `from_template`, `from_file`, `from_examples`, `format`, `invoke`, `ainvoke`, and batching. |\n",
    "| **FewShotPromptTemplate**              | String-based prompt template with few-shot example support.                 |\n",
    "| **FewShotPromptWithTemplates**         | String template variant with embedded few-shot examples.                    |\n",
    "| **PipelinePromptTemplate**             | Combines multiple prompt templates into a pipeline.                         |\n",
    "| **BaseChatPromptTemplate**             | Base class for chat-style prompt templates.                                 |\n",
    "| **ChatPromptTemplate**                 | Template for chat models; build multi-role messages. Supports `from_messages` and dynamic placeholders. |\n",
    "| **AgentScratchPadChatPromptTemplate**  | Specialized chat prompt for agent scratchpad patterns.                      |\n",
    "| **AutoGPTPrompt**                      | Chat prompt variant used in AutoGPT-style workflows.                        |\n",
    "| **BaseMessagePromptTemplate**          | Base for message-level prompt templates.                                    |\n",
    "| **BaseStringMessagePromptTemplate**    | Base class for message templates using string patterns.                     |\n",
    "| **ChatMessagePromptTemplate**          | Generates chat messages (with roles, e.g. system/human/AI) from template strings. |\n",
    "| **HumanMessagePromptTemplate**         | Template specifically for human messages.                                   |\n",
    "| **AIMessagePromptTemplate**            | Template specifically for AI messages.                                      |\n",
    "| **SystemMessagePromptTemplate**        | Template specifically for system messages.                                  |\n",
    "| **MessagesPlaceholder**                | Placeholder to inject dynamic message history into a chat template.         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee86c34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the `PromptTemplate` class. You can use the `from_template` method to create a template. A substitutable pattern is specified by the `{}`. The `format` method of the `PromptTempalate` class returns a string with all substituted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52750b4b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your input is: Hello!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "ans = PromptTemplate.from_template(\"Your input is: {here}\")\n",
    "print(type(ans))\n",
    "ans.format(here=\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94191de7",
   "metadata": {},
   "source": [
    "## Vector stores\n",
    "\n",
    "Langchain integrates with various vector stores. The following table shows a few of them:\n",
    "\n",
    "| Class name                            | Package                                                                     |\n",
    "| ------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| `InMemoryVectorStore`                 | `langchain-core.vectorstores`                                               |\n",
    "| `FAISS`                               | `langchain_community.vectorstores.faiss`                                    |\n",
    "| `PGVector`                            | `langchain-postgres` (`langchain.vectorstores.pgvector`)                    |\n",
    "| `ElasticsearchStore`                  | `langchain-elasticsearch` (`langchain.vectorstores.elasticsearch`)          |\n",
    "| `AzureCosmosDBMongoVCoreVectorSearch` | `langchain-azure-ai` (`langchain.vectorstores.azure_cosmos_db_mongo_vcore`) |\n",
    "| `AzureCosmosDBNoSqlVectorSearch`      | `langchain-azure-ai` (`langchain.vectorstores.azure_cosmos_db_no_sql`)      |\n",
    "| `AzureSearch`                         | `langchain-azure-ai` (`langchain.vectorstores.azuresearch`)                 |\n",
    "| `SQLServer_VectorStore`               | `langchain-sqlserver` (`langchain.vectorstores.sqlserver`)                  |\n",
    "\n",
    "For more details check: \n",
    "\n",
    "- [Vector stores](https://python.langchain.com/docs/integrations/vectorstores/) of the official documentation.\n",
    "- The description [`langchain_core.vectorstores.base.VectorStore`](https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.base.VectorStore.html) which defines interface for the vector stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5689a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the simpliest launch option option `InMemoryVectorStore`, for basic opeartions.\n",
    "\n",
    "In order to initialize the corresponding object, you must first create the embedding object. In this case, we will use `OllamaEmbeddings`, so you're supposed to launch Ollama locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a651e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "vector_store = InMemoryVectorStore(OllamaEmbeddings(model=\"all-minilm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4930115",
   "metadata": {},
   "source": [
    "Use the `add_documents` method to add items to the vector storage. This method takes a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5d464",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5895b10e-af40-4263-b0c6-ff4803bd49a6',\n",
       " '4ed7ff85-4f40-4881-8dae-59158b608c62',\n",
       " 'bac54e30-a7f8-4d7e-b682-d312b29c580c']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents = [\n",
    "    Document(s) for s in [\n",
    "        \"This is dog\",\n",
    "        \"This is cat.\",\n",
    "        \"My car was crased\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43fa11",
   "metadata": {},
   "source": [
    "The `similarity_search` method locates documents that are similar to the provided text. The following cells show some outputs for selected examles to make the outputs easier to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b368a8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5895b10e-af40-4263-b0c6-ff4803bd49a6', metadata={}, page_content='This is dog'),\n",
       " Document(id='4ed7ff85-4f40-4881-8dae-59158b608c62', metadata={}, page_content='This is cat.'),\n",
       " Document(id='bac54e30-a7f8-4d7e-b682-d312b29c580c', metadata={}, page_content='My car was crased')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"This is cow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a5ae6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bac54e30-a7f8-4d7e-b682-d312b29c580c', metadata={}, page_content='My car was crased'),\n",
       " Document(id='5895b10e-af40-4263-b0c6-ff4803bd49a6', metadata={}, page_content='This is dog'),\n",
       " Document(id='4ed7ff85-4f40-4881-8dae-59158b608c62', metadata={}, page_content='This is cat.')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"Accidents sometimes happens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135835f",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "\n",
    "The `as_retriever` function gives you access a special retriever object that can be used for searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bed3e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bac54e30-a7f8-4d7e-b682-d312b29c580c', metadata={}, page_content='My car was crased'),\n",
       " Document(id='4ed7ff85-4f40-4881-8dae-59158b608c62', metadata={}, page_content='This is cat.'),\n",
       " Document(id='5895b10e-af40-4263-b0c6-ff4803bd49a6', metadata={}, page_content='This is dog')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrievier = vector_store.as_retriever(k=1)\n",
    "retrievier.invoke(\"09.11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2bfad",
   "metadata": {},
   "source": [
    "## Output parsers\n",
    "\n",
    "The output parser lets you to specify the desired format for the model's responses. It then parses those outputs from the machine learning model. There are following parsers implemented in langchain now:\n",
    "\n",
    "- `JsonOutputParser`: Parse the output of an LLM call to a JSON object.\n",
    "- `JsonOutputToolsParser`: Parse tools from OpenAI response to JSON format.\n",
    "- `PydanticOtputParser`: Parse the output of an LLM call to the specified instance of the Pydantic model.\n",
    "- `PydanticToolsParser`: Parse tools from OpenAI response to pydantic object. \n",
    "- `BaseOutputParser`: Allows to create child classes with specified custom parsing approach.\n",
    "- `BaseLLMOutputParser`: Abstract base class.\n",
    "\n",
    "For more details, check the [Output parsers](https://reference.langchain.com/python/langchain_core/output_parsers/) official reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b1499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the main features of the output parsers using the example of the `PydanticOutputParser`.\n",
    "\n",
    "Imagine that you need to extract some information about the laptop the client wants to buy. The request may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e40a1b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "request = \"I want to buy the hp-9000, with 8GB of RAM, intel-i8 processor.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d103912",
   "metadata": {},
   "source": [
    "The following cell defines the model's schema. A child of `pydantic.BaseModel` defines the attributes that you want to extracte from the input. The `PydanticOutputParser` instance is initialised to process this format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ad6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    model: str = Field(description=\"The model of the device.\")\n",
    "    ram: int = Field(description=\"RAM of the device in GB.\")\n",
    "    processor: str = Field(description=\"Model of the processor.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MyModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22eb42",
   "metadata": {},
   "source": [
    "The `get_format_instructions` method allows you to get the kind of instruction parser provides to the model. The following cell shows the type of description parser provides to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034fcdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"model\": {\"description\": \"The model of the device.\", \"title\": \"Model\", \"type\": \"string\"}, \"ram\": {\"description\": \"RAM of the device in GB.\", \"title\": \"Ram\", \"type\": \"integer\"}, \"processor\": {\"description\": \"Model of the processor.\", \"title\": \"Processor\", \"type\": \"string\"}}, \"required\": [\"model\", \"ram\", \"processor\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467f2a7",
   "metadata": {},
   "source": [
    "The next cell prepares the request sequence, provides it to the model, and displays the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3998c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user wants to buy an HP-9000 with 8GB RAM and an Intel-i8 processor. I need to extract the model, RAM, and processor from their query.\n",
      "\n",
      "First, the model is mentioned as \"hp-9000\". I should check if that's the exact model name. The user wrote it in lowercase, but the schema might expect a specific format. Maybe it's better to keep it as is unless there's a standard naming convention. But since the example in the schema isn't provided, I'll stick with the given value.\n",
      "\n",
      "Next, the RAM is specified as 8GB. The schema requires RAM as an integer in GB. So 8GB would be 8. That's straightforward. No need for units here, just the number.\n",
      "\n",
      "Then the processor is \"intel-i8\". The user wrote it in lowercase, but maybe the actual model is \"Intel i8\" or \"Intel-i8\". The schema's example might have it as a string, so I'll use \"intel-i8\" as given. Wait, the user wrote \"intel-i8\" with a hyphen. Should I capitalize the 'I'? The schema doesn't specify, so I'll follow the user's input exactly.\n",
      "\n",
      "Now, checking the required fields: model, ram, processor. All three are present. The JSON should have these keys. Let me structure it:\n",
      "\n",
      "{\n",
      "  \"model\": \"hp-9000\",\n",
      "  \"ram\": 8,\n",
      "  \"processor\": \"intel-i8\"\n",
      "}\n",
      "\n",
      "I need to make sure there are no typos. The RAM is an integer, so 8 without quotes. The other fields are strings. Looks good. No extra information needed. The user didn't mention anything else, so this should be the correct extraction.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"model\": \"hp-9000\",\n",
      "  \"ram\": 8,\n",
      "  \"processor\": \"intel-i8\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "system_message = (\n",
    "    \"Your goal is to extract data according to the following pattern.\\n\\n\" +\n",
    "    parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "model = ChatOllama(model=\"qwen3:8b\", temperature=0)\n",
    "\n",
    "answer = model.invoke([\n",
    "    SystemMessage(system_message),\n",
    "    HumanMessage(request)\n",
    "])\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf1644",
   "metadata": {},
   "source": [
    "At the end of the answer, there is parsed information in JSON format. Using the parser's invoke method, you cat retrieve the instance of the Pydantic model that you defined as a format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41220f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(model='hp-9000', ram=8, processor='intel-i8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}