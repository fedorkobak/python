{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcc2e19",
   "metadata": {},
   "source": [
    "# Data sources\n",
    "\n",
    "This section describes the tools implemented in Spark for saving, loading, and versioning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d77d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/22 14:20:40 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/22 14:20:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/22 14:20:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_session = SparkSession.builder.appName(\"Temp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79beb924",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "Spark has a set of methods for saving data frames for future use. Consider the most important ones. For that data frame has an attribute `write` that refers to a set of methods: `parquiet`, `csv`, and `json`; with really transparent naming.\n",
    "\n",
    "There is also a `saveAsTable` method, that saves the data to the special storage managed by the Spark. This storage could be: `Hive Metastore` or `DeltaLake`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a7378",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider saving the simple table as a JSON table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 35)],\n",
    "    schema=[\"Name\", \"Age\"]\n",
    ")\n",
    "\n",
    "data_path = \"/tmp/my_data\"\n",
    "df.write.json(data_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3cf9d",
   "metadata": {},
   "source": [
    "A set of `.json` files is created in the destination folder to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f73168b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00011-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " '.part-00000-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " '.part-00005-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " '.part-00011-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " 'part-00017-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " '.part-00017-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " 'part-00005-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff185de8",
   "metadata": {},
   "source": [
    "Consider important for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1848d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Bob', 'Age': 30}\n",
      "{'Name': 'Cathy', 'Age': 35}\n",
      "{'Name': 'Alice', 'Age': 25}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(data_path + \"/\" + file_name, \"r\") as f:\n",
    "            try:\n",
    "                print(json.load(f))\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e687f",
   "metadata": {},
   "source": [
    "## Read csv\n",
    "\n",
    "Use the `read.csv` method of the spark session to read a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a118f4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell reads the `spark.csv` file that I prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be621c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string,  Age: double,  Salary: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark_session.read.csv(\n",
    "    \"data_sources_files/scv_example.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    multiLine=True,\n",
    "    escape=','\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec059c",
   "metadata": {},
   "source": [
    "### Shcema\n",
    "\n",
    "Use the `schema` argument to define the schema. The schema can be specified as a simple string that matches column names with their expected data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e8043",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the matching of the `int` data type to the `Age` column instead of the default `double` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a860fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Salary: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"\"\"\n",
    "Name string,\n",
    "Age int,\n",
    "Salary double\n",
    "\"\"\"\n",
    "\n",
    "spark_session.read.csv(\n",
    "    \"data_sources_files/scv_example.csv\",\n",
    "    schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0d187",
   "metadata": {},
   "source": [
    "## SQL catalog\n",
    "\n",
    "The *Spark SQL catalog* is a special file system that provies SQL access and data is described by a special metadata provided by PySpark. This section demonstrates how to access the capabilitites of the SQL catalog from the python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252efda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The SQL catalog is specified by the `spark.sql.warehouse.dir` attribute in the Spark configuration. The following cell displays the SQL catalog for the current Spark session.\n",
    "\n",
    "**Note** It should be specified when creating of the session.\n",
    "\n",
    "The following cell recreates the session with required configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cabdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file:/tmp/spark-warehouse'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SparkSession.getActiveSession() is not None:\n",
    "    spark_session.stop()\n",
    "\n",
    "spark_session = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark_session.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0a7e1",
   "metadata": {},
   "source": [
    "The following cell uses the `write.saveAsTable` method to store the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 35)],\n",
    "    schema=[\"Name\", \"Age\"]\n",
    ")\n",
    "df.write.saveAsTable(\"example_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bb289",
   "metadata": {},
   "source": [
    "The corresponding folder should now be in the warehouse storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab76c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_save\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/spark-warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8a5c6",
   "metadata": {},
   "source": [
    "This folder contains the partitions of the saved dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd44941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "part-00005-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "part-00011-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "part-00017-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/spark-warehouse/example_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fab91c",
   "metadata": {},
   "source": [
    "You can now access the saved using the SQL syntax provided by the `sql` method of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f0ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Cathy| 35|\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_session.sql(\"SELECT * FROM example_save;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a8a7f",
   "metadata": {},
   "source": [
    "### Delta lake\n",
    "\n",
    "You can specify the delta lake to be a SQL catalog for your spark session you need:\n",
    "\n",
    "- Install `delta-lake` python package.\n",
    "- Create session from builded by using `delta.configure_sparak_with_delta_pip`.\n",
    "- Set parameters:\n",
    "    - `spark.sql.extensions` to the `io.delta.sql.DeltaSparkSessionExtension`.\n",
    "    - `spark.sql.catalog.spark_catalog` to the `org.apache.spark.sql.delta.catalog.DeltaCatalog`.\n",
    "\n",
    "Delta Lake only provides just a way to store information, so all the other parameters that configure the SQL catalog are still valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d0067",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell build session which configuration includes the delta lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c610f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/22 15:14:05 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/22 15:14:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/user/.virtualenvironments/python/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/user/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/user/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8cbeb2d-5543-4026-b5b3-d384e23d527e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 176ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8cbeb2d-5543-4026-b5b3-d384e23d527e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/09/22 15:14:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DeltaLocalExample\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3dc97",
   "metadata": {},
   "source": [
    "The following cell creates a table in the delta format, allowing it to take advantage of delta lake benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028b1540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 15:15:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    data=[\n",
    "        (1, 2),\n",
    "        (4, 1),\n",
    "        (2, 3)\n",
    "    ],\n",
    "    schema=[\"val1\", \"val2\"]\n",
    ")\n",
    "df.write.format('delta').saveAsTable(\"version_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e40f7",
   "metadata": {},
   "source": [
    "One important feature of the delta lake is its ability to version data. The following cell adds a new column and saves the resulting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9381b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .withColumn(\"val3\", df[\"val1\"] + df[\"val2\"])\n",
    "    .write.format(\"delta\").mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(\"version_example\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf926c",
   "metadata": {},
   "source": [
    "Using the sql command `DESCRIBE HISTORY <table_name>;` allows you to load the table's change log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da23cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2025-09-22 15:15:...|  NULL|    NULL|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      0|2025-09-22 15:15:...|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY version_example;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b3fe3",
   "metadata": {},
   "source": [
    "To retrieve the corresponding data version as a data frame, specify the desired version you want to get in the `versionAsOf` option of the `read` constructor. The two following cells load both versions created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1dcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|val1|val2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   2|   3|\n",
      "|   4|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"version_example\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af9ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|val1|val2|val3|\n",
      "+----+----+----+\n",
      "|   2|   3|   5|\n",
      "|   1|   2|   3|\n",
      "|   4|   1|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).table(\"version_example\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}