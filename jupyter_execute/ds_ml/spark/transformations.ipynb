{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf329d52",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "This section explores a variety of tools for transforming data frames in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb26fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/23 13:43:41 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/23 13:43:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/23 13:43:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/23 13:43:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_session = SparkSession.builder.appName(\"Temp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680916c3",
   "metadata": {},
   "source": [
    "## With column\n",
    "\n",
    "\n",
    "The dataframe object provides a `withColumn` method to operate with columns. You are supposed to provide:\n",
    "- The name of the column in which the result should be srored. If the column doesn't exists, it will be created in output dataframe.\n",
    "- The column object or computational expression that defines the new column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef168cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates the data frame that we will use for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035928cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|numbers|strings|\n",
      "+-------+-------+\n",
      "|      8| value1|\n",
      "|      9| value2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (8, \"value1\"),\n",
    "        (9, \"value2\")\n",
    "    ],\n",
    "    schema=[\"numbers\", \"strings\"]\n",
    ")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ae303",
   "metadata": {},
   "source": [
    "The following code modifies the example data frame by using `withColumn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaaa1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|numbers|strings|\n",
      "+-------+-------+\n",
      "|     98| value1|\n",
      "|     99| value2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.withColumn(\n",
    "    \"numbers\",\n",
    "    col(\"numbers\") + 90\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae05df",
   "metadata": {},
   "source": [
    "## Select expression\n",
    "\n",
    "The Spark DataFrame has a `selectExpr` method that allows you to build a new data frame just by specifying columns of the result data using SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6e391",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines and displays the data frame that will be used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5dc391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|score|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|   10|\n",
      "|  2|    Bob|   20|\n",
      "|  3|Charlie|   30|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [(1, \"Alice\", 10), (2, \"Bob\", 20), (3, \"Charlie\", 30)],\n",
    "    [\"id\", \"name\", \"score\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25781f6",
   "metadata": {},
   "source": [
    "The following cell demonstrates the use of the `selectExpr` method in the following patterns: performing multiplication on a constand, producing a boolean value, and performing an operation on two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+-------------+------------+\n",
      "| id|   name|double_score|is_high_score|(id + score)|\n",
      "+---+-------+------------+-------------+------------+\n",
      "|  1|  Alice|          20|        false|          11|\n",
      "|  2|    Bob|          40|         true|          22|\n",
      "|  3|Charlie|          60|         true|          33|\n",
      "+---+-------+------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"score * 2 as double_score\",\n",
    "    \"score > 15 as is_high_score\",\n",
    "    \"id + score\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d700a96",
   "metadata": {},
   "source": [
    "## Nan values\n",
    "\n",
    "The Spark DataFrame contains an `na` attribute that provides access to the methods associated with processing missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3d37b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the dataset that we will use as an example when dealing with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d3afd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|  _1|  _2|  _3|\n",
      "+----+----+----+\n",
      "|  20|  30|NULL|\n",
      "|NULL|NULL|NULL|\n",
      "|  43|NULL|NULL|\n",
      "|  58|  30|  12|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (20, 30, None),\n",
    "        (None, None, None),\n",
    "        (43, None, None),\n",
    "        (58, 30, 12)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cca48a",
   "metadata": {},
   "source": [
    "### Drop\n",
    "\n",
    "The `na.drop` method removes rows that contain a specified number of empty values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff06b62",
   "metadata": {},
   "source": [
    "The following cell illustrates the purpose of the `how='any'` argument, which replaces all rows with at least one missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84ec32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| _1| _2| _3|\n",
      "+---+---+---+\n",
      "| 58| 30| 12|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45870a",
   "metadata": {},
   "source": [
    "In case `how='all'`, only the row cantaining all missing values are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5745386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| _1|  _2|  _3|\n",
      "+---+----+----+\n",
      "| 20|  30|NULL|\n",
      "| 43|NULL|NULL|\n",
      "| 58|  30|  12|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23ce40",
   "metadata": {},
   "source": [
    "The `thresh=2` instructs the method to ignore the value of the `how` parameter, and drop all rows with 2 or more empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae2f51ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "| _1| _2|  _3|\n",
      "+---+---+----+\n",
      "| 20| 30|NULL|\n",
      "| 58| 30|  12|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\", thresh=2).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}