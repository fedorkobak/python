{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f10b03",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "This page discusses various aspects of the Hugging Face infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5942",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "`transformers` is python package that allows you to use pre-trained machine learning models that belong to the transformers architecture.\n",
    "\n",
    "| Component                   | Description                                                                                 |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Models**                  | Pretrained architectures for tasks like classification, generation, or embeddings.          |\n",
    "| **Tokenizers**              | Convert text into numerical input for models; handle batching, padding, truncation.         |\n",
    "| **Pipelines**               | High-level API combining tokenizer + model for a specific task (e.g., `summarization`).     |\n",
    "| **Configurations**          | Define model hyperparameters and architecture settings (e.g., `BertConfig`).                |\n",
    "| **Trainer**                 | High-level training API handling loops, evaluation, logging, and checkpointing.             |\n",
    "| **Schedulers & Optimizers** | Learning rate schedulers and optimizer integrations for training models.                    |\n",
    "| **Data Utilities**          | Helpers for preprocessing and batching (e.g., `DataCollator`, `BatchEncoding`).             |\n",
    "| **Hub Integration**         | Download/upload pretrained models from Hugging Face Hub (`from_pretrained`, `push_to_hub`). |\n",
    "\n",
    "Check more details in the [transformers](hugging_face/transformers.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6b3a5",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "A package that implements different tokenization approaches and related tools.\n",
    "\n",
    "| Component          | Description                                                                       |\n",
    "| ------------------ | --------------------------------------------------------------------------------- |\n",
    "| **PreTokenizers**  | Split text into initial units (words, punctuation, subwords) before encoding.     |\n",
    "| **Models**         | Define the algorithm for tokenization (BPE, WordPiece, SentencePiece, Unigram).   |\n",
    "| **Normalizers**    | Clean and standardize text (lowercasing, accent stripping, punctuation handling). |\n",
    "| **Trainers**       | Learn tokenization vocabulary from a dataset.                                     |\n",
    "| **Decoders**       | Convert token IDs back to readable text.                                          |\n",
    "| **Processors**     | Post-process tokenized output (e.g., adding special tokens like `[CLS]`).         |\n",
    "| **Batch Encoding** | Handle batch tokenization with padding, truncation, and attention masks.          |\n",
    "\n",
    "Check:\n",
    "\n",
    "- [Documentation](https://huggingface.co/docs/tokenizers/en/index) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492d4ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the most essential components that are typically used with the `tokenizers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9c11e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06dd30",
   "metadata": {},
   "source": [
    "Most tokenization algorithms require an initial data transformation. The text is first split into sections using a deterministic approach. Then, a fitting procedure is applied to these sections to determine the final set of tokens.\n",
    "\n",
    "The following cell shows application of the pretokinizer to arbitrary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e63613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Some', (0, 4)), ('test', (5, 9)), ('text', (10, 14))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokinizer = Whitespace()\n",
    "pretokinizer.pre_tokenize_str(\"Some test text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00499d2",
   "metadata": {},
   "source": [
    "The `tokenizers.Tokenizer` class, is tool for interacting with a tokenizer. It takes a model that defines the exact approach to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f275cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pretokinizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0ffc4",
   "metadata": {},
   "source": [
    "The trainer class is another component of the whole system, and it defines some parameters. The following cell shows the training of the tokenizer defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(vocab_size=20)\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    [\n",
    "        \"some super check\",\n",
    "        \"super some check\"\n",
    "    ],\n",
    "    trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81352ea3",
   "metadata": {},
   "source": [
    "The following cell shows the vocabulary of the final tokenizer. Each token has an ID that will be used after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3c45e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some': 18,\n",
       " 'per': 16,\n",
       " 'ck': 11,\n",
       " 'h': 2,\n",
       " 'u': 9,\n",
       " 'me': 14,\n",
       " 'o': 5,\n",
       " 'er': 12,\n",
       " 'p': 6,\n",
       " 'r': 7,\n",
       " 'c': 0,\n",
       " 'k': 3,\n",
       " 'ch': 10,\n",
       " 's': 8,\n",
       " 'e': 1,\n",
       " 'check': 19,\n",
       " 'ome': 15,\n",
       " 'm': 4,\n",
       " 'su': 17,\n",
       " 'eck': 13}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c581acb",
   "metadata": {},
   "source": [
    "Here is a result of a transformation for a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "102f44dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'r', 'some', 'check']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"start some check\").tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
