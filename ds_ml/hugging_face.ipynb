{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f10b03",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "This page discusses various aspects of the Hugging Face infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5942",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "`transformers` is python package that allows you to use pre-trained machine learning models that belong to the transformers architecture.\n",
    "\n",
    "| Component                   | Description                                                                                 |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Models**                  | Pretrained architectures for tasks like classification, generation, or embeddings.          |\n",
    "| **Tokenizers**              | Convert text into numerical input for models; handle batching, padding, truncation.         |\n",
    "| **Pipelines**               | High-level API combining tokenizer + model for a specific task (e.g., `summarization`).     |\n",
    "| **Configurations**          | Define model hyperparameters and architecture settings (e.g., `BertConfig`).                |\n",
    "| **Trainer**                 | High-level training API handling loops, evaluation, logging, and checkpointing.             |\n",
    "| **Schedulers & Optimizers** | Learning rate schedulers and optimizer integrations for training models.                    |\n",
    "| **Data Utilities**          | Helpers for preprocessing and batching (e.g., `DataCollator`, `BatchEncoding`).             |\n",
    "| **Hub Integration**         | Download/upload pretrained models from Hugging Face Hub (`from_pretrained`, `push_to_hub`). |\n",
    "\n",
    "Check more details in the [transformers](hugging_face/transformers.ipynb) page."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
