{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f10b03",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "This page discusses various aspects of the Hugging Face infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5942",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "`transformers` is python package that allows you to use pre-trained machine learning models that belong to the transformers architecture.\n",
    "\n",
    "| Component                   | Description                                                                                 |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Models**                  | Pretrained architectures for tasks like classification, generation, or embeddings.          |\n",
    "| **Tokenizers**              | Convert text into numerical input for models; handle batching, padding, truncation.         |\n",
    "| **Pipelines**               | High-level API combining tokenizer + model for a specific task (e.g., `summarization`).     |\n",
    "| **Configurations**          | Define model hyperparameters and architecture settings (e.g., `BertConfig`).                |\n",
    "| **Trainer**                 | High-level training API handling loops, evaluation, logging, and checkpointing.             |\n",
    "| **Schedulers & Optimizers** | Learning rate schedulers and optimizer integrations for training models.                    |\n",
    "| **Data Utilities**          | Helpers for preprocessing and batching (e.g., `DataCollator`, `BatchEncoding`).             |\n",
    "| **Hub Integration**         | Download/upload pretrained models from Hugging Face Hub (`from_pretrained`, `push_to_hub`). |\n",
    "\n",
    "Check more details in the [transformers](hugging_face/transformers.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6b3a5",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "A package that implements different tokenization approaches and related tools.\n",
    "\n",
    "| Component          | Description                                                                       |\n",
    "| ------------------ | --------------------------------------------------------------------------------- |\n",
    "| **PreTokenizers**  | Split text into initial units (words, punctuation, subwords) before encoding.     |\n",
    "| **Models**         | Define the algorithm for tokenization (BPE, WordPiece, SentencePiece, Unigram).   |\n",
    "| **Normalizers**    | Clean and standardize text (lowercasing, accent stripping, punctuation handling). |\n",
    "| **Trainers**       | Learn tokenization vocabulary from a dataset.                                     |\n",
    "| **Decoders**       | Convert token IDs back to readable text.                                          |\n",
    "| **Processors**     | Post-process tokenized output (e.g., adding special tokens like `[CLS]`).         |\n",
    "| **Batch Encoding** | Handle batch tokenization with padding, truncation, and attention masks.          |\n",
    "\n",
    "Check:\n",
    "\n",
    "- [Documentation](https://huggingface.co/docs/tokenizers/en/index) of the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492d4ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the most essential components that are typically used with the `tokenizers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c11e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e63613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Some', (0, 4)), ('test', (5, 9)), ('text', (10, 14))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokinizer = Whitespace()\n",
    "pretokinizer.pre_tokenize_str(\"Some test text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
