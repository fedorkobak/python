{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d73836b",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "\n",
    "This page considers LangChain interfaces for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1270a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "model = ChatOllama(model=\"llama3.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63404c8f-186c-44a3-a277-a394a8272602",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "The `invoke` method triggers the request to LLM.\n",
    "\n",
    "In most cases, it returns an `AIMessage`, but in some special cases, it can return some special output. For example, structured output langchain object return `dict` or `Pydantic.BaseModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860106d-14cd-4faf-b36d-7ef6545eb48b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the kind of object `langchain_core.language_models.BaseChatModel` heir returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98213b78-2163-4e68-b1ae-bdb5df8c43a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2026-01-06T14:39:17.313641499Z', 'done': True, 'done_reason': 'stop', 'total_duration': 23004167206, 'load_duration': 22818871591, 'prompt_eval_count': 12, 'prompt_eval_duration': 27074996, 'eval_count': 10, 'eval_duration': 141396789, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--fe6d8bde-d9c8-4e4c-a7f2-df3839f539a6-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0c115",
   "metadata": {},
   "source": [
    "## Structured ouput\n",
    "\n",
    "Some providers support the structured output. The model will return the data in the specified format.\n",
    "\n",
    "To specify the model to follow the specified format, use the `with_strucutred_ouput` method. It returns the modified chat object that will follow specified rules. \n",
    "\n",
    "Check if the provider supports structured ouput in the JSON mode column of the [provided features](https://docs.langchain.com/oss/python/integrations/chat#featured-providers) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c22e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell illustrates how the the user characteristics are extracted from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3957c81c-c0cd-40e2-a10d-ba1d34a8c123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputSchema(id='777', name='llm_lover')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(OutputSchema)\n",
    "response = structured_model.invoke(\n",
    "    \"Extract data: 'User llm_lover with id 777 tries to acess the database.'\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3364a11-1413-4255-bf45-ace6ada4f4e3",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "There is a set of methods in the `langchain_core.language_models.base.BaseLanguageModel` that allows to **estimate** the number of tokens that a piece of text will take:\n",
    "\n",
    "- `get_token_ids`: returns the indeces of tokens.\n",
    "- `get_num_tokens`: returns the number of tokens for given text. \n",
    "- `get_num_tokens_from_messages`: returns the number of tokens for given list of messages.\n",
    "\n",
    "**Note.** The methods use a special default tokeniser, so the particular model will actually use different tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2d568-7a46-4f55-8f1d-dd69a1d828e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines two models wrapped with ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac8033-52b2-4058-bdef-c7abf08d9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "llama = ChatOllama(model=\"llama3\")\n",
    "\n",
    "test_text = \"This is some tricky text: olala\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504504f-bfc9-448b-8c9a-29015fe344d3",
   "metadata": {},
   "source": [
    "The output of the `get_tokens_ids` method for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75cd9634-633c-4ef3-8fd4-ff8c4b48fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 17198, 2420, 25, 25776, 6081]\n",
      "[1212, 318, 617, 17198, 2420, 25, 25776, 6081]\n"
     ]
    }
   ],
   "source": [
    "print(deepseek.get_token_ids(test_text))\n",
    "print(llama.get_token_ids(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b5829-ae0f-41a0-bb0f-b021753221a4",
   "metadata": {},
   "source": [
    "The outputs are the same because the default tokenizer was used, despite the models use different tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42533783-01c0-49c0-a0b5-e44e59138c54",
   "metadata": {},
   "source": [
    "The following cell show the output of the `get_num_tokens` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "243a253b-3072-4602-bd7f-519d55ff2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek.get_num_tokens(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d7113-b625-4bae-9083-104878b8bee4",
   "metadata": {},
   "source": [
    "And the application of the `get_num_tokens_from_messages` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2ab1f35-278c-48cf-8fe3-ab1676b10df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core import messages\n",
    "\n",
    "deepseek.get_num_tokens_from_messages(\n",
    "    [messages.HumanMessage(test_text)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4185f6a-1f90-44ac-b7a0-bb687d107054",
   "metadata": {},
   "source": [
    "The output is higher because the number includes the service tokens that wrap the messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
