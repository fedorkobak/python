{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dfd5f4-cbd4-4a82-9327-c0046fd087ad",
   "metadata": {},
   "source": [
    "# Messages\n",
    "\n",
    "The native way for LangChain to process messages is to keep them in the special abstractions that define different types of messages. Much of the functionality of LangChain and LangGraph is designed to consume and produce these messages abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d18c52-e03a-4d6b-8307-96920a25616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import langgraph\n",
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a173ff-5c15-4936-92b4-436d7e2cf2b6",
   "metadata": {},
   "source": [
    "## Pretty print\n",
    "\n",
    "The messages have a `pretty_print` method that prints them in a special format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bfcbb-29e8-4133-b7fd-26b8a175e6a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the outputs of the `pretty_print` method for a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2137d1db-1e20-4546-a277-979f4da78500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(\"What is the weather in SF\")\n",
    "human_message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5345ec5-599b-4100-83b7-643eb159dfa2",
   "metadata": {},
   "source": [
    "However, its true potential is in using the pretty print for the lists of messages from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe7101-cd9d-4e8e-9c7a-05368f641a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "However, I'm a large language model, I don't have real-time access to current weather conditions\n"
     ]
    }
   ],
   "source": [
    "messages = [human_message]\n",
    "model = ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    "messages.append(model.invoke(messages))\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae474bb-8222-4f72-a5f1-741dbbc221cd",
   "metadata": {},
   "source": [
    "## Trimming\n",
    "\n",
    "The `langchain_core.messages.trim_messages` allows you to trim the chat history according to the token logic. This leaves only the messages that corespond to the specified number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ee3b4-5769-463f-9071-cc7caeff419c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines some messages and trims them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab203535-f13e-4371-8e2f-83f1a85c94b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='This is too hard question for me!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_core.messages.trim_messages(\n",
    "    [\n",
    "        HumanMessage(\"Hello! What is the capital of France\"),\n",
    "        AIMessage(\"This is too hard question for me!\")\n",
    "    ],\n",
    "    max_tokens=10,\n",
    "    token_counter=ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a88457-6c79-4c0c-a108-71ddbba9682b",
   "metadata": {},
   "source": [
    "## Tool call\n",
    "\n",
    "If the model with the bineded tool decides to call the tool, LangChain stores the infromation about tool call in the `tool_calls` attribute of the `AIMessage` as the element of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42f5d6-9da0-4be6-9295-b492a30707e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell instructs the model to invoke the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef05b27-9c22-4ec8-af5a-793cdd5f8ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2026-01-15T15:01:23.135300205Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2646139260, 'load_duration': 2285408213, 'prompt_eval_count': 155, 'prompt_eval_duration': 85093620, 'eval_count': 17, 'eval_duration': 247002812, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--50cbb232-3afe-415a-8546-5986cb20d389-0', tool_calls=[{'name': 'my_tool', 'args': {'inp': 'hello'}, 'id': '72012362-31cb-485b-bfdc-a9e99fec5955', 'type': 'tool_call'}], usage_metadata={'input_tokens': 155, 'output_tokens': 17, 'total_tokens': 172})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import tools\n",
    "\n",
    "\n",
    "@tools.tool\n",
    "def my_tool(inp: str) -> str:\n",
    "    \"\"\"Always call this tool\"\"\"\n",
    "    return \"result of the tool\"\n",
    "\n",
    "\n",
    "model_with_tools = model.bind_tools([my_tool])\n",
    "ai_message = model_with_tools.invoke(\"Call my_tool('hello')\")\n",
    "ai_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e0c7f-1ff9-44b4-8aa8-b632d2b28b16",
   "metadata": {},
   "source": [
    "As the result, there is a corresponding element in the `tool_calls` attribute of the `AIMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb7c4d6-d303-4c52-8048-a766bfc7f4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'my_tool',\n",
       "  'args': {'inp': 'hello'},\n",
       "  'id': '72012362-31cb-485b-bfdc-a9e99fec5955',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308786e-5af0-48f5-9b18-23eed54e59ff",
   "metadata": {},
   "source": [
    "## Tool message\n",
    "\n",
    "The `ToolMessage` is the output of the `ToolNode` which contains the results of the tool call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8443157-229f-4dce-bcdd-dd723a1e7c8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates the `AIMessage` containing the tool call and passes it to the `ToolNode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d5a5e3-d656-4bc8-b9bb-811cc5e5f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import tools\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "ai_message = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"id\": \"miracle\",\n",
    "            \"name\": \"my_tool\",\n",
    "            \"args\": {\"inp\": \"hello\"},\n",
    "            \"type\": \"tool_call\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0ff46d6-cb81-4c20-a19e-192eb3709647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tools.tool\n",
    "def my_tool(inp: str) -> None:\n",
    "    \"\"\"\n",
    "    The dummy tool.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "graph = (\n",
    "    langgraph.graph.StateGraph(MessagesState)\n",
    "    .add_node(\"tool\", langgraph.prebuilt.ToolNode([my_tool]))\n",
    "    .add_edge(\"__start__\", \"tool\")\n",
    "    .add_edge(\"tool\", \"__end__\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "out = graph.invoke(\n",
    "    MessagesState(messages=[ai_message])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa2629-c120-4947-8f01-946f778140e9",
   "metadata": {},
   "source": [
    "The following cell displays the `ToolMessage` that is the result of the tool node invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57dfbf5d-faa8-4b3d-854e-583835f319cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='null', name='my_tool', id='a881dd41-9d83-4af0-bb67-650a2b2abcf5', tool_call_id='miracle')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4786e17-af3f-444b-a8f3-a9209a2d3604",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "The `content` attribute of the `ToolMessage` contains information about the steps that the tool is supposed to contribute to the execution process.\n",
    "\n",
    "**Note.** The `content` attribute always contains `str` type. If you want to provide a formal object from the tool use the `artifact` attribute of the `ToolMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa7edd-c1f7-4ccf-b025-5a090ad6521b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "By default, the `ToolNode` creates `content` from the output of the function bound as a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c6bea1b-e2fd-4aed-a55f-b70b0b130ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"The\", \"content\"]'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tools.tool\n",
    "def my_tool(inp: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    The dummy tool.\n",
    "    \"\"\"\n",
    "    return [\"The\", \"content\"]\n",
    "\n",
    "\n",
    "graph = (\n",
    "    langgraph.graph.StateGraph(MessagesState)\n",
    "    .add_node(\"tool\", langgraph.prebuilt.ToolNode([my_tool]))\n",
    "    .add_edge(\"__start__\", \"tool\")\n",
    "    .add_edge(\"tool\", \"__end__\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "out = graph.invoke(MessagesState(messages=[ai_message]))\n",
    "out[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118fcf3-5ca8-4d64-a9b4-1a645094271a",
   "metadata": {},
   "source": [
    "**Note** the `content` is the `str` instance descpite the fact that `my_tool` returned data as the `list[str]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0a3e5-8bf3-4eed-9621-be0f9284a9d2",
   "metadata": {},
   "source": [
    "## OpenAI format\n",
    "\n",
    "Many tools associated with LLMs accept a sequence of messages in the OpenAI format. To convert LangChain messages to this format, use the `langchain_community.adapters.openai.convert_message_to_dict` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c914b6-38bc-4186-83dc-4a38e088cad5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell converts some LangChain message objects into the dictionary format expected by OpenAI-compatible tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe73070-c67b-4feb-a9ef-350a69dade7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Human message'},\n",
       " {'role': 'system', 'content': 'System message'},\n",
       " {'role': 'assistant', 'content': 'AIMessage'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.adapters.openai import convert_message_to_dict\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"Human message\"),\n",
    "    SystemMessage(\"System message\"),\n",
    "    AIMessage(\"AIMessage\"),\n",
    "]\n",
    "\n",
    "[\n",
    "    convert_message_to_dict(message)\n",
    "    for message in messages\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
