{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dfd5f4-cbd4-4a82-9327-c0046fd087ad",
   "metadata": {},
   "source": [
    "# Messages\n",
    "\n",
    "The native way for LangChain to process messages is to keep them in the special abstractions that define different types of messages. Much of the functionality of LangChain and LangGraph is designed to consume and produce these messages abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d18c52-e03a-4d6b-8307-96920a25616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langgraph\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a173ff-5c15-4936-92b4-436d7e2cf2b6",
   "metadata": {},
   "source": [
    "## Pretty print\n",
    "\n",
    "The messages have a `pretty_print` method that prints them in a special format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bfcbb-29e8-4133-b7fd-26b8a175e6a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the outputs of the `pretty_print` method for a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2137d1db-1e20-4546-a277-979f4da78500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(\"What is the weather in SF\")\n",
    "human_message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5345ec5-599b-4100-83b7-643eb159dfa2",
   "metadata": {},
   "source": [
    "However, its true potential is in using the pretty print for the lists of messages from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe7101-cd9d-4e8e-9c7a-05368f641a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "However, I'm a large language model, I don't have real-time access to current weather conditions\n"
     ]
    }
   ],
   "source": [
    "messages = [human_message]\n",
    "model = ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    "messages.append(model.invoke(messages))\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae474bb-8222-4f72-a5f1-741dbbc221cd",
   "metadata": {},
   "source": [
    "## Trimming\n",
    "\n",
    "The `langchain_core.messages.trim_messages` allows you to trim the chat history according to the token logic. This leaves only the messages that corespond to the specified number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ee3b4-5769-463f-9071-cc7caeff419c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines some messages and trims them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab203535-f13e-4371-8e2f-83f1a85c94b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='This is too hard question for me!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_core.messages.trim_messages(\n",
    "    [\n",
    "        HumanMessage(\"Hello! What is the capital of France\"),\n",
    "        AIMessage(\"This is too hard question for me!\")\n",
    "    ],\n",
    "    max_tokens=10,\n",
    "    token_counter=ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a88457-6c79-4c0c-a108-71ddbba9682b",
   "metadata": {},
   "source": [
    "## Tool call\n",
    "\n",
    "If the model with the bineded tool decides to call the tool, LangChain stores the infromation about tool call in the `tool_calls` attribute of the `AIMessage` as the element of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42f5d6-9da0-4be6-9295-b492a30707e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell instructs the model to invoke the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef05b27-9c22-4ec8-af5a-793cdd5f8ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2026-01-15T15:01:23.135300205Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2646139260, 'load_duration': 2285408213, 'prompt_eval_count': 155, 'prompt_eval_duration': 85093620, 'eval_count': 17, 'eval_duration': 247002812, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--50cbb232-3afe-415a-8546-5986cb20d389-0', tool_calls=[{'name': 'my_tool', 'args': {'inp': 'hello'}, 'id': '72012362-31cb-485b-bfdc-a9e99fec5955', 'type': 'tool_call'}], usage_metadata={'input_tokens': 155, 'output_tokens': 17, 'total_tokens': 172})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import tools\n",
    "\n",
    "\n",
    "@tools.tool\n",
    "def my_tool(inp: str) -> str:\n",
    "    \"\"\"Always call this tool\"\"\"\n",
    "    return \"result of the tool\"\n",
    "\n",
    "\n",
    "model_with_tools = model.bind_tools([my_tool])\n",
    "ai_message = model_with_tools.invoke(\"Call my_tool('hello')\")\n",
    "ai_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e0c7f-1ff9-44b4-8aa8-b632d2b28b16",
   "metadata": {},
   "source": [
    "As the result, there is a corresponding element in the `tool_calls` attribute of the `AIMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb7c4d6-d303-4c52-8048-a766bfc7f4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'my_tool',\n",
       "  'args': {'inp': 'hello'},\n",
       "  'id': '72012362-31cb-485b-bfdc-a9e99fec5955',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308786e-5af0-48f5-9b18-23eed54e59ff",
   "metadata": {},
   "source": [
    "## Tool message\n",
    "\n",
    "The `ToolMessage` is the output of the `ToolNode` which contains the results of the tool call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8443157-229f-4dce-bcdd-dd723a1e7c8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates the `AIMessage` containing the tool call and passes it to the `ToolNode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ff46d6-cb81-4c20-a19e-192eb3709647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool is executed\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "ai_message = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"id\": \"miracle\",\n",
    "            \"name\": \"my_tool\",\n",
    "            \"args\": {\"inp\": \"hello\"},\n",
    "            \"type\": \"tool_call\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from langchain import tools\n",
    "\n",
    "\n",
    "@tools.tool\n",
    "def my_tool(inp: str) -> None:\n",
    "    \"\"\"\n",
    "    The dummy tool.\n",
    "    \"\"\"\n",
    "    print(\"Tool is executed\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list[BaseMessage]\n",
    "\n",
    "\n",
    "graph = (\n",
    "    langgraph.graph.StateGraph(State)\n",
    "    .add_node(\"tool\", langgraph.prebuilt.ToolNode([my_tool]))\n",
    "    .add_edge(\"__start__\", \"tool\")\n",
    "    .add_edge(\"tool\", \"__end__\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "out = graph.invoke(\n",
    "    State(messages=[ai_message])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa2629-c120-4947-8f01-946f778140e9",
   "metadata": {},
   "source": [
    "The following cell displays the `ToolMessage` that is the result of the tool node invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57dfbf5d-faa8-4b3d-854e-583835f319cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='null', name='my_tool', tool_call_id='miracle')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"messages\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
