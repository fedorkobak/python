{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dfd5f4-cbd4-4a82-9327-c0046fd087ad",
   "metadata": {},
   "source": [
    "# Messages\n",
    "\n",
    "The native way for LangChain to process messages is to keep them in the special abstractions that define different types of messages. Much of the functionality of LangChain and LangGraph is designed to consume and produce these messages abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d18c52-e03a-4d6b-8307-96920a25616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a173ff-5c15-4936-92b4-436d7e2cf2b6",
   "metadata": {},
   "source": [
    "## Pretty print\n",
    "\n",
    "The messages have a `pretty_print` method that prints them in a special format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bfcbb-29e8-4133-b7fd-26b8a175e6a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the outputs of the `pretty_print` method for a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2137d1db-1e20-4546-a277-979f4da78500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n"
     ]
    }
   ],
   "source": [
    "human_message = HumanMessage(\"What is the weather in SF\")\n",
    "human_message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5345ec5-599b-4100-83b7-643eb159dfa2",
   "metadata": {},
   "source": [
    "However, its true potential is in using the pretty print for the lists of messages from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe7101-cd9d-4e8e-9c7a-05368f641a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in SF\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "However, I'm a large language model, I don't have real-time access to current weather conditions\n"
     ]
    }
   ],
   "source": [
    "messages = [human_message]\n",
    "model = ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    "messages.append(model.invoke(messages))\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae474bb-8222-4f72-a5f1-741dbbc221cd",
   "metadata": {},
   "source": [
    "## Trimming\n",
    "\n",
    "The `langchain_core.messages.trim_messages` allows you to trim the chat history according to the token logic. This leaves only the messages that corespond to the specified number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ee3b4-5769-463f-9071-cc7caeff419c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines some messages and trims them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab203535-f13e-4371-8e2f-83f1a85c94b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='This is too hard question for me!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_core.messages.trim_messages(\n",
    "    [\n",
    "        HumanMessage(\"Hello! What is the capital of France\"),\n",
    "        AIMessage(\"This is too hard question for me!\")\n",
    "    ],\n",
    "    max_tokens=10,\n",
    "    token_counter=ChatOllama(model=\"llama3.1\", num_predict=20)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
