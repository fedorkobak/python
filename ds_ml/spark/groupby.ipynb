{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3155eaf6",
   "metadata": {},
   "source": [
    "# Group by\n",
    "\n",
    "Spark SQL supports a typical \"group by\" operations. The corresponding tools are provided by the grouping data object that comes from the data frame's `groupBy` method. This page discusses the options for using the grouped data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1102fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/20 12:49:01 WARN Utils: Your hostname, fedor-NUC10i7FNK, resolves to a loopback address: 127.0.1.1; using 192.168.100.19 instead (on interface wlp0s20f3)\n",
      "25/09/20 12:49:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/20 12:49:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder.appName('Temp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99342a",
   "metadata": {},
   "source": [
    "## Direct methos\n",
    "\n",
    "There is a set of methods that directly return just one specific aggregation: `min`, `max`, `avg`, `mean`, and `count`. You can list the columns for which you want to compute these aggregations. The meaning completely matches the functions names. They will calculate the aggregations by all available columns by default, but you can also specify the specific columns to be used in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17a4e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the data frame and grouped data that will be used as the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143668c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [\n",
    "        (\"a\", 10, 7, 9),\n",
    "        (\"a\", 18, 3, 1),\n",
    "        (\"b\", 12, 9, 1),\n",
    "        (\"b\", 15, 7, 0),\n",
    "        (\"c\", 4, 9, 12),\n",
    "        (\"c\", 12, 15, 5) \n",
    "    ],\n",
    "    schema=['group', \"value1\", \"value2\", \"value3\"]\n",
    ")\n",
    "gb = df.groupBy(\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deefac",
   "metadata": {},
   "source": [
    "The following cell shows the application of the `min` function, without specifying wich column to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6459d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-----------+\n",
      "|group|min(value1)|min(value2)|min(value3)|\n",
      "+-----+-----------+-----------+-----------+\n",
      "|    a|         10|          3|          1|\n",
      "|    b|         12|          7|          0|\n",
      "|    c|          4|          9|          5|\n",
      "+-----+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.min().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efd5b3",
   "metadata": {},
   "source": [
    "The `max` function is only used for the `value1` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6004df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|group|max(value1)|\n",
      "+-----+-----------+\n",
      "|    a|         18|\n",
      "|    b|         15|\n",
      "|    c|         12|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.max(\"value1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465b5de",
   "metadata": {},
   "source": [
    "The application of the `avg` to `value1` and `value2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7d3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+\n",
      "|group|avg(value1)|avg(value2)|\n",
      "+-----+-----------+-----------+\n",
      "|    a|       14.0|        5.0|\n",
      "|    b|       13.5|        8.0|\n",
      "|    c|        8.0|       12.0|\n",
      "+-----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.avg(\"value1\", \"value2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8de851",
   "metadata": {},
   "source": [
    "## Agg\n",
    "\n",
    "The `agg` method of the grouped data object provides general aggregations. You only need to list the expressions that instruct Spark what to compute. The following table lists the functions that can be used to design an aggregation:\n",
    "\n",
    "| Function | Description |\n",
    "|---------|-------------|\n",
    "| `count(col)` | Number of rows for the given column (non-null only). |\n",
    "| `countDistinct(col, *cols)` | Count of distinct values across one or more columns. |\n",
    "| `approx_count_distinct(col, rsd=0.05)` | Approximate count of distinct values using HyperLogLog (faster than `countDistinct`). |\n",
    "| `sum(col)` | Sum of values in a column. |\n",
    "| `sumDistinct(col)` | Sum of distinct values in a column. |\n",
    "| `avg(col)` / `mean(col)` | Average (mean) of column values. |\n",
    "| `max(col)` | Maximum value in the column. |\n",
    "| `min(col)` | Minimum value in the column. |\n",
    "| `first(col, ignorenulls=False)` | First value in the group. |\n",
    "| `last(col, ignorenulls=False)` | Last value in the group. |\n",
    "| `collect_list(col)` | Collects values into a Python list (duplicates preserved). |\n",
    "| `collect_set(col)` | Collects unique values into a Python set (duplicates removed). |\n",
    "| `variance(col)` / `var_samp(col)` | Sample variance of values in the group. |\n",
    "| `var_pop(col)` | Population variance of values in the group. |\n",
    "| `stddev(col)` / `stddev_samp(col)` | Sample standard deviation of values in the group. |\n",
    "| `stddev_pop(col)` | Population standard deviation of values in the group. |\n",
    "| `corr(col1, col2)` | Pearson correlation coefficient between two columns. |\n",
    "| `covar_samp(col1, col2)` | Sample covariance between two columns. |\n",
    "| `covar_pop(col1, col2)` | Population covariance between two columns. |\n",
    "| `skewness(col)` | Skewness of values in the group. |\n",
    "| `kurtosis(col)` | Kurtosis of values in the group. |\n",
    "| `approx_percentile(col, percentage, accuracy=10000)` | Approximate percentile of column values (for quantile analysis). |\n",
    "| `bit_and(col)` | Bitwise AND of all values in the group. |\n",
    "| `bit_or(col)` | Bitwise OR of all values in the group. |\n",
    "| `bit_xor(col)` | Bitwise XOR of all values in the group. |\n",
    "| `mode(col)` | Returns the most frequent value (mode) in the column. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd6bdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a data frame that will be used as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e424b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [\n",
    "        (\"a\", 10),\n",
    "        (\"a\", 18),\n",
    "        (\"b\", 12),\n",
    "        (\"b\", 15),\n",
    "        (\"c\", 4),\n",
    "        (\"c\", 12) \n",
    "    ],\n",
    "    schema=['group', \"value\"]\n",
    ")\n",
    "gb = df.groupBy(\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b083a6f",
   "metadata": {},
   "source": [
    "There is also the usage of the `agg` method with a few aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e546aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+-----------+\n",
      "|group|sum(value)|new name|mode(value)|\n",
      "+-----+----------+--------+-----------+\n",
      "|    a|        28|    14.0|         18|\n",
      "|    b|        27|    13.5|         15|\n",
      "|    c|        16|     8.0|          4|\n",
      "+-----+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "gb.agg(\n",
    "    F.sum(\"value\"),\n",
    "    F.avg(\"value\").alias(\"new name\"),\n",
    "    F.mode(\"value\")\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
