{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3155eaf6",
   "metadata": {},
   "source": [
    "# Group by\n",
    "\n",
    "Spark SQL supports a typical \"group by\" operations. The corresponding tools are provided by the grouping data object that comes from the data frame's `groupBy` method. This page discusses the options for using the grouped data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1102fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark_session = SparkSession.builder.appName('Temp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99342a",
   "metadata": {},
   "source": [
    "## Direct methos\n",
    "\n",
    "There is a set of methods that directly return just one specific aggregation: `min`, `max`, `avg`, `mean`, and `count`. You can list the columns for which you want to compute these aggregations. The meaning completely matches the functions names. They will calculate the aggregations by all available columns by default, but you can also specify the specific columns to be used in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17a4e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the data frame and grouped data that will be used as the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143668c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [\n",
    "        (\"a\", 10, 7, 9),\n",
    "        (\"a\", 18, 3, 1),\n",
    "        (\"b\", 12, 9, 1),\n",
    "        (\"b\", 15, 7, 0),\n",
    "        (\"c\", 4, 9, 12),\n",
    "        (\"c\", 12, 15, 5) \n",
    "    ],\n",
    "    schema=['group', \"value1\", \"value2\", \"value3\"]\n",
    ")\n",
    "gb = df.groupBy(\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deefac",
   "metadata": {},
   "source": [
    "The following cell shows the application of the `min` function, without specifying wich column to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6459d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-----------+\n",
      "|group|min(value1)|min(value2)|min(value3)|\n",
      "+-----+-----------+-----------+-----------+\n",
      "|    a|         10|          3|          1|\n",
      "|    b|         12|          7|          0|\n",
      "|    c|          4|          9|          5|\n",
      "+-----+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.min().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efd5b3",
   "metadata": {},
   "source": [
    "The `max` function is only used for the `value1` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6004df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|group|max(value1)|\n",
      "+-----+-----------+\n",
      "|    a|         18|\n",
      "|    b|         15|\n",
      "|    c|         12|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.max(\"value1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465b5de",
   "metadata": {},
   "source": [
    "The application of the `avg` to `value1` and `value2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7d3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+\n",
      "|group|avg(value1)|avg(value2)|\n",
      "+-----+-----------+-----------+\n",
      "|    a|       14.0|        5.0|\n",
      "|    b|       13.5|        8.0|\n",
      "|    c|        8.0|       12.0|\n",
      "+-----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.avg(\"value1\", \"value2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8de851",
   "metadata": {},
   "source": [
    "## Agg\n",
    "\n",
    "The `agg` method of the grouped data object provides general aggregations. You only need to list the expressions that instruct Spark what to compute. The following table lists the functions that can be used to design an aggregation:\n",
    "\n",
    "| Function | Description |\n",
    "|---------|-------------|\n",
    "| `count(col)` | Number of rows for the given column (non-null only). |\n",
    "| `countDistinct(col, *cols)` | Count of distinct values across one or more columns. |\n",
    "| `approx_count_distinct(col, rsd=0.05)` | Approximate count of distinct values using HyperLogLog (faster than `countDistinct`). |\n",
    "| `sum(col)` | Sum of values in a column. |\n",
    "| `sumDistinct(col)` | Sum of distinct values in a column. |\n",
    "| `avg(col)` / `mean(col)` | Average (mean) of column values. |\n",
    "| `max(col)` | Maximum value in the column. |\n",
    "| `min(col)` | Minimum value in the column. |\n",
    "| `first(col, ignorenulls=False)` | First value in the group. |\n",
    "| `last(col, ignorenulls=False)` | Last value in the group. |\n",
    "| `collect_list(col)` | Collects values into a Python list (duplicates preserved). |\n",
    "| `collect_set(col)` | Collects unique values into a Python set (duplicates removed). |\n",
    "| `variance(col)` / `var_samp(col)` | Sample variance of values in the group. |\n",
    "| `var_pop(col)` | Population variance of values in the group. |\n",
    "| `stddev(col)` / `stddev_samp(col)` | Sample standard deviation of values in the group. |\n",
    "| `stddev_pop(col)` | Population standard deviation of values in the group. |\n",
    "| `corr(col1, col2)` | Pearson correlation coefficient between two columns. |\n",
    "| `covar_samp(col1, col2)` | Sample covariance between two columns. |\n",
    "| `covar_pop(col1, col2)` | Population covariance between two columns. |\n",
    "| `skewness(col)` | Skewness of values in the group. |\n",
    "| `kurtosis(col)` | Kurtosis of values in the group. |\n",
    "| `approx_percentile(col, percentage, accuracy=10000)` | Approximate percentile of column values (for quantile analysis). |\n",
    "| `bit_and(col)` | Bitwise AND of all values in the group. |\n",
    "| `bit_or(col)` | Bitwise OR of all values in the group. |\n",
    "| `bit_xor(col)` | Bitwise XOR of all values in the group. |\n",
    "| `mode(col)` | Returns the most frequent value (mode) in the column. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd6bdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a data frame that will be used as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e424b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [\n",
    "        (\"a\", 10),\n",
    "        (\"a\", 18),\n",
    "        (\"b\", 12),\n",
    "        (\"b\", 15),\n",
    "        (\"c\", 4),\n",
    "        (\"c\", 12) \n",
    "    ],\n",
    "    schema=['group', \"value\"]\n",
    ")\n",
    "gb = df.groupBy(\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b083a6f",
   "metadata": {},
   "source": [
    "There is also the usage of the `agg` method with a few aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+-----------+\n",
      "|group|sum(value)|new name|mode(value)|\n",
      "+-----+----------+--------+-----------+\n",
      "|    a|        28|    14.0|         18|\n",
      "|    b|        27|    13.5|         15|\n",
      "|    c|        16|     8.0|          4|\n",
      "+-----+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb.agg(\n",
    "    F.sum(\"value\"),\n",
    "    F.avg(\"value\").alias(\"new name\"),\n",
    "    F.mode(\"value\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0d231",
   "metadata": {},
   "source": [
    "## Pivot\n",
    "\n",
    "The grouped data objects have a special method, `pivot`, that creates an additional grouping along a new axis. This method returns a new grouped data object that can generally be operated on as if it were a regular grouped data object. However, if a grouped data object has already undergone a pivot transformation, it can't be applied again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57e718",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates a data frame that we will use as an example. We applied the `groupBy` and `pivot` so you can see what the output object looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54183cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedData[grouping expressions: [group1], value: [group1: string, group2: string ... 2 more fields], type: Pivot]\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [\n",
    "        (\"a\", \"x\", 13, 12),\n",
    "        (\"a\", \"y\", 15, 17),\n",
    "        (\"b\", \"y\", 18, 4),\n",
    "        (\"b\", \"x\", 11, 5),\n",
    "        (\"c\", \"x\", 10, 33),\n",
    "        (\"c\", \"x\", 7, 1) \n",
    "    ],\n",
    "    schema=['group1', \"group2\", \"value1\", \"value2\"]\n",
    ")\n",
    "gb = df.groupBy(\"group1\")\n",
    "pivot = gb.pivot(\"group2\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a07dc",
   "metadata": {},
   "source": [
    "The simpliest case, with just one output value for each group, looks like as presented in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8dc1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|group1|  x|   y|\n",
      "+------+---+----+\n",
      "|     c| 17|NULL|\n",
      "|     b| 11|  18|\n",
      "|     a| 13|  15|\n",
      "+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot.sum(\"value1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e589d8",
   "metadata": {},
   "source": [
    "This case infolves aggregting sevaral values for each group of different variables using different aggregation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "407f5d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+-------------+-------------+\n",
      "|group1|x_sum(value1)|x_avg(value2)|y_sum(value1)|y_avg(value2)|\n",
      "+------+-------------+-------------+-------------+-------------+\n",
      "|     c|           17|         17.0|         NULL|         NULL|\n",
      "|     b|           11|          5.0|           18|          4.0|\n",
      "|     a|           13|         12.0|           15|         17.0|\n",
      "+------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot.agg(F.sum(\"value1\"), F.avg(\"value2\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
