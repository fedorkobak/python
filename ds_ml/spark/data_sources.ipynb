{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcc2e19",
   "metadata": {},
   "source": [
    "# Data sources\n",
    "\n",
    "This section describes the tools implemented in Spark for saving, loading, and versioning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d77d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/22 10:39:39 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/22 10:39:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/22 10:39:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/22 10:39:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_session = (\n",
    "    SparkSession.builder.appName(\"Temp\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79beb924",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "Spark has a set of methods for saving data frames for future use. Consider the most important ones. For that data frame has an attribute `write` that refers to a set of methods: `parquiet`, `csv`, and `json`; with really transparent naming.\n",
    "\n",
    "There is also a `saveAsTable` method, that saves the data to the special storage managed by the Spark. This storage could be: `Hive Metastore` or `DeltaLake`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a7378",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider saving the simple table as a JSON table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5a591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 35)],\n",
    "    schema=[\"Name\", \"Age\"]\n",
    ")\n",
    "\n",
    "data_path = \"/tmp/my_data\"\n",
    "df.write.json(data_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3cf9d",
   "metadata": {},
   "source": [
    "A set of `.json` files is created in the destination folder to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f73168b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00011-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " '.part-00000-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " '.part-00005-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " '.part-00011-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " 'part-00017-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " '.part-00017-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json.crc',\n",
       " 'part-00005-87a1ac33-4c79-4760-b7f5-16147b167620-c000.json',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff185de8",
   "metadata": {},
   "source": [
    "Consider important for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1848d1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Bob', 'Age': 30}\n",
      "{'Name': 'Cathy', 'Age': 35}\n",
      "{'Name': 'Alice', 'Age': 25}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(data_path + \"/\" + file_name, \"r\") as f:\n",
    "            try:\n",
    "                print(json.load(f))\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e687f",
   "metadata": {},
   "source": [
    "## Read csv\n",
    "\n",
    "Use the `read.csv` method of the spark session to read a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a118f4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell reads the `spark.csv` file that I prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be621c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string,  Age: double,  Salary: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark_session.read.csv(\n",
    "    \"data_sources_files/scv_example.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    multiLine=True,\n",
    "    escape=','\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec059c",
   "metadata": {},
   "source": [
    "### Shcema\n",
    "\n",
    "Use the `schema` argument to define the schema. The schema can be specified as a simple string that matches column names with their expected data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e8043",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the matching of the `int` data type to the `Age` column instead of the default `double` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a860fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Salary: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"\"\"\n",
    "Name string,\n",
    "Age int,\n",
    "Salary double\n",
    "\"\"\"\n",
    "\n",
    "spark_session.read.csv(\n",
    "    \"data_sources_files/scv_example.csv\",\n",
    "    schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0d187",
   "metadata": {},
   "source": [
    "## SQL catalog\n",
    "\n",
    "The *Spark SQL catalog* is a special file system that provies SQL access and data is described by a special metadata provided by PySpark. This section demonstrates how to access the capabilitites of the SQL catalog from the python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252efda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The SQL catalog is specified by the `spark.sql.warehouse.dir` attribute in the Spark configuration. The following cell displays the SQL catalog for the current Spark session.\n",
    "\n",
    "**Note** It should be specified when creating of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cabdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/tmp/spark-warehouse'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0a7e1",
   "metadata": {},
   "source": [
    "The following cell uses the `write.saveAsTable` method to store the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 35)],\n",
    "    schema=[\"Name\", \"Age\"]\n",
    ")\n",
    "df.write.saveAsTable(\"example_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bb289",
   "metadata": {},
   "source": [
    "The corresponding folder should now be in the warehouse storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab76c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_save\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/spark-warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8a5c6",
   "metadata": {},
   "source": [
    "This folder contains the partitions of the saved dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd44941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "part-00005-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "part-00011-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "part-00017-74ba1525-2161-4c89-94b1-c925b52a41ff-c000.snappy.parquet\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/spark-warehouse/example_save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
