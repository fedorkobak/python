{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ee22eb",
   "metadata": {},
   "source": [
    "# ML\n",
    "\n",
    "The `pyspark` package conatins the `ml` module, which contains a set of classes for building typical for machine learning pipelines. This page discusses the details of the `pyspark.ml` module. \n",
    "\n",
    "Check the official description in [Machine Learning Library Guide](https://spark.apache.org/docs/latest/ml-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b84357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/23 15:57:13 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/23 15:57:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/23 15:57:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/23 15:57:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/23 15:57:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/09/23 15:57:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.appName(\"temp\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b98f17",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "The `features` module contains a tools for preparing the input data for a machine learning models. The following table overviews the most classes assosiated with that:\n",
    "\n",
    "| Class / Transformer         | Description                                                                 |\n",
    "|-----------------------------|-----------------------------------------------------------------------------|\n",
    "| **Binarizer**              | Converts continuous values into 0/1 based on a threshold.                   |\n",
    "| **Bucketizer**             | Splits a continuous feature into buckets (bins) given split points.         |\n",
    "| **QuantileDiscretizer**    | Similar to Bucketizer but automatically computes bins by quantiles.         |\n",
    "| **StringIndexer**          | Converts string labels into numeric indices.                                |\n",
    "| **IndexToString**          | Converts numeric indices back to original string labels.                    |\n",
    "| **OneHotEncoder**          | Converts indexed categorical values into one-hot encoded vectors.           |\n",
    "| **VectorAssembler**        | Combines multiple columns into a single vector column (features).           |\n",
    "| **VectorIndexer**          | Identifies categorical features in a vector and indexes them automatically. |\n",
    "| **PolynomialExpansion**    | Generates polynomial features up to a specified degree.                     |\n",
    "| **Interaction**            | Generates interaction features between input columns.                       |\n",
    "| **Normalizer**             | Normalizes feature vectors to unit norm (L1, L2, max).                      |\n",
    "| **StandardScaler**         | Standardizes features by removing mean and scaling to unit variance.        |\n",
    "| **MinMaxScaler**           | Scales features to a specified range (default [0, 1]).                      |\n",
    "| **MaxAbsScaler**           | Scales features to [-1, 1] based on max absolute value per feature.         |\n",
    "| **Imputer**                | Fills missing values in numeric columns with mean, median, or mode.         |\n",
    "| **PCA**                    | Performs Principal Component Analysis for dimensionality reduction.         |\n",
    "| **ChiSqSelector**          | Selects categorical features based on Chi-Squared test results.             |\n",
    "| **StopWordsRemover**       | Removes stop words from text columns.                                       |\n",
    "| **Tokenizer**              | Splits text into individual words (tokens).                                 |\n",
    "| **RegexTokenizer**         | Tokenizes text using a regular expression pattern.                          |\n",
    "| **HashingTF**              | Maps a sequence of terms to fixed-length feature vectors using hashing.     |\n",
    "| **CountVectorizer**        | Converts text documents to term frequency vectors with a learned vocab.     |\n",
    "| **IDF**                    | Computes inverse document frequency for TF-IDF transformation.              |\n",
    "| **Word2Vec**               | Learns word embeddings from text data.                                      |\n",
    "| **ElementwiseProduct**     | Multiplies each element of a vector by a constant scaling vector.           |\n",
    "| **DCT**                    | Applies Discrete Cosine Transform to a vector column.                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920c23e",
   "metadata": {},
   "source": [
    "### Imputer\n",
    "\n",
    "The imputer replaces the missing values in the input data with a constant value which is mean, mode or median from the column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5f90d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows usage of the `Inputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cb98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+--------+\n",
      "|   a|   b|a_transf|b_transf|\n",
      "+----+----+--------+--------+\n",
      "|   2|NULL|       2|       5|\n",
      "|NULL|   4|       4|       4|\n",
      "|   6|   6|       6|       6|\n",
      "+----+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (2, None),\n",
    "        (None, 4),\n",
    "        (6, 6)\n",
    "    ],\n",
    "    schema=[\"a\", \"b\"]\n",
    ")\n",
    "\n",
    "(\n",
    "    Imputer(\n",
    "        inputCols=[\"a\", \"b\"],\n",
    "        outputCols=[\"a_transf\", \"b_transf\"]\n",
    "    )\n",
    "    .fit(df).transform(df).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382e932",
   "metadata": {},
   "source": [
    "### OHE \n",
    "\n",
    "Ohe is realised through `OneHotEncoder`. Some of details assocaited with using it:\n",
    "\n",
    "- It only handle only numeric values, so you must replace your categorical values with numeric labels before, for example, using the `StringIndexer`.\n",
    "- The output will be column of the sparse vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543c58a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates how to apply the `OneHotEncoder` to the simple spark dataframe and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|column|       result|\n",
      "+------+-------------+\n",
      "|     1|(3,[1],[1.0])|\n",
      "|     2|(3,[2],[1.0])|\n",
      "|     1|(3,[1],[1.0])|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "df = spark_session.createDataFrame(\n",
    "    data=[(1,), (2,), (1,)],\n",
    "    schema=[\"column\"]\n",
    ")\n",
    "OneHotEncoder(\n",
    "    inputCols=[\"column\"],\n",
    "    outputCols=[\"result\"],\n",
    "    dropLast=False\n",
    ").fit(df).transform(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
