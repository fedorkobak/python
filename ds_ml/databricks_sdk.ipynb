{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86382367",
   "metadata": {},
   "source": [
    "# Databricks SDK\n",
    "\n",
    "This page considers details on working with databricks SDK. Check the [Databricks SDK for Python](https://databricks-sdk-py.readthedocs.io/en/latest/) for official reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d134a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a86fd0",
   "metadata": {},
   "source": [
    "## Workspace client\n",
    "\n",
    "The most popular way to communicate with databricks workspace is through a `databricks.sdk.WorkspaceClient`. To create it, you must set up Databricks authentification:\n",
    "\n",
    "- Through setting `~/.databrickscfg` file. It may look like this:\n",
    "- Through defining environment variables. The most popular are:\n",
    "    - `DATABRICKS_HOST`: set your databricks host.\n",
    "    - `DATABRICKS_TOKEN`: set your access token.\n",
    "\n",
    "The default `.databrickscfg` file may look like this: \n",
    "\n",
    "```\n",
    "[DEFAULT]\n",
    "host = https:////dbc-<some unique for workspace>.cloud.databricks.com\n",
    "token = <here is your token>\n",
    "```\n",
    "\n",
    "- The profile name `DEFAULT` is important. You can specify a different name, but this will be used by default.\n",
    "- The `host` you can copy from the browser url line (just host, without path).\n",
    "- The `token` you can get through databricks UI: settings->developer->Access tokens->Manage.\n",
    "\n",
    "**Note.** If you have problems with authentication, check the environment variables. Some tools, such as the VSCode Databricks extension, may define some default values starting with `DATABRICKS_...`. Also, check the `~/.ipython/profile_default/startup` if there are some startup scripts that can invisibly change the behavior of the IPython."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959306b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If everything cofigured correctly, the following cell shold be runned without any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12271cdc",
   "metadata": {},
   "source": [
    "## VSCode\n",
    "\n",
    "Some examples that you can only be run using the Databricks extension for VSCode. Its configuration can be extremely tricky.\n",
    "\n",
    "For now (oct 2025) it requires:\n",
    "\n",
    "- The `python3.11`: or any other version which supporst [`distutils`](https://docs.python.org/3/library/distutils.html).\n",
    "- The `readline` module can only be added to the Python distribution if it is built with the `realine` package installed on the system.\n",
    "\n",
    "**Note** The extension adds an initialization file to the `ipython` (`/home/user/.ipython/profile_default/startup`), so your `ipython` can change it behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabd0e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If the setup is correct, you will have access to the `spark` and `dbutils` objects from the notebook without direct assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7983f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x73f9de975a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c381a2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<databricks.sdk.dbutils.RemoteDbUtils at 0x73f9dc5b51d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a504b",
   "metadata": {},
   "source": [
    "## Spark session\n",
    "\n",
    "You can get a databricks session that will have access to your databricks workspace by using `databricks.connect.DatabricksSession.builder.remote().getOrCreate` method.\n",
    "\n",
    "- You cannnot create crate a `DatabricksSession` if you have a regular `pyspark` installed on your system. You must run this code from a different Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816627d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates a Spark session that attched to the Databricks environment runned in the \"serverless\" mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5861762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.remote(serverless=True).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea131633",
   "metadata": {},
   "source": [
    "The following cell displays the list of the tables that are available in my Databricks workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24a75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|  telco_churn_bronze|      false|\n",
      "| default|telco_churn_features|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef4687",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "The `databricks.feature_engineering` module allows to manipulate feature storage in databricks.\n",
    "\n",
    "The `databricks.feature_engineering.FeatureEngineeringClient` object provides methods:\n",
    "\n",
    "| Method                                                                       | Description                                                                                                                           |\n",
    "| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `create_feature_table(...)`                                                  | Creates a feature table in Unity Catalog, defining its primary keys, schema, timestamp column, and metadata.                          |\n",
    "| `create_training_set(...)`                                                   | Joins features (via `FeatureLookup` or `FeatureSpec`) to a DataFrame to form a training set with metadata.                            |\n",
    "| `log_model(...)`                                                             | Logs an MLflow model together with feature metadata so the required features can be fetched automatically at inference.               |\n",
    "| `score_batch(...)`                                                           | Runs batch inference: given a model URI and a DataFrame, automatically fetches missing features, joins them, and returns predictions. |\n",
    "| `create_feature_spec(...)`                                                   | Defines a feature spec (collection of `FeatureLookup`/`FeatureFunction`) for use in training sets or feature serving.                 |\n",
    "| `create_feature_serving_endpoint(...)`                                       | Creates an endpoint for real-time / online feature serving.                                                                           |\n",
    "| `get_feature_serving_endpoint(...)` / `delete_feature_serving_endpoint(...)` | Manage (retrieve or delete) feature serving endpoints.                                                                                |\n",
    "| `publish_table(...)`                                                         | Publishes an offline feature table to an online store for low-latency feature access.                                                 |\n",
    "| `read_table(...)`                                                            | Reads the contents of a feature table into a Spark DataFrame.                                                                         |\n",
    "| `write_table(...)`                                                           | Inserts or upserts data into a feature table; supports streaming DataFrames.                                                          |\n",
    "| `set_feature_table_tag(...)` / `delete_feature_table_tag(...)`               | Manage tags (set or delete) on feature tables for governance and organization.                                                        |\n",
    "| `drop_online_table(...)`                                                     | Removes a published feature table from an online store.                                                                               |\n",
    "\n",
    "For more details and examples check the [Feature engineering](databricks_sdk/feature_engineering.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505031f4",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "Databricks offers many possibilities of posibilities for serving machine, learning models as well as served models. You can access many functions can be accessed through the Python SDK. The following talbe lists some of them:\n",
    "\n",
    "| Possibility | Description | Python SDK Context / Method Category |\n",
    "| :--- | :--- | :--- |\n",
    "| **Create Endpoint** | Programmatically create a new model serving endpoint, including configuration for custom models, Foundation Model APIs, or external models. | `w.serving_endpoints.create()` |\n",
    "| **Get/List Endpoints** | Retrieve the status, configuration, and metadata for a specific serving endpoint or list all serving endpoints in the workspace. | `w.serving_endpoints.get()`, `w.serving_endpoints.list()` |\n",
    "| **Update Endpoint Configuration** | Modify an existing serving endpoint's configuration, such as changing the served model version, adjusting traffic split, or changing the workload size. | `w.serving_endpoints.update_config()` |\n",
    "| **Delete Endpoint** | Remove a serving endpoint. | `w.serving_endpoints.delete()` |\n",
    "| **Query Endpoint (Scoring)** | Send real-time inference requests to a deployed model serving endpoint, often using an OpenAI-compatible client configured via the SDK for LLMs, or standard methods for custom models. | Handled via the Databricks-configured OpenAI client or other scoring methods (e.g., `mlflow.deployments.get_deploy_client()` in some contexts). |\n",
    "| **Manage Provisioned Throughput (PT) Endpoints** | Create and manage endpoints specifically configured for Foundation Models with guaranteed performance (Provisioned Throughput). | `w.serving_endpoints.create_pt_endpoint()` (and related PT methods) |\n",
    "| **Retrieve Build Logs** | Get the build logs for a served model on a serving endpoint, useful for debugging deployment issues. | `w.serving_endpoints.get_served_model_build_logs()` |\n",
    "| **Configure AI Gateway** | Set configurations related to AI Gateway features like fallbacks, guardrails, inference tables, and usage tracking for the serving endpoint. | Used within the `ai_gateway` parameter of `create()`/`update_config()`. |\n",
    "| **Configure Rate Limits** | Apply rate limits to the serving endpoint (though documentation suggests using AI Gateway for newer rate limit management). | Used within the `rate_limits` parameter of `create()`. |\n",
    "| **Route Optimization** | Enable configuration for route optimization on the serving endpoint for low-latency workloads. | Used within the `route_optimized` parameter of `create()`/`update_config()`. |\n",
    "\n",
    "Check more in the [Serving](databricks_sdk/serving.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c851b",
   "metadata": {},
   "source": [
    "## Workspace\n",
    "\n",
    "Manage your workspace. You can access this API through `WorkSpaceClient().workspace`. The following table lists the methods and their descriptions:\n",
    "\n",
    "| Name                                                                                                                                                 | Description                                                          |\n",
    "| ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |\n",
    "| `delete(path: str, recursive: Optional[bool])`                                                                                                       | Deletes an object or directory (optionally recursively).             |\n",
    "| `download(path: str, format: ExportFormat)`                                                                                                          | Downloads a notebook or file from the workspace.                     |\n",
    "| `export(path: str, format: Optional[ExportFormat])`                                                                                                  | Exports an object or directory (e.g. notebook or folder).            |\n",
    "| `get_permission_levels(workspace_object_type: str, workspace_object_id: str)`                                                                        | Gets the permission levels a user can have on an object.             |\n",
    "| `get_permissions(workspace_object_type: str, workspace_object_id: str)`                                                                              | Gets the current permissions on a workspace object.                  |\n",
    "| `get_status(path: str)`                                                                                                                              | Gets the status (metadata / existence) of an object or directory.    |\n",
    "| `import_(path: str, content: Optional[str], format: Optional[ImportFormat], language: Optional[Language], overwrite: Optional[bool])`                | Imports an object (notebook / file) or directory into the workspace. |\n",
    "| `list(path: str, notebooks_modified_after: int, recursive: bool = False)`                                                                            | Lists workspace objects under a path (optionally recursive).         |\n",
    "| `mkdirs(path: str)`                                                                                                                                  | Creates a directory (and any necessary parent dirs).                 |\n",
    "| `set_permissions(workspace_object_type: str, workspace_object_id: str, access_control_list: Optional[List[WorkspaceObjectAccessControlRequest]])`    | Sets permissions on an object (replacing existing ones).             |\n",
    "| `update_permissions(workspace_object_type: str, workspace_object_id: str, access_control_list: Optional[List[WorkspaceObjectAccessControlRequest]])` | Updates permissions on an object (modifies existing ones).           |\n",
    "| `upload(path: str, content: bytes, format: ImportFormat, language: Language, overwrite: bool = False)`                                               | Uploads a notebook / file or directory content to the workspace.     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6e5bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates how to create the file can be created in your Databricks API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9224248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import workspace\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "notebook_path = f\"/Workspace/Users/{w.current_user.me().user_name}/knowledge/some_file\"\n",
    "\n",
    "w.workspace.import_(\n",
    "    content=base64.b64encode((\"CREATE LIVE TABLE dlt_sample AS SELECT 1\").encode()).decode(),\n",
    "    format=workspace.ImportFormat.SOURCE,\n",
    "    language=workspace.Language.SQL,\n",
    "    overwrite=True,\n",
    "    path=notebook_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f19f1",
   "metadata": {},
   "source": [
    "## dbutils\n",
    "\n",
    "The `dbutils` is an object that is pre-initialized in Databricks notebooks. You should only run it in a configured Databricks environment. The following table lists the main modules of dbutils, and their description:\n",
    "\n",
    "| Module      | Description                                                         |\n",
    "|-------------|---------------------------------------------------------------------|\n",
    "| credentials | Utilities for interacting with credentials within notebooks         |\n",
    "| data        | Utilities for understanding and interacting with datasets (EXPERIMENTAL) |\n",
    "| fs          | Utilities for accessing the Databricks file system (DBFS)           |\n",
    "| jobs        | Utilities for leveraging job features                               |\n",
    "| library     | Deprecated. Utilities for managing session-scoped libraries         |\n",
    "| meta        | Utilities to hook into the compiler (EXPERIMENTAL)                  |\n",
    "| notebook    | Utilities for managing the control flow of notebooks (EXPERIMENTAL) |\n",
    "| preview     | Utilities in preview                                                |\n",
    "| secrets     | Utilities for leveraging secrets within notebooks                   |\n",
    "| widgets     | Utilities for parameterizing notebooks                              |\n",
    "| api         | Utilities for managing application builds                           |\n",
    "\n",
    "Check more in [Databricks Utilities (`dbutils`) reference](https://docs.databricks.com/aws/en/dev-tools/databricks-utils) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bab92b",
   "metadata": {},
   "source": [
    "### Jobs\n",
    "\n",
    "The `dbutils.jobs` module allows you to manipulate the behavour of Dataricks jobs. To throw messages between jobs use:\n",
    "\n",
    "- The `dbutils.jobs.taskValues.set` allows to set message that would be picked up by the some other script.\n",
    "- The `dbutils.jobs.taskValues.set` to pick up the message generated by the other job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df57991",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell sets the \"hello\" message under the \"test\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key=\"test\", value=\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55d5cd",
   "metadata": {},
   "source": [
    "The following cell picks up the `test` message leaved by task `some_task`. For debuging purposes `debugValue` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e95c599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not found'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.jobs.taskValues.get(taskKey=\"some_task\", key=\"test\", debugValue=\"not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dac41",
   "metadata": {},
   "source": [
    "Since the code is not running in the job environment, the `debugValue` is returned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
