{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86382367",
   "metadata": {},
   "source": [
    "# Databricks SDK\n",
    "\n",
    "This page considers details on working with databricks SDK. Check the [Databricks SDK for Python](https://databricks-sdk-py.readthedocs.io/en/latest/) for official reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d134a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a86fd0",
   "metadata": {},
   "source": [
    "## Workspace client\n",
    "\n",
    "The most popular way to communicate with databricks workspace is through a `databricks.sdk.WorkspaceClient`. To create it, you must set up Databricks authentification:\n",
    "\n",
    "- Through setting `~/.databrickscfg` file. It may look like this:\n",
    "- Through defining environment variables. The most popular are:\n",
    "    - `DATABRICKS_HOST`: set your databricks host.\n",
    "    - `DATABRICKS_TOKEN`: set your access token.\n",
    "\n",
    "The default `.databrickscfg` file may look like this: \n",
    "\n",
    "```\n",
    "[DEFAULT]\n",
    "host = https:////dbc-<some unique for workspace>.cloud.databricks.com\n",
    "token = <here is your token>\n",
    "```\n",
    "\n",
    "- The profile name `DEFAULT` is important. You can specify a different name, but this will be used by default.\n",
    "- The `host` you can copy from the browser url line (just host, without path).\n",
    "- The `token` you can get through databricks UI: settings->developer->Access tokens->Manage.\n",
    "\n",
    "**Note.** If you have problems with authentication, check the environment variables. Some tools, such as the VSCode Databricks extension, may define some default values starting with `DATABRICKS_...`. Also, check the `~/.ipython/profile_default/startup` if there are some startup scripts that can invisibly change the behavior of the IPython."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959306b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If everything cofigured correctly, the following cell shold be runned without any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a504b",
   "metadata": {},
   "source": [
    "## Spark session\n",
    "\n",
    "You can get a databricks session that will have access to your databricks workspace by using `databricks.connect.DatabricksSession.builder.remote().getOrCreate` method.\n",
    "\n",
    "- You cannnot create crate a `DatabricksSession` if you have a regular `pyspark` installed on your system. You must run this code from a different Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816627d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates a Spark session that attched to the Databricks environment runned in the \"serverless\" mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5861762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.remote(serverless=True).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea131633",
   "metadata": {},
   "source": [
    "The following cell displays the list of the tables that are available in my Databricks workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24a75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|  telco_churn_bronze|      false|\n",
      "| default|telco_churn_features|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef4687",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "The `databricks.feature_engineering` module allows to manipulate feature storage in databricks.\n",
    "\n",
    "The `databricks.feature_engineering.FeatureEngineeringClient` object provides methods:\n",
    "\n",
    "| Method                                                                       | Description                                                                                                                           |\n",
    "| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `create_feature_table(...)`                                                  | Creates a feature table in Unity Catalog, defining its primary keys, schema, timestamp column, and metadata.                          |\n",
    "| `create_training_set(...)`                                                   | Joins features (via `FeatureLookup` or `FeatureSpec`) to a DataFrame to form a training set with metadata.                            |\n",
    "| `log_model(...)`                                                             | Logs an MLflow model together with feature metadata so the required features can be fetched automatically at inference.               |\n",
    "| `score_batch(...)`                                                           | Runs batch inference: given a model URI and a DataFrame, automatically fetches missing features, joins them, and returns predictions. |\n",
    "| `create_feature_spec(...)`                                                   | Defines a feature spec (collection of `FeatureLookup`/`FeatureFunction`) for use in training sets or feature serving.                 |\n",
    "| `create_feature_serving_endpoint(...)`                                       | Creates an endpoint for real-time / online feature serving.                                                                           |\n",
    "| `get_feature_serving_endpoint(...)` / `delete_feature_serving_endpoint(...)` | Manage (retrieve or delete) feature serving endpoints.                                                                                |\n",
    "| `publish_table(...)`                                                         | Publishes an offline feature table to an online store for low-latency feature access.                                                 |\n",
    "| `read_table(...)`                                                            | Reads the contents of a feature table into a Spark DataFrame.                                                                         |\n",
    "| `write_table(...)`                                                           | Inserts or upserts data into a feature table; supports streaming DataFrames.                                                          |\n",
    "| `set_feature_table_tag(...)` / `delete_feature_table_tag(...)`               | Manage tags (set or delete) on feature tables for governance and organization.                                                        |\n",
    "| `drop_online_table(...)`                                                     | Removes a published feature table from an online store.                                                                               |\n",
    "\n",
    "For more details and examples check the [Feature engineering](databricks_sdk/feature_engineering.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505031f4",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "Databricks offers many possibilities of posibilities for serving machine, learning models as well as served models. You can access many functions can be accessed through the Python SDK. The following talbe lists some of them:\n",
    "\n",
    "| Possibility | Description | Python SDK Context / Method Category |\n",
    "| :--- | :--- | :--- |\n",
    "| **Create Endpoint** | Programmatically create a new model serving endpoint, including configuration for custom models, Foundation Model APIs, or external models. | `w.serving_endpoints.create()` |\n",
    "| **Get/List Endpoints** | Retrieve the status, configuration, and metadata for a specific serving endpoint or list all serving endpoints in the workspace. | `w.serving_endpoints.get()`, `w.serving_endpoints.list()` |\n",
    "| **Update Endpoint Configuration** | Modify an existing serving endpoint's configuration, such as changing the served model version, adjusting traffic split, or changing the workload size. | `w.serving_endpoints.update_config()` |\n",
    "| **Delete Endpoint** | Remove a serving endpoint. | `w.serving_endpoints.delete()` |\n",
    "| **Query Endpoint (Scoring)** | Send real-time inference requests to a deployed model serving endpoint, often using an OpenAI-compatible client configured via the SDK for LLMs, or standard methods for custom models. | Handled via the Databricks-configured OpenAI client or other scoring methods (e.g., `mlflow.deployments.get_deploy_client()` in some contexts). |\n",
    "| **Manage Provisioned Throughput (PT) Endpoints** | Create and manage endpoints specifically configured for Foundation Models with guaranteed performance (Provisioned Throughput). | `w.serving_endpoints.create_pt_endpoint()` (and related PT methods) |\n",
    "| **Retrieve Build Logs** | Get the build logs for a served model on a serving endpoint, useful for debugging deployment issues. | `w.serving_endpoints.get_served_model_build_logs()` |\n",
    "| **Configure AI Gateway** | Set configurations related to AI Gateway features like fallbacks, guardrails, inference tables, and usage tracking for the serving endpoint. | Used within the `ai_gateway` parameter of `create()`/`update_config()`. |\n",
    "| **Configure Rate Limits** | Apply rate limits to the serving endpoint (though documentation suggests using AI Gateway for newer rate limit management). | Used within the `rate_limits` parameter of `create()`. |\n",
    "| **Route Optimization** | Enable configuration for route optimization on the serving endpoint for low-latency workloads. | Used within the `route_optimized` parameter of `create()`/`update_config()`. |\n",
    "\n",
    "Check more in the [Serving](databricks_sdk/serving.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c851b",
   "metadata": {},
   "source": [
    "## Workspace\n",
    "\n",
    "Manage your workspace. You can access this API through `WorkSpaceClient().workspace`. The following table lists the methods and their descriptions:\n",
    "\n",
    "| Name                                                                                                                                                 | Description                                                          |\n",
    "| ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |\n",
    "| `delete(path: str, recursive: Optional[bool])`                                                                                                       | Deletes an object or directory (optionally recursively).             |\n",
    "| `download(path: str, format: ExportFormat)`                                                                                                          | Downloads a notebook or file from the workspace.                     |\n",
    "| `export(path: str, format: Optional[ExportFormat])`                                                                                                  | Exports an object or directory (e.g. notebook or folder).            |\n",
    "| `get_permission_levels(workspace_object_type: str, workspace_object_id: str)`                                                                        | Gets the permission levels a user can have on an object.             |\n",
    "| `get_permissions(workspace_object_type: str, workspace_object_id: str)`                                                                              | Gets the current permissions on a workspace object.                  |\n",
    "| `get_status(path: str)`                                                                                                                              | Gets the status (metadata / existence) of an object or directory.    |\n",
    "| `import_(path: str, content: Optional[str], format: Optional[ImportFormat], language: Optional[Language], overwrite: Optional[bool])`                | Imports an object (notebook / file) or directory into the workspace. |\n",
    "| `list(path: str, notebooks_modified_after: int, recursive: bool = False)`                                                                            | Lists workspace objects under a path (optionally recursive).         |\n",
    "| `mkdirs(path: str)`                                                                                                                                  | Creates a directory (and any necessary parent dirs).                 |\n",
    "| `set_permissions(workspace_object_type: str, workspace_object_id: str, access_control_list: Optional[List[WorkspaceObjectAccessControlRequest]])`    | Sets permissions on an object (replacing existing ones).             |\n",
    "| `update_permissions(workspace_object_type: str, workspace_object_id: str, access_control_list: Optional[List[WorkspaceObjectAccessControlRequest]])` | Updates permissions on an object (modifies existing ones).           |\n",
    "| `upload(path: str, content: bytes, format: ImportFormat, language: Language, overwrite: bool = False)`                                               | Uploads a notebook / file or directory content to the workspace.     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6e5bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates how to create the file can be created in your Databricks API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9224248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import workspace\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "notebook_path = f\"/Workspace/Users/{w.current_user.me().user_name}/knowledge/some_file\"\n",
    "\n",
    "w.workspace.import_(\n",
    "    content=base64.b64encode((\"CREATE LIVE TABLE dlt_sample AS SELECT 1\").encode()).decode(),\n",
    "    format=workspace.ImportFormat.SOURCE,\n",
    "    language=workspace.Language.SQL,\n",
    "    overwrite=True,\n",
    "    path=notebook_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
