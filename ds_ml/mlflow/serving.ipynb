{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d0f54",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "MLFlow allows to serve models that are registered. This page looks at the relevant tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22d83b",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "The `mlflow` command-line interface allows you to start an http server that deploys the specified model. The following table shows parameters for the `mlflow models serve` command that allows to run the server.\n",
    "\n",
    "| Option                  | Description                                                                                    |\n",
    "| ----------------------- | -----------------------------------------------------------------------------------------------|\n",
    "| `-m, --model-uri <URI>` | Path or URI of the model to serve (local path, S3, GCS, DBFS, registry URI).                   |\n",
    "| `-p, --port <PORT>`     | Port to serve the model on (default: `5000`).                                                  |\n",
    "| `-h, --host <HOST>`     | Host address to bind (default: `127.0.0.1`). Use `0.0.0.0` to make it accessible externally.   |\n",
    "| `--no-conda`            | Prevents creation of a new conda environment; runs in the current environment.                 |\n",
    "| `--env-manager`         | Controls how the serving environment is created (default: `conda`).                            |\n",
    "| `--enable-mlserver`     | Use **MLServer** backend instead of the default gunicorn/waitress server (for better scaling). |\n",
    "| `--workers <N>`         | Number of worker processes to handle requests (only for `gunicorn` on Unix).                   |\n",
    "| `--install-mlflow`      | Reinstalls MLflow in the serving environment (useful if it’s missing).                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27004e38",
   "metadata": {},
   "source": [
    "## Docker\n",
    "\n",
    "Use `mlflow models build-docker` interface to pack the model as a Docker image. The following table shows the important arguments:\n",
    "\n",
    "| Option                    | Description                                                                      |\n",
    "| ------------------------- | -------------------------------------------------------------------------------- |\n",
    "| `-m, --model-uri <URI>`   | Path or URI of the model to include in the Docker image.                         |\n",
    "| `-n, --name <IMAGE_NAME>` | Name of the resulting Docker image.                                              |\n",
    "| `-b, --build <flavor>`    | Choose which model flavor to build (`python_function`, `crate`, etc.).           |\n",
    "| `--enable-mlserver`       | Use **MLServer** as the serving backend instead of the default.                  |\n",
    "| `--install-mlflow`        | Ensures MLflow is installed in the image (sometimes required for compatibility). |\n",
    "| `--env-manager` | Specifies how dependencies should be managed inside the image.                             |\n",
    "| `--platform <PLATFORM>`   | Target platform for multi-arch builds (e.g., `linux/amd64`, `linux/arm64`).      |\n",
    "| `--no-cache`              | Do not use Docker’s build cache.                                                 |\n",
    "| `--build-arg KEY=VALUE`   | Pass custom build arguments to `docker build`.                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb923c00",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eb288fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a37ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/24 15:38:27 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2025/10/24 15:38:27 INFO mlflow.pyfunc.backend: === Running command 'exec uvicorn --host localhost --port 1234 --workers 1 mlflow.pyfunc.scoring_server.app:app'\n",
      "INFO:     Started server process [319132]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:1234 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "model_uri = \"models:/rf_model/1\"\n",
    "\n",
    "def run_model_serve():\n",
    "    backend = mlflow.models.flavor_backend_registry.get_flavor_backend(\n",
    "        model_uri=model_uri,\n",
    "        env_manager=\"local\"\n",
    "    )\n",
    "\n",
    "    backend.serve(\n",
    "        model_uri=model_uri,\n",
    "        port=1234,\n",
    "        host=\"localhost\",\n",
    "        timeout=60,\n",
    "        enable_mlserver=False\n",
    "    )\n",
    "\n",
    "process = Process(target=run_model_serve)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [319132]\n"
     ]
    }
   ],
   "source": [
    "process.terminate()\n",
    "process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
