{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18428b2c-1235-48c9-8f91-450e5e7c3144",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "`transformers` is python package that allows you to use pre-trained machine learning models that belong to the transformers architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9389c4f5-0976-475c-913c-8db542879212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408846a9",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "With pipeline you can easily define any model you like. Find out more:\n",
    "\n",
    "- Corresponding [documentation page](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
    "- [Special page on this site](transformers/pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a8c53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following page shows a very simple example of loading pipeline for text classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9228928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998487234115601}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.8964248299598694}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "print(\n",
    "    pipeline(\"Wow this is awesome\"),\n",
    "    pipeline(\"The fish had really strange taste\"),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065525c8",
   "metadata": {},
   "source": [
    "In some cases, you may need to explicitly specify the `task`, `model` and `tokinizer` arguments. The following cell shows using this case for access to the `question-answering` mode of the `distilbert-base-cased-distilled-squad` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc0b7ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9831558465957642, 'start': 20, 'end': 25, 'answer': 'Paris'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pipeline = transformers.pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\", \n",
    "    tokenizer=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "qa_pipeline(\n",
    "    question=\"What is the capital of France?\",\n",
    "    context=\"France's capital is Paris.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705cbda",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "You can opearte separately with tokenizer by using corresponding objects. Check more features in the [corresponding page](transformers/tokenizer.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d344a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell loading of the `bert-base-cased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1659c6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd0217",
   "metadata": {},
   "source": [
    "So here is example how \"Hello world!\" phrase can be tokinized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1053b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8667, 1362,  106,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\n",
    "    'Hello world!', \n",
    "    add_special_tokens=True, \n",
    "    return_token_type_ids=False, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8803b34-a5cb-4182-ae2e-d114f5d7d971",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Model can be loaded separately as well ass tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef584c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell load model `Bert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d2120f-e773-4bc3-8cee-7042c2a83187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4b80c",
   "metadata": {},
   "source": [
    "The next cell shows the use of the model. Input must first be tokinised before it can be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da9cb413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6283,  0.2166,  0.5605,  ...,  0.0136,  0.6158, -0.1712],\n",
       "          [ 0.6108, -0.2253,  0.9263,  ..., -0.3028,  0.4500, -0.0714],\n",
       "          [ 0.8040,  0.1809,  0.7076,  ..., -0.0685,  0.4837, -0.0774],\n",
       "          [ 1.3290,  0.2360,  0.4567,  ...,  0.1509,  0.9621, -0.4841]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-0.7105,  0.4876,  0.9999, -0.9947,  0.9599,  0.9521,  0.9767, -0.9946,\n",
       "          -0.9815, -0.6238,  0.9776,  0.9984, -0.9989, -0.9998,  0.8559, -0.9755,\n",
       "           0.9895, -0.5281, -1.0000, -0.7414, -0.7056, -0.9999,  0.2901,  0.9786,\n",
       "           0.9729,  0.0734,  0.9828,  1.0000,  0.8981, -0.1109,  0.2780, -0.9920,\n",
       "           0.8693, -0.9985,  0.1461,  0.2067,  0.8092, -0.2430,  0.8580, -0.9585,\n",
       "          -0.8130, -0.6138,  0.7961, -0.5727,  0.9737,  0.2362, -0.1194, -0.0789,\n",
       "           0.0031,  0.9997]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized = tokenizer.encode_plus(\n",
    "    'Hello!', \n",
    "    add_special_tokens=True, \n",
    "    return_token_type_ids=False, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "out = model(**tokenized)\n",
    "out[0], out[1][:, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900567e-a5b2-4308-ba24-43ffd86b3efe",
   "metadata": {},
   "source": [
    "## Apply to dataset\n",
    "\n",
    "This section shows how the BERT model can be applied to a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fbf27-20a6-4fcd-ac5e-37fb64368055",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3374942-a756-4e09-a7a2-3f6ddafcfc95",
   "metadata": {},
   "source": [
    "We will be working with the `datasets` library, which is usually used in conjunction with the `transformers` library. So in the following cell we have extracted the `imdb` library, which contains reviews for the movies. We use a small subset of the dataframe to reduce the amount of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c47668-c68e-45b4-8e1a-22fc24f4881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The Film must have been shot in a day,there are scenes where you can see the camera reflections and its red pointer,even the scenery's green light that blends with the actors!!!The plot and the lines are really awful without even the slightest inspiration(At least as a thriller genre movie).Everything that got to do with Poe in the movie,has a shallow and childish approach.The film is full of clise and no thrilling.If you want to watch a funny b-movie for a relaxing evening with friends then go for it you will enjoy it (As I Did) but there's no way to take this film seriously!\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "np.random.seed(100)\n",
    "idx = np.random.randint(len(dataset), size=200)\n",
    "dataset = dataset.select(idx)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c692e-7031-418c-aa88-2bfdc306e8d5",
   "metadata": {},
   "source": [
    "Applying tokenization to the dataset. Finally for each element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6aee675-bacf-4392-8dce-718d616d58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        example['text'],\n",
    "        add_special_tokens=True, \n",
    "        return_token_type_ids=False, \n",
    "        truncation=True\n",
    "    )\n",
    "dataset = dataset.map(\n",
    "    tokenization, batched=True\n",
    ")\n",
    "dataset.set_format(\n",
    "    type=\"torch\", \n",
    "    columns=[\"input_ids\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd84007-8792-42b7-ac82-b2b64cad8b8f",
   "metadata": {},
   "source": [
    "After that we have extra keys for each object `input_ids` and `attention_mask` - wich is required for bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa92ac52-bf13-4a0f-8412-48644af38c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Keys:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br><b>Tokens:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1109,  2352,  1538,  1138,  1151,  2046,  1107,   170,  1285,\n",
      "          117,  1175,  1132,  4429,  1187,  1128,  1169,  1267,  1103,  4504,\n",
      "        26906,  1105,  1157,  1894,  1553,  1200,   117,  1256,  1103, 19335,\n",
      "          112,   188,  2448,  1609,  1115, 13390,  1116,  1114,  1103,  5681,\n",
      "          106,   106,   106,  1109,  4928,  1105,  1103,  2442,  1132,  1541,\n",
      "         9684,  1443,  1256,  1103, 16960,  7670,   113,  1335,  1655,  1112,\n",
      "          170, 11826,  6453,  2523,   114,   119,  5268,  1115,  1400,  1106,\n",
      "         1202,  1114, 21377,  1107,  1103,  2523,   117,  1144,   170,  8327,\n",
      "         1105,  2027,  2944,  3136,   119,  1109,  1273,  1110,  1554,  1104,\n",
      "          172,  6137,  1162,  1105,  1185, 21401,  1158,   119,  1409,  1128,\n",
      "         1328,  1106,  2824,   170,  6276,   171,   118,  2523,  1111,   170,\n",
      "        22187,  3440,  1114,  2053,  1173,  1301,  1111,  1122,  1128,  1209,\n",
      "         5548,  1122,   113,  1249,   146,  2966,   114,  1133,  1175,   112,\n",
      "          188,  1185,  1236,  1106,  1321,  1142,  1273,  5536,   106,   102])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br><b>Mask:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "display(HTML(\"<b>Keys:</b>\"))\n",
    "print(list(dataset[0].keys()))\n",
    "display(HTML(\"<br><b>Tokens:</b>\"))\n",
    "print(dataset[0]['input_ids'])\n",
    "display(HTML(\"<br><b>Mask:</b>\"))\n",
    "print(dataset[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b6a4a-b93d-4135-9e15-24f6ce311592",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009946cf-c455-42ab-817e-bb8dec8b1e69",
   "metadata": {},
   "source": [
    "Preparing the classical `torch.loader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e550ad-9765-44a7-95d4-6c3329d23ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    collate_fn=data_collator, \n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c11d7-cca3-4287-a939-1da9d039c04e",
   "metadata": {},
   "source": [
    "Now let's look at what the first element of `loader` will be - it's dict, with all the necessary stuff for fitting and forwarding through bert model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1fff79c-5d65-4fc2-8566-bb36d0ee1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(loader))\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ee675-8afa-4b75-b8ba-9341a7a200d6",
   "metadata": {},
   "source": [
    "Finally you can pass `item` to the `model` to get prediction, but you need high-performance hardware to run this: \n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "model(**item)[\"pooler_output\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b0326-a7c2-46a9-a7fd-2f9216fe761a",
   "metadata": {},
   "source": [
    "Finally, we need to model all the batches. In the following cell we have just extracted embeddings for the whole `imdb` dataset.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- This cell should be started on powerful hardware;\n",
    "- The function nested in `torch.inference_mode` is crucial, without it the model takes too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8badcac-b096-4132-ae5f-524c423081eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:37<00:00, 13.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "\n",
    "        batch = {\n",
    "            key: batch[key].to(device) \n",
    "            for key in ['attention_mask', 'input_ids']\n",
    "        }\n",
    "        \n",
    "        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n",
    "        total_embeddings.append(embeddings)\n",
    "\n",
    "    return torch.cat(total_embeddings, dim=0)\n",
    "\n",
    "embeddings = get_embeddings(model, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795c7d0-440e-4a11-ba87-4f1765e561e1",
   "metadata": {},
   "source": [
    "So for each line from the dataset we got an embedding from the model. It's representation and shape is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c726a9-8da0-4d88-84c0-3a3e611ccb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Embeddings:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6028,  0.1125, -0.2223,  ..., -0.1698,  0.1655,  0.0645],\n",
       "        [ 0.6183,  0.0239, -0.2428,  ..., -0.1532,  0.1792,  0.1111],\n",
       "        [ 0.3977,  0.1045, -0.1627,  ..., -0.0737,  0.2369,  0.1243],\n",
       "        ...,\n",
       "        [ 0.5241,  0.1857, -0.4136,  ..., -0.2503,  0.0959,  0.2350],\n",
       "        [ 0.6121,  0.0789, -0.1658,  ..., -0.1467,  0.4310,  0.1626],\n",
       "        [ 0.6343,  0.0062, -0.1479,  ..., -0.2116,  0.1337,  0.1664]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><b>Shape:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<b>Embeddings:</b>\"))\n",
    "display(embeddings)\n",
    "display(HTML(\"<br><b>Shape:</b>\"))\n",
    "display(embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
