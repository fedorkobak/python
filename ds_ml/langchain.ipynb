{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e24c21",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangCain is software that allows to build applications based on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30554ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Typically, you must export the API key corresponding to the model type you want to use from your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f7a52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the code that will only work if the \"GOOGLE_API_KEY\" variable exists in your environment with the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86af27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the details of your test request.  I need information such as:\n",
      "\n",
      "* **What kind of test are you requesting?** (e.g., a unit test, an integration test, a performance test, a stress test, a usability test, a grammar test, a logic test, a factual accuracy test, etc.)\n",
      "* **What is the subject of the test?** (e.g., a piece of code, a website, a document, a sentence, an argument, etc.)\n",
      "* **What are the inputs or data for the test?** (If applicable)\n",
      "* **What are the expected outputs or results?** (If applicable)\n",
      "* **What are the acceptance criteria?** (How will you know if the test passed or failed?)\n",
      "\n",
      "The more information you give me, the better I can assist you.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "ans = llm.invoke(\"Test request\")\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772afd6",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "That's an excellent point. It's crucial to know the specific package needed for each integration to correctly set up the environment. LLM is a core component of the any agent based system the following cell shows the classes that implement different LLMs into the LangChain module.\n",
    "\n",
    "| Provider | Type | LangChain Class Name (Python) | Python Package |\n",
    "|---|---|---|---|\n",
    "| OpenAI | Commercial API | `langchain_openai.chat_models.ChatOpenAI`, `langchain_openai.llms.OpenAI` | `langchain-openai` |\n",
    "| Google | Commercial API | `langchain_google_genai.chat_models.ChatGoogleGenerativeAI` | `langchain-google-genai` |\n",
    "| Anthropic | Commercial API | `langchain_anthropic.chat_models.ChatAnthropic`, `langchain_anthropic.llms.AnthropicLLM` | `langchain-anthropic` |\n",
    "| Mistral AI | Commercial API | `langchain_mistralai.chat_models.ChatMistralAI` | `langchain-mistralai` |\n",
    "| Cohere | Commercial API | `langchain_cohere.chat_models.ChatCohere`, `langchain_cohere.llms.CohereLLM` | `langchain-cohere` |\n",
    "| AWS | Cloud Platform | `langchain_aws.chat_models.ChatBedrock`, `langchain_aws.llms.BedrockLLM` | `langchain-aws` |\n",
    "| Hugging Face | Community/Open-Source | `langchain_huggingface.llms.HuggingFaceHub`, `langchain_huggingface.llms.HuggingFacePipeline` | `langchain-huggingface` |\n",
    "| Ollama | On-Premise/Local | `langchain_community.chat_models.ChatOllama`, `langchain_community.llms.OllamaLLM` | `langchain-ollama` |\n",
    "| Llama.cpp | On-Premise/Local | `langchain_community.llms.LlamaCpp` | `llama-cpp-python` |\n",
    "| Replicate | Commercial API | `langchain_replicate.llms.Replicate` | `langchain-replicate` |\n",
    "| Fireworks AI | Commercial API | `langchain_fireworks.chat_models.ChatFireworks`, `langchain_fireworks.llms.FireworksLLM` | `langchain-fireworks` |\n",
    "| Databricks | Cloud Platform | `databricks_langchain.llms.Databricks` | `databricks-langchain` |\n",
    "| Azure OpenAI | Commercial API | `langchain_openai.chat_models.AzureChatOpenAI`, `langchain_openai.llms.AzureOpenAI` | `langchain-openai` |\n",
    "| AI21 Labs | Commercial API | `langchain_ai21.llms.AI21LLM`, `langchain_ai21.chat_models.ChatAI21` | `langchain-ai21` |\n",
    "| Aleph Alpha | Commercial API | `langchain_community.llms.AlephAlpha` | `langchain-aleph-alpha` |\n",
    "| Groq | Commercial API | `langchain_groq.chat_models.ChatGroq` | `langchain-groq` |\n",
    "| Together AI | Commercial API | `langchain_together.llms.TogetherLLM`, `langchain_together.chat_models.ChatTogether` | `langchain-together` |\n",
    "| IBM | Cloud Platform | `langchain_community.chat_models.ChatWatsonx` | `langchain-ibm` |\n",
    "| DeepInfra | Commercial API | `langchain_deepinfra.llms.DeepInfra` | `langchain-deepinfra` |\n",
    "| Yandex | Commercial API | `langchain_community.llms.YandexGPT`, `langchain_community.llms.YandexGPTPredictor` | `langchain-yandex` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e324a7",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "There are several classes that represent different aspects of prompting with LangChain.\n",
    "\n",
    "| Class Name    | Role           | General Description                                                                                                                                      |\n",
    "|---------------|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| SystemMessage | System         | Provides instructions or context to \"prime\" the model's behavior. It sets the persona, tone, or rules for the entire conversation. Typically the first message in a list. |\n",
    "| HumanMessage  | Human          | Represents the user's input. This is the message that a human sends to the model to ask a question or provide a command.                                  |\n",
    "| AIMessage     | AI (Assistant) | Represents the response from the language model. This is the output you get after invoking a model. It can contain text, tool calls, or other data.       |\n",
    "| ToolMessage   | Tool           | Represents the output or result of a tool function that was invoked by the AI. This is used to pass the outcome of a tool call back to the model for further processing. |\n",
    "\n",
    "The primary design of LangChain is to pass a list of objects to the model. It returns an output of type `AIMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825b45d",
   "metadata": {},
   "source": [
    "All LangChain messages are children of the `langchain_core.messages.BaseMessage` class. The  follwing cell shows the relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d64ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "\n",
    "(\n",
    "    issubclass(HumanMessage, BaseMessage),\n",
    "    issubclass(SystemMessage, BaseMessage),\n",
    "    issubclass(AIMessage, BaseMessage),\n",
    "    issubclass(ToolMessage, BaseMessage)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7684082",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "In the LangChain paradigm, a prompt is a structured input for a model. It can include a system message, user input, or messaging history. The `lang_chain` package provides various tools for prompt templating. The following cell lists the most popular classes used for templating and their descriptions.\n",
    "\n",
    "| Class / Function                        | Description                                                                 |\n",
    "|----------------------------------------|-----------------------------------------------------------------------------|\n",
    "| **BasePromptTemplate**                 | Abstract base class for all prompt templates.                               |\n",
    "| **StringPromptTemplate**               | Base class for string-based templates (like f-string).                      |\n",
    "| **PromptTemplate**                     | Core template class for generating prompts with variables. Supports methods like `from_template`, `from_file`, `from_examples`, `format`, `invoke`, `ainvoke`, and batching. |\n",
    "| **FewShotPromptTemplate**              | String-based prompt template with few-shot example support.                 |\n",
    "| **FewShotPromptWithTemplates**         | String template variant with embedded few-shot examples.                    |\n",
    "| **PipelinePromptTemplate**             | Combines multiple prompt templates into a pipeline.                         |\n",
    "| **BaseChatPromptTemplate**             | Base class for chat-style prompt templates.                                 |\n",
    "| **ChatPromptTemplate**                 | Template for chat models; build multi-role messages. Supports `from_messages` and dynamic placeholders. |\n",
    "| **AgentScratchPadChatPromptTemplate**  | Specialized chat prompt for agent scratchpad patterns.                      |\n",
    "| **AutoGPTPrompt**                      | Chat prompt variant used in AutoGPT-style workflows.                        |\n",
    "| **BaseMessagePromptTemplate**          | Base for message-level prompt templates.                                    |\n",
    "| **BaseStringMessagePromptTemplate**    | Base class for message templates using string patterns.                     |\n",
    "| **ChatMessagePromptTemplate**          | Generates chat messages (with roles, e.g. system/human/AI) from template strings. |\n",
    "| **HumanMessagePromptTemplate**         | Template specifically for human messages.                                   |\n",
    "| **AIMessagePromptTemplate**            | Template specifically for AI messages.                                      |\n",
    "| **SystemMessagePromptTemplate**        | Template specifically for system messages.                                  |\n",
    "| **MessagesPlaceholder**                | Placeholder to inject dynamic message history into a chat template.         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf67a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the `PromptTemplate` class. You can use the `from_template` method to create a template. A substitutable pattern is specified by the `{}`. The `format` method of the `PromptTempalate` class returns a string with all substituted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "322bf96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your input is: Hello!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "ans = PromptTemplate.from_template(\"Your input is: {here}\")\n",
    "print(type(ans))\n",
    "ans.format(here=\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6210b",
   "metadata": {},
   "source": [
    "## Text splitters\n",
    "\n",
    "Langchain contains module `text_splitter` which contains implementations of approaches to split texts into pieces. Can be usefull, for example for chunking in RAG pipeline.\n",
    "\n",
    "The following table shows the awailable text splitters.\n",
    "\n",
    "| Class Name | Description | Common Use Case |\n",
    "| :--- | :--- | :--- |\n",
    "| `CharacterTextSplitter` | Splits text based on a specified character (e.g., `\\n`, ` `). | Simple, quick splitting where structural integrity is not a major concern. |\n",
    "| `RecursiveCharacterTextSplitter` | The recommended default. Splits text based on a list of characters in a hierarchical order (e.g., `[\"\\n\\n\", \"\\n\", \" \"]`) to maintain logical chunks. | General-purpose text, such as articles, essays, and unstructured documents. |\n",
    "| `TokenTextSplitter` | Splits text based on the number of tokens, using a specific tokenizer (e.g., `tiktoken` for OpenAI models). | Preparing text to fit within a specific LLM's context window. |\n",
    "| `HTMLHeaderTextSplitter` | Splits HTML documents based on specified header tags (`h1`, `h2`, etc.). | Processing HTML content where you want to preserve sections defined by headers. |\n",
    "| `MarkdownTextSplitter` | Splits Markdown documents based on Markdown syntax, such as headers and code blocks. | Processing Markdown files while keeping logical sections together. |\n",
    "| `SentenceTransformersTokenTextSplitter` | Splits text using a tokenizer from the `sentence-transformers` library, based on a token count. | Working with models from the `sentence-transformers` library. |\n",
    "| `NLTKTextSplitter` | Splits text into sentences using the `NLTK` library's sentence tokenizer. | Splitting a document into individual sentences for fine-grained processing. |\n",
    "| `SpacyTextSplitter` | Splits text into sentences using the `spaCy` library. | Similar to NLTK, but leverages `spaCy` for sentence boundary detection, which can be more robust for some languages. |\n",
    "| `SemanticChunker` | A more advanced splitter that uses an embedding model to identify semantic breakpoints (topic shifts) in the text. | Creating semantically coherent chunks for more effective retrieval-augmented generation. |\n",
    "| `Language-specific Code Splitters` | A family of splitters for various programming languages (e.g., `PythonCodeTextSplitter`, `JavaScriptCodeTextSplitter`). | Processing code files to keep functions, classes, and other logical blocks intact. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef82a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "After initializing the splitter, use the `sprit_text` method to split the given text. The following cell demonstrates the application of the recursive text splitter to a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0725d291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the last people\n",
      "you’d expect to be involved in anything strange or mysterious, because they just\n",
      "didn’t hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made\n",
      "drills. He was a big, beefy man with hardly any neck, although he did have a\n",
      "very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the\n",
      "usual amount of neck, which came in very useful as she spent so much of her\n",
      "time craning over garden fences, spying on the neighbors. The Dursleys had a\n",
      "small son called Dudley and in their opinion there was no finer boy anywhere.\n",
      "\n",
      "The Dursleys had everything they wanted, but they also had a secret, and\n",
      "their greatest fear was that somebody would discover it. They didn’t think they\n",
      "could bear it if anyone found out about the Potters. Mrs. Potter was Mrs.\n",
      "Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley\n",
      "pretended she didn’t have a sister, because her sister and her good-for-nothing\n",
      "husband were as unDursleyish as it was possible to be. The Dursleys shuddered\n",
      "\n",
      "to think what the neighbors would say if the Potters arrived in the street. The\n",
      "Dursleys knew that the Potters had a small son, too, but they had never even\n",
      "seen him. This boy was another good reason for keeping the Potters away; they\n",
      "didn’t want Dudley mixing with a child like that.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "data = \"\"\"M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
    "that they were perfectly normal, thank you very much. They were the last people\n",
    "you’d expect to be involved in anything strange or mysterious, because they just\n",
    "didn’t hold with such nonsense.\n",
    "\n",
    "Mr. Dursley was the director of a firm called Grunnings, which made\n",
    "drills. He was a big, beefy man with hardly any neck, although he did have a\n",
    "very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the\n",
    "usual amount of neck, which came in very useful as she spent so much of her\n",
    "time craning over garden fences, spying on the neighbors. The Dursleys had a\n",
    "small son called Dudley and in their opinion there was no finer boy anywhere.\n",
    "\n",
    "The Dursleys had everything they wanted, but they also had a secret, and\n",
    "their greatest fear was that somebody would discover it. They didn’t think they\n",
    "could bear it if anyone found out about the Potters. Mrs. Potter was Mrs.\n",
    "Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley\n",
    "pretended she didn’t have a sister, because her sister and her good-for-nothing\n",
    "husband were as unDursleyish as it was possible to be. The Dursleys shuddered\n",
    "to think what the neighbors would say if the Potters arrived in the street. The\n",
    "Dursleys knew that the Potters had a small son, too, but they had never even\n",
    "seen him. This boy was another good reason for keeping the Potters away; they\n",
    "didn’t want Dudley mixing with a child like that.\"\"\"\n",
    "\n",
    "out = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=20\n",
    ").split_text(data)\n",
    "\n",
    "for t in out:\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
