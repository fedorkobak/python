{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e24c21",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangCain is software that allows to build applications based on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30554ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Typically, you must export the API key corresponding to the model type you want to use from your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f7a52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the code that will only work if the \"GOOGLE_API_KEY\" variable exists in your environment with the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86af27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the details of the test you'd like to request.  To help me understand your request, please tell me:\n",
      "\n",
      "* **What kind of test is it?** (e.g., unit test, integration test, performance test, usability test, A/B test, etc.)\n",
      "* **What is the subject of the test?** (e.g., a specific piece of code, a website, a software application, a marketing campaign, etc.)\n",
      "* **What are the goals of the test?** (e.g., to identify bugs, to measure performance, to compare different versions, to assess user experience, etc.)\n",
      "* **What are the inputs or data required for the test?** (e.g., specific files, parameters, user accounts, etc.)\n",
      "* **What are the expected outputs or results?** (e.g., pass/fail status, performance metrics, user feedback, etc.)\n",
      "* **What is the testing environment?** (e.g., operating system, browser, hardware specifications, etc.)\n",
      "* **Are there any specific test cases or scenarios you want to include?**\n",
      "\n",
      "The more information you provide, the better I can understand your request and potentially help you.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "ans = llm.invoke(\"Test request\")\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772afd6",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "That's an excellent point. It's crucial to know the specific package needed for each integration to correctly set up the environment. LLM is a core component of the any agent based system the following cell shows the classes that implement different LLMs into the LangChain module.\n",
    "\n",
    "| Provider | Type | LangChain Class Name (Python) | Python Package |\n",
    "|---|---|---|---|\n",
    "| OpenAI | Commercial API | `langchain_openai.chat_models.ChatOpenAI`, `langchain_openai.llms.OpenAI` | `langchain-openai` |\n",
    "| Google | Commercial API | `langchain_google_genai.chat_models.ChatGoogleGenerativeAI` | `langchain-google-genai` |\n",
    "| Anthropic | Commercial API | `langchain_anthropic.chat_models.ChatAnthropic`, `langchain_anthropic.llms.AnthropicLLM` | `langchain-anthropic` |\n",
    "| Mistral AI | Commercial API | `langchain_mistralai.chat_models.ChatMistralAI` | `langchain-mistralai` |\n",
    "| Cohere | Commercial API | `langchain_cohere.chat_models.ChatCohere`, `langchain_cohere.llms.CohereLLM` | `langchain-cohere` |\n",
    "| AWS | Cloud Platform | `langchain_aws.chat_models.ChatBedrock`, `langchain_aws.llms.BedrockLLM` | `langchain-aws` |\n",
    "| Hugging Face | Community/Open-Source | `langchain_huggingface.llms.HuggingFaceHub`, `langchain_huggingface.llms.HuggingFacePipeline` | `langchain-huggingface` |\n",
    "| Ollama | On-Premise/Local | `langchain_community.chat_models.ChatOllama`, `langchain_community.llms.OllamaLLM` | `langchain-ollama` |\n",
    "| Llama.cpp | On-Premise/Local | `langchain_community.llms.LlamaCpp` | `llama-cpp-python` |\n",
    "| Replicate | Commercial API | `langchain_replicate.llms.Replicate` | `langchain-replicate` |\n",
    "| Fireworks AI | Commercial API | `langchain_fireworks.chat_models.ChatFireworks`, `langchain_fireworks.llms.FireworksLLM` | `langchain-fireworks` |\n",
    "| Databricks | Cloud Platform | `databricks_langchain.llms.Databricks` | `databricks-langchain` |\n",
    "| Azure OpenAI | Commercial API | `langchain_openai.chat_models.AzureChatOpenAI`, `langchain_openai.llms.AzureOpenAI` | `langchain-openai` |\n",
    "| AI21 Labs | Commercial API | `langchain_ai21.llms.AI21LLM`, `langchain_ai21.chat_models.ChatAI21` | `langchain-ai21` |\n",
    "| Aleph Alpha | Commercial API | `langchain_community.llms.AlephAlpha` | `langchain-aleph-alpha` |\n",
    "| Groq | Commercial API | `langchain_groq.chat_models.ChatGroq` | `langchain-groq` |\n",
    "| Together AI | Commercial API | `langchain_together.llms.TogetherLLM`, `langchain_together.chat_models.ChatTogether` | `langchain-together` |\n",
    "| IBM | Cloud Platform | `langchain_community.chat_models.ChatWatsonx` | `langchain-ibm` |\n",
    "| DeepInfra | Commercial API | `langchain_deepinfra.llms.DeepInfra` | `langchain-deepinfra` |\n",
    "| Yandex | Commercial API | `langchain_community.llms.YandexGPT`, `langchain_community.llms.YandexGPTPredictor` | `langchain-yandex` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
