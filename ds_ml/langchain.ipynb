{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e24c21",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangCain is software that allows to build applications based on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30554ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Typically, you must export the API key corresponding to the model type you want to use from your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f7a52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the code that will only work if the \"GOOGLE_API_KEY\" variable exists in your environment with the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86af27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the details of your test request.  I need information such as:\n",
      "\n",
      "* **What kind of test are you requesting?** (e.g., a unit test, an integration test, a performance test, a stress test, a usability test, a grammar test, a logic test, a factual accuracy test, etc.)\n",
      "* **What is the subject of the test?** (e.g., a piece of code, a website, a document, a sentence, an argument, etc.)\n",
      "* **What are the inputs or data for the test?** (If applicable)\n",
      "* **What are the expected outputs or results?** (If applicable)\n",
      "* **What are the acceptance criteria?** (How will you know if the test passed or failed?)\n",
      "\n",
      "The more information you give me, the better I can assist you.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "ans = llm.invoke(\"Test request\")\n",
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772afd6",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "That's an excellent point. It's crucial to know the specific package needed for each integration to correctly set up the environment. LLM is a core component of the any agent based system the following cell shows the classes that implement different LLMs into the LangChain module.\n",
    "\n",
    "| Provider | Type | LangChain Class Name (Python) | Python Package |\n",
    "|---|---|---|---|\n",
    "| OpenAI | Commercial API | `langchain_openai.chat_models.ChatOpenAI`, `langchain_openai.llms.OpenAI` | `langchain-openai` |\n",
    "| Google | Commercial API | `langchain_google_genai.chat_models.ChatGoogleGenerativeAI` | `langchain-google-genai` |\n",
    "| Anthropic | Commercial API | `langchain_anthropic.chat_models.ChatAnthropic`, `langchain_anthropic.llms.AnthropicLLM` | `langchain-anthropic` |\n",
    "| Mistral AI | Commercial API | `langchain_mistralai.chat_models.ChatMistralAI` | `langchain-mistralai` |\n",
    "| Cohere | Commercial API | `langchain_cohere.chat_models.ChatCohere`, `langchain_cohere.llms.CohereLLM` | `langchain-cohere` |\n",
    "| AWS | Cloud Platform | `langchain_aws.chat_models.ChatBedrock`, `langchain_aws.llms.BedrockLLM` | `langchain-aws` |\n",
    "| Hugging Face | Community/Open-Source | `langchain_huggingface.llms.HuggingFaceHub`, `langchain_huggingface.llms.HuggingFacePipeline` | `langchain-huggingface` |\n",
    "| Ollama | On-Premise/Local | `langchain_community.chat_models.ChatOllama`, `langchain_community.llms.OllamaLLM` | `langchain-ollama` |\n",
    "| Llama.cpp | On-Premise/Local | `langchain_community.llms.LlamaCpp` | `llama-cpp-python` |\n",
    "| Replicate | Commercial API | `langchain_replicate.llms.Replicate` | `langchain-replicate` |\n",
    "| Fireworks AI | Commercial API | `langchain_fireworks.chat_models.ChatFireworks`, `langchain_fireworks.llms.FireworksLLM` | `langchain-fireworks` |\n",
    "| Databricks | Cloud Platform | `databricks_langchain.llms.Databricks` | `databricks-langchain` |\n",
    "| Azure OpenAI | Commercial API | `langchain_openai.chat_models.AzureChatOpenAI`, `langchain_openai.llms.AzureOpenAI` | `langchain-openai` |\n",
    "| AI21 Labs | Commercial API | `langchain_ai21.llms.AI21LLM`, `langchain_ai21.chat_models.ChatAI21` | `langchain-ai21` |\n",
    "| Aleph Alpha | Commercial API | `langchain_community.llms.AlephAlpha` | `langchain-aleph-alpha` |\n",
    "| Groq | Commercial API | `langchain_groq.chat_models.ChatGroq` | `langchain-groq` |\n",
    "| Together AI | Commercial API | `langchain_together.llms.TogetherLLM`, `langchain_together.chat_models.ChatTogether` | `langchain-together` |\n",
    "| IBM | Cloud Platform | `langchain_community.chat_models.ChatWatsonx` | `langchain-ibm` |\n",
    "| DeepInfra | Commercial API | `langchain_deepinfra.llms.DeepInfra` | `langchain-deepinfra` |\n",
    "| Yandex | Commercial API | `langchain_community.llms.YandexGPT`, `langchain_community.llms.YandexGPTPredictor` | `langchain-yandex` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e324a7",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "There are several classes that represent different aspects of prompting with LangChain.\n",
    "\n",
    "| Class Name    | Role           | General Description                                                                                                                                      |\n",
    "|---------------|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| SystemMessage | System         | Provides instructions or context to \"prime\" the model's behavior. It sets the persona, tone, or rules for the entire conversation. Typically the first message in a list. |\n",
    "| HumanMessage  | Human          | Represents the user's input. This is the message that a human sends to the model to ask a question or provide a command.                                  |\n",
    "| AIMessage     | AI (Assistant) | Represents the response from the language model. This is the output you get after invoking a model. It can contain text, tool calls, or other data.       |\n",
    "| ToolMessage   | Tool           | Represents the output or result of a tool function that was invoked by the AI. This is used to pass the outcome of a tool call back to the model for further processing. |\n",
    "\n",
    "The primary design of LangChain is to pass a list of objects to the model. It returns an output of type `AIMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825b45d",
   "metadata": {},
   "source": [
    "All LangChain messages are children of the `langchain_core.messages.BaseMessage` class. The  follwing cell shows the relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d64ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "\n",
    "(\n",
    "    issubclass(HumanMessage, BaseMessage),\n",
    "    issubclass(SystemMessage, BaseMessage),\n",
    "    issubclass(AIMessage, BaseMessage),\n",
    "    issubclass(ToolMessage, BaseMessage)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
