{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d548bd96",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "The DS/ML section discusses the python packages/frameworks specialised for building database systems and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e7dad",
   "metadata": {},
   "source": [
    "## Hugging face\n",
    "\n",
    "Huggingface is an ecosystem of packages that are related to all aspects of working with deep learning objects.\n",
    "\n",
    "The first thing you need to do is log in:\n",
    "\n",
    "`huggingface-cli login --token <your HF token>`\n",
    "\n",
    "The following table shows the structure of the ecosystem:\n",
    "\n",
    "| Package                           | Purpose                                                                                                                              | Example Usage                                                     |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------- |\n",
    "| **ðŸ¤— Hub (`huggingface_hub`)**    | Central repository for models, datasets, and Spaces. Lets you push/pull models and datasets.                                         | Upload a fine-tuned model to share with others.                   |\n",
    "| **Transformers (`transformers`)** | High-level library with pretrained NLP, vision, and multimodal models. Handles training, inference, and tokenization (via wrappers). | `pipeline(\"sentiment-analysis\")`                                  |\n",
    "| **Tokenizers (`tokenizers`)**     | Fast, low-level text tokenization library (written in Rust). Often used inside `transformers`.                                       | Train a custom WordPiece/BPE tokenizer.                           |\n",
    "| **Datasets (`datasets`)**         | Efficient dataset loading, processing, and streaming. Optimized for large ML datasets.                                               | Load `imdb` dataset in one line: `datasets.load_dataset(\"imdb\")`. |\n",
    "| **Evaluate (`evaluate`)**         | Standardized evaluation metrics library. Works well with `datasets` and `transformers`.                                              | Compute accuracy, F1, BLEU, etc.                                  |\n",
    "| **Diffusers (`diffusers`)**       | Library for diffusion models (e.g., Stable Diffusion) for images, audio, video.                                                      | Generate images from text prompts.                                |\n",
    "| **Accelerate (`accelerate`)**     | Utility for running training on any hardware setup (CPU, GPU, multi-GPU, TPU) with minimal code changes.                             | Scale PyTorch model training across GPUs.                         |\n",
    "| **PEFT (`peft`)**                 | Parameter-Efficient Fine-Tuning library (LoRA, adapters, etc.) for large models.                                                     | Fine-tune a 13B LLM on a laptop GPU.                              |\n",
    "| **Optimum (`optimum`)**           | Optimizations for transformers (ONNX, quantization, hardware-specific acceleration).                                                 | Run models faster on Intel/NVIDIA chips.                          |\n",
    "| **Gradio (`gradio`)** (partnered) | Simple UI framework to demo models in the browser.                                                                                   | Deploy a model demo in a few lines.                               |\n",
    "\n",
    "\n",
    "Check the set of tutorials: \n",
    "- [LLM course](https://huggingface.co/learn/llm-course/chapter0/1) from hugging face.\n",
    "- The [transformers](transformers.ipynb) package, provided by the Hugging Face, allows you to use a variety of popular transformer-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61485bb0",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark is a framework for processing large amounts of data. This section covers its Python SDK.\n",
    "\n",
    "Some configuration is required to start experimenting with Spark in local mode:\n",
    "\n",
    "- `pip3 install pyspark`: for spark instalation.\n",
    "- Install java: `openjdk-17-jdk` package in `apt`. Set path to the jdk to the `$JAVA_HOME` variable. In ubuntu case `export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbb28f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you have completed the specified configuration correctly, you will be able to run the script below, which creates a local `SparkContext` - way to experiment with spark without any clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d154d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/22 15:13:04 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.37.58 instead (on interface wlp0s20f3)\n",
      "25/08/22 15:13:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/22 15:13:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setMaster(\"local\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756a9ad",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Spark actually operates with Resilient Distributed Datasets (RDDs), but I'll use the term \"dataset\", as this section mosttly focuses on the syntax of `pyspark`, not actual its features associated with the ability to distribute the computations. Check more details on [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e433b16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates how to create an RDD from a simple python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633eaafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.core.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sc.parallelize([1, 2, 3, 4, 5])\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aec035",
   "metadata": {},
   "source": [
    "## Sentence transformer\n",
    "\n",
    "The sentence transformer package implements models for building embeddings from sets of texts. Check [SBERT](https://sbert.net/) page for mode details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cb764",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider a basic example of using the `sentence_transformers` package.\n",
    "\n",
    "The following cell loads the model and displays the type. It's a special object that build to privide specific interfaces associated with building embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbe2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_transformers.SentenceTransformer.SentenceTransformer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a7fcf",
   "metadata": {},
   "source": [
    "The obtained object have an `encode` method - that takes a range of texts and returns `numpy.array` of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85442a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01919573,  0.12008536,  0.15959828, ..., -0.0053629 ,\n",
       "        -0.08109505,  0.05021338],\n",
       "       [-0.01869039,  0.04151868,  0.07431544, ...,  0.00486597,\n",
       "        -0.06190442,  0.03187514],\n",
       "       [ 0.136502  ,  0.08227322, -0.02526165, ...,  0.08762047,\n",
       "         0.03045845, -0.01075752]], shape=(3, 384), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99d079",
   "metadata": {},
   "source": [
    "The following cell uses the `similarity` method to create a matrix of the embeddings' similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead365c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6660, 0.1046],\n",
       "        [0.6660, 1.0000, 0.1411],\n",
       "        [0.1046, 0.1411, 1.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = model.similarity(embeddings, embeddings)\n",
    "similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
