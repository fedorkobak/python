{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d548bd96",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "The DS/ML section discusses the python packages/frameworks specialised for building database systems and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e7dad",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "Huggingface is an ecosystem of packages that are related to all aspects of working with deep learning objects.\n",
    "\n",
    "The first thing you need to do is log in:\n",
    "\n",
    "`huggingface-cli login --token <your HF token>`\n",
    "\n",
    "The following table shows the structure of the ecosystem:\n",
    "\n",
    "| Package                           | Purpose                                                                                                                              |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **🤗 Hub (`huggingface_hub`)**    | Central repository for models, datasets, and Spaces. Lets you push/pull models and datasets.                                         |\n",
    "| **Transformers (`transformers`)** | High-level library with pretrained NLP, vision, and multimodal models. Handles training, inference, and tokenization (via wrappers). |\n",
    "| **Tokenizers (`tokenizers`)**     | Fast, low-level text tokenization library (written in Rust). Often used inside `transformers`.                                       |\n",
    "| **Datasets (`datasets`)**         | Efficient dataset loading, processing, and streaming. Optimized for large ML datasets.                                               |\n",
    "| **Evaluate (`evaluate`)**         | Standardized evaluation metrics library. Works well with `datasets` and `transformers`.                                              |\n",
    "| **Diffusers (`diffusers`)**       | Library for diffusion models (e.g., Stable Diffusion) for images, audio, video.                                                      |\n",
    "| **Accelerate (`accelerate`)**     | Utility for running training on any hardware setup (CPU, GPU, multi-GPU, TPU) with minimal code changes.                             |\n",
    "| **PEFT (`peft`)**                 | Parameter-Efficient Fine-Tuning library (LoRA, adapters, etc.) for large models.                                                     |\n",
    "| **Optimum (`optimum`)**           | Optimizations for transformers (ONNX, quantization, hardware-specific acceleration).                                                 |\n",
    "| **Smollagents (`smolagents`)**    | Building agentic systems.                                                                                                            |\n",
    "| **Gradio (`gradio`)** (partnered) | Simple UI framework to demo models in the browser.                                                                                   |\n",
    "\n",
    "Find out more: \n",
    "\n",
    "- [LLM course](https://huggingface.co/learn/llm-course/chapter0/1) from hugging face.\n",
    "- The [Hugging Face](hugging_face.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61485bb0",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark is a framework for processing large amounts of data. This section covers its Python SDK.\n",
    "\n",
    "Some configuration is required to start experimenting with Spark in local mode:\n",
    "\n",
    "- `pip3 install pyspark`: for spark instalation.\n",
    "- Install java: `openjdk-17-jdk` package in `apt`. Set path to the jdk to the `$JAVA_HOME` variable. In ubuntu case `export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbb28f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you have completed the specified configuration correctly, you will be able to run the script below, which creates a local `SparkContext` - way to experiment with spark without any clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d154d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/19 09:04:44 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/19 09:04:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/19 09:04:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setMaster(\"local\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180e3a6",
   "metadata": {},
   "source": [
    "### Context and session\n",
    "\n",
    "There are two ways to interact with the Spark provided by the PySpark:\n",
    "\n",
    "- Spark Context: is a low-level API for manipulating with computational resources provided by Spark.\n",
    "- Spark Session: is built on top of the SparkContext tool to implement the way users interact with SparkSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4375cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell provides an example of how to create a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14e3ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Temp\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbbd31",
   "metadata": {},
   "source": [
    "**Note:** The Spark Session still has a SparkContext under the hood. The following cell shows that the session's `sparkContext` attribute is the same as the context created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d589dacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext is sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756a9ad",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Spark actually operates with Resilient Distributed Datasets (RDDs), but I'll use the term \"dataset\", as this section mosttly focuses on the syntax of `pyspark`, not actual its features associated with the ability to distribute the computations. Check more details on [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e433b16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates how to create an RDD from a simple python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633eaafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.core.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sc.parallelize([1, 2, 3, 4, 5])\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48dfdea",
   "metadata": {},
   "source": [
    "## Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce5db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 08:55:20 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: path/to/data.json.\n",
      "java.io.FileNotFoundException: File path/to/data.json does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/user/Documents/code/python/ds_ml/path/to/data.json. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m spark = SparkSession.builder \\\n\u001b[32m      5\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mMy Spark Application\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .getOrCreate()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Now you can use the 'spark' object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath/to/data.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvironments/python/lib/python3.13/site-packages/pyspark/sql/readwriter.py:468\u001b[39m, in \u001b[36mDataFrameReader.json\u001b[39m\u001b[34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers, useUnsafeRow)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvironments/python/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvironments/python/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/home/user/Documents/code/python/ds_ml/path/to/data.json. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Explicitly creating the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aec035",
   "metadata": {},
   "source": [
    "## Sentence transformer\n",
    "\n",
    "The sentence transformer package implements models for building embeddings from sets of texts. Check [SBERT](https://sbert.net/) page for mode details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cb764",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider a basic example of using the `sentence_transformers` package.\n",
    "\n",
    "The following cell loads the model and displays the type. It's a special object that build to privide specific interfaces associated with building embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbe2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_transformers.SentenceTransformer.SentenceTransformer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a7fcf",
   "metadata": {},
   "source": [
    "The obtained object have an `encode` method - that takes a range of texts and returns `numpy.array` of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85442a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01919573,  0.12008536,  0.15959828, ..., -0.0053629 ,\n",
       "        -0.08109505,  0.05021338],\n",
       "       [-0.01869039,  0.04151868,  0.07431544, ...,  0.00486597,\n",
       "        -0.06190442,  0.03187514],\n",
       "       [ 0.136502  ,  0.08227322, -0.02526165, ...,  0.08762047,\n",
       "         0.03045845, -0.01075752]], shape=(3, 384), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99d079",
   "metadata": {},
   "source": [
    "The following cell uses the `similarity` method to create a matrix of the embeddings' similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead365c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6660, 0.1046],\n",
       "        [0.6660, 1.0000, 0.1411],\n",
       "        [0.1046, 0.1411, 1.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = model.similarity(embeddings, embeddings)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddf043",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "The Lang chain is the core library for developing modern, agent-based solutions. The following table lists and describes the central components of the lang chain package.\n",
    "\n",
    "| Component | Analogy | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **Models** | The brains | These are the core language models (LLMs) that handle the actual work, like generating text, holding conversations, or creating embeddings. |\n",
    "| **Prompts** | The instructions | These are the templates used to provide specific instructions and context to the models. They ensure the model responds in a consistent and desired format. |\n",
    "| **Chains** | The workflow | A way to link multiple components together into a single, automated sequence. This allows you to perform multi-step tasks, like combining a prompt with a model call. |\n",
    "| **Agents** | The reasoning engine | A more advanced chain that uses an LLM to decide which external **Tools** to use to achieve a goal. It can think, act, and observe, repeating the process until the task is complete. |\n",
    "| **Tools** | The external capabilities | These are functionalities an agent can use to interact with the world. Examples include a search engine, a calculator, or a database lookup. |\n",
    "\n",
    "\n",
    "Check more in [Lang Chain](langchain.ipynb) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc7d44",
   "metadata": {},
   "source": [
    "## MCP SDK\n",
    "\n",
    "There is an MCP SDK for python. It is provided by the `mcp[cli]` package.\n",
    "\n",
    "Define the assign a server object using the `mcp.server.fastmcp.FastMCP` class. Use decorators: `tool`, `resource`, `prompt`, and `sampling` to wrap the funcitons that implement the corresponding facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c3667",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the following cell we will consider how to run the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ee76b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting intro_files/mcp_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile intro_files/mcp_server.py\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Some service\")\n",
    "\n",
    "@mcp.tool()\n",
    "def some_tool(inp: str) -> str:\n",
    "    return f\"Output of some tool for {inp}.\"\n",
    "\n",
    "mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d992570",
   "metadata": {},
   "source": [
    "Run your server using the command `mcp dev intro_files/mcp_server.py`. The following cell runs the server from python using `os.system` command to demonstrate the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f94c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MCP inspector...\n",
      "⚙️ Proxy server listening on localhost:6277\n",
      "🔑 Session token: 4c21942ece36a04554ee01562067c5b129c3b03eaa945ead9d0b8964d9334fe8\n",
      "   Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth\n",
      "\n",
      "🚀 MCP Inspector is up and running at:\n",
      "   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=4c21942ece36a04554ee01562067c5b129c3b03eaa945ead9d0b8964d9334fe8\n",
      "\n",
      "🌐 Opening browser...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"mcp dev intro_files/mcp_server.py &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a069ac",
   "metadata": {},
   "source": [
    "**Note:** To use an inspector tool, you must install `npm` on your system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
