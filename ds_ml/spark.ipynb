{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90eb301c",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "This page considers the python SDK for Spark. For more information, check out the[PySpark Overveiew](https://spark.apache.org/docs/latest/api/python/index.html) tutorial on the official website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a579cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/23 11:20:27 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/23 11:20:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/23 11:20:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_session = SparkSession.builder.appName('Temp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f34cb2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Some configuration is required to start experimenting with Spark in local mode:\n",
    "\n",
    "- `pip3 install pyspark`: for spark instalation.\n",
    "- Install java: `openjdk-17-jdk` package in `apt`. Set path to the jdk to the `$JAVA_HOME` variable. In ubuntu case `export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575ef73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you have completed the specified configuration correctly, you will be able to run the script below, which creates a local `SparkContext` - way to experiment with spark without any clusters.\n",
    "\n",
    "**Spark Context**: is a low-level API for manipulating with computational resources provided by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/19 09:04:44 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/19 09:04:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/19 09:04:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setMaster(\"local\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3bf76",
   "metadata": {},
   "source": [
    "### Session\n",
    "\n",
    "**Spark Session**: is built on top of the SparkContext tool to implement the way users interact with SparkSQL.\n",
    "\n",
    "The following list shows the different functions that allow manipulation of the session lifecycle:\n",
    "\n",
    "- The `SparkSession.builder.getOrCreate()` creates the session.\n",
    "- The `SparkSession.getActiveSession()` to get the active session, it will return `None` if there is no active session.\n",
    "- The `stop` method allows you to stop the current session. **Note.** Spark does not allow to keep two sessions on the same JVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a4cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell illustrates an example of how to create a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c509cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Temp\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176d1d",
   "metadata": {},
   "source": [
    "After that, `SparkSession.getActiveSession` returns an object representing the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd849d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = SparkSession.getActiveSession()\n",
    "type(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c80aa8",
   "metadata": {},
   "source": [
    "After calling `stop` from the Spark session the `getActiveSession` returns just `None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7572fa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()\n",
    "SparkSession.getActiveSession() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c7510",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "\n",
    "Spark SQL contains a DataFrame objects that provide a way to interact with tabular data.\n",
    "\n",
    "You can define a data frame: \n",
    "\n",
    "- Directly from your code using the `createDataFrame` method of the session object.\n",
    "- Using some special methods to read from external sources stored in the `read` attribute of the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e04fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the Spark dataset, which is formatted so that each row is a tuple whose values correspond to each column. And shows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbc857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Cathy| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 35)]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e085c",
   "metadata": {},
   "source": [
    "The following cell shows an alternative way to define the same data frame. Each row here is represented as a dictionary, and the values are specified under the keys, which correesponds to the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a830849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25|Alice|\n",
      "| 30|  Bob|\n",
      "| 35|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        {\"name\": \"Alice\", \"age\": 25},\n",
    "        {\"name\": \"Bob\", \"age\": 30},\n",
    "        {\"name\": \"Cathy\", \"age\": 35}\n",
    "    ]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9290d",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "\n",
    "Spark is a typical tool for building ETL pipelines, which include cyclical improvements through the process of saving and loading data. More over spark have special tools for data versioning.\n",
    "\n",
    "For more details check:\n",
    "- A comprehensive [Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html) guide in the corresponding page. \n",
    "- [Data Sources](spark/data_sources.ipynb) for more practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ba425",
   "metadata": {},
   "source": [
    "The Spark data frame contains the `write` interface, for saving data into the storage. The following table lists important methods of the interface.\n",
    "\n",
    "| Method | Description | Primary Use Case | Key Features |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`save()`** | Writes the DataFrame to a file system path. You specify the format (e.g., Parquet, CSV, JSON). | Simple, file-based persistence of data. | Creates **unmanaged** data; Spark does not track its metadata. Dropping the table (if you create one) does not delete the files. |\n",
    "| **`csv()`, `json()`, `parquet()`** | Writes the DataFrame to a file system path, in a corresponding format |  Simple, file-based persistence of data. | Creates **unmanaged** data; Spark does not track its metadata. Dropping the table (if you create one) does not delete the files. |\n",
    "| **`parquet()`** | A specific and highly-recommended way to save data in the Parquet format. | High-performance, schema-aware storage for analytics. | **Columnar storage**, automatic schema preservation, and efficient compression. This is the **default** for `save()`. |\n",
    "| **`saveAsTable()`** | Creates a **managed table** in the Hive Metastore. | Creating a named table for easy querying with Spark SQL. | Spark manages both the data and its metadata. Dropping the table deletes both the catalog entry and the data files. |\n",
    "| **`jdbc()`** | Writes the DataFrame to a relational database using a JDBC connection. | Storing data in a traditional database for transactional or reporting purposes. | Requires a JDBC driver and connection string. Allows you to specify table names, modes, and connection properties. |\n",
    "| **`format('...').save()`** | A more generic way to save data, explicitly specifying the format and path. | When using a format that isn't a dedicated method (e.g., Avro, ORC). | Gives you full control over the data source format. Also used for setting format-specific options. |\n",
    "| **`partitionBy()`** | A method to partition the data on disk based on one or more columns. | Optimizing queries that frequently filter on specific columns. | Creates subdirectories for each unique value of the specified partition column(s), significantly speeding up read operations. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a0cb6",
   "metadata": {},
   "source": [
    "The Spark data frame implements the `read` interface, which has the following specific methods:\n",
    "\n",
    "| Method | Description | Primary Use Case | Key Options |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`load()`** | A generic method to read data from a source. You must explicitly specify the format. | Reading from a data source when you need full control over the format and options. | `format()`, `path()`, and any format-specific options (e.g., `header`). It defaults to Parquet if no format is specified. |\n",
    "| **`parquet()`** | A dedicated, highly-optimized method for reading Parquet files. | Loading high-performance data that was previously saved by Spark. | This method automatically infers the schema from the Parquet file's metadata, so it requires fewer options. |\n",
    "| **`csv()`** | Loads data from CSV files. | Reading human-readable, simple-structured data where the schema is not embedded. | **`header=True`** (to use the first row as column names) and **`inferSchema=True`** (to automatically detect data types). |\n",
    "| **`json()`** | Loads data from line-delimited JSON files. | Reading semi-structured data from web logs, APIs, or data dumps. | The method automatically infers the schema, but you can provide a schema to avoid inference. |\n",
    "| **`jdbc()`** | Connects to and reads data from a relational database table. | Loading data from a traditional database for ETL or analysis. | Requires a `url`, `table`, and `driver` string. You can also specify `partitionColumn`, `lowerBound`, and `upperBound` for parallel reads. |\n",
    "| **`table()`** | Reads a managed table from the Spark/Hive Metastore. | Reading a table that was previously created using `df.write.saveAsTable()`. | This is the easiest way to load data since you only need the table name. Spark handles locating the data and its schema automatically. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1ee29",
   "metadata": {},
   "source": [
    "It's also important to know that Spark's most native data sources its *spark SQL catalog*, which in different configuratoins can be different. Possible options are:\n",
    "\n",
    "- Just folder that stores all necesarry files.\n",
    "- Hive storage.\n",
    "- Delta Lake.\n",
    "- Apahce Iceberg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea257ea",
   "metadata": {},
   "source": [
    "## Columns\n",
    "\n",
    "Data frame consists of a set of columns. There are two concepts important to know for refering the columns:\n",
    "\n",
    "- There are corresponding attibute of the data frame.\n",
    "- The `pyspark.sql.functions.col` allows you to define a reference to a column, when applied to a particular dataset, will be interpreted as a specific column in that dataset.\n",
    "\n",
    "The following table shows the typical use cases in which you may be required to reference a column.\n",
    "\n",
    "| Category         | Method / Operator                  | Example                          | Description |\n",
    "|------------------|------------------------------------|----------------------------------|-------------|\n",
    "| **Comparison**   | `==`, `!=`, `>`, `<`, `>=`, `<=`   | `col(\"age\") > 18`               | Compares column values. Returns a boolean column. |\n",
    "| **Boolean**      | `&`, `|`, `~`                     | `(col(\"age\") > 18) & (col(\"vip\") == True)` | Logical AND (`&`), OR (`|`), and NOT (`~`). |\n",
    "| **Arithmetic**   | `+`, `-`, `*`, `/`, `%`           | `(col(\"price\") * col(\"qty\"))`   | Arithmetic operations between columns or with literals. |\n",
    "| **Aliasing**     | `.alias(name)`                    | `col(\"age\").alias(\"user_age\")`  | Renames the column in the resulting DataFrame. |\n",
    "| **Casting**      | `.cast(dataType)`                 | `col(\"age\").cast(\"string\")`     | Changes the column type. |\n",
    "| **Null Handling**| `.isNull()`, `.isNotNull()`       | `col(\"name\").isNotNull()`       | Tests for `NULL` values. |\n",
    "| **String Ops**   | `.contains()`, `.startswith()`, `.endswith()` | `col(\"name\").contains(\"Al\")` | String matching and filtering. |\n",
    "| **Math**         | (via `pyspark.sql.functions`)      | `sqrt(col(\"value\"))`            | Use functions like `abs`, `log`, `sqrt`, `exp`, `pow`. |\n",
    "| **Aggregation**  | (via `pyspark.sql.functions`)      | `sum(col(\"value\"))`             | Use `avg`, `min`, `max`, `sum`, `count`. |\n",
    "| **Conditional**  | (via `when`)                      | `when(col(\"age\") > 18, \"adult\")`| Build conditional expressions. |\n",
    "| **Window Ops**   | (via `over`)                      | `row_number().over(windowSpec)` | Used for ranking, lead/lag, etc. |\n",
    "| **Collection**   | `.getItem(index)`                 | `col(\"array_col\").getItem(0)`   | Access element of array column. |\n",
    "| **Struct Access**| `.getField(name)`                 | `col(\"struct_col\").getField(\"x\")` | Access field of struct column. |\n",
    "\n",
    "\n",
    "Spark uses these references to the columns when performing operations like: `withColumn` and `filter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76726723",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the data frame that will be used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db655689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|      8|     20|\n",
      "|      9|     43|\n",
      "|     15|     88|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (8, 20),\n",
    "        (9, 43),\n",
    "        (15, 88)\n",
    "    ],\n",
    "    schema=[\"column1\", \"column2\"]\n",
    ")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911232d",
   "metadata": {},
   "source": [
    "The following cell apply the `filter` with the condition specified using a direct reference to the `test_df.column1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ebcd675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.classic.column.Column'>\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|     15|     88|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condition = (test_df.column1 > 10)\n",
    "print(type(condition))\n",
    "test_df.filter(condition=condition).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5eab8c",
   "metadata": {},
   "source": [
    "Alternatively, the next cell specifies the `calculation` using the abstract `column2`. However, the `withColumn` function of the `test_df` interprets it as a reference to `column2` it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|column1|column2|result|\n",
      "+-------+-------+------+\n",
      "|      8|     20|    28|\n",
      "|      9|     43|    51|\n",
      "|     15|     88|    96|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculation = col(\"column2\") + 8\n",
    "test_df.withColumn(\"result\", calculation).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f1216",
   "metadata": {},
   "source": [
    "## Computations\n",
    "\n",
    "There are two methods for performing computations on a data frame:\n",
    "\n",
    "- `withColumn`: allows to specify the result column, and a computation that would be used to produce values.\n",
    "- `selectExr`: allows you to specify the operations under the columns using SQL syntax to produce the new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7691875",
   "metadata": {},
   "source": [
    "### With column\n",
    "\n",
    "\n",
    "The dataframe object provides a `withColumn` method to operate with columns. You are supposed to provide:\n",
    "- The name of the column in which the result should be srored. If the column doesn't exists, it will be created in output dataframe.\n",
    "- The column object or computational expression that defines the new column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4689df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates the data frame that we will use for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "625ae5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|numbers|strings|\n",
      "+-------+-------+\n",
      "|      8| value1|\n",
      "|      9| value2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (8, \"value1\"),\n",
    "        (9, \"value2\")\n",
    "    ],\n",
    "    schema=[\"numbers\", \"strings\"]\n",
    ")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72477250",
   "metadata": {},
   "source": [
    "The following code modifies the example data frame by using `withColumn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d94d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|numbers|strings|\n",
      "+-------+-------+\n",
      "|     98| value1|\n",
      "|     99| value2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.withColumn(\n",
    "    \"numbers\",\n",
    "    col(\"numbers\") + 90\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488b716",
   "metadata": {},
   "source": [
    "### Select expression\n",
    "\n",
    "The Spark DataFrame has a `selectExpr` method that allows you to build a new data frame just by specifying columns of the result data using SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec7ed3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines and displays the data frame that will be used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|score|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|   10|\n",
      "|  2|    Bob|   20|\n",
      "|  3|Charlie|   30|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    [(1, \"Alice\", 10), (2, \"Bob\", 20), (3, \"Charlie\", 30)],\n",
    "    [\"id\", \"name\", \"score\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4b03a",
   "metadata": {},
   "source": [
    "The following cell demonstrates the use of the `selectExpr` method in the following patterns: performing multiplication on a constand, producing a boolean value, and performing an operation on two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+-------------+------------+\n",
      "| id|   name|double_score|is_high_score|(id + score)|\n",
      "+---+-------+------------+-------------+------------+\n",
      "|  1|  Alice|          20|        false|          11|\n",
      "|  2|    Bob|          40|         true|          22|\n",
      "|  3|Charlie|          60|         true|          33|\n",
      "+---+-------+------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"score * 2 as double_score\",\n",
    "    \"score > 15 as is_high_score\",\n",
    "    \"id + score\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f35fab",
   "metadata": {},
   "source": [
    "## Group by\n",
    "\n",
    "The data frame contains the `groupBy` method method, which returns a special `GroupedData` object. This object contains a set of tools for building an aggregations over the data:\n",
    "\n",
    "| Method             | Description                                       |\n",
    "| ------------------ | ------------------------------------------------- |\n",
    "| `agg` | General aggregation with one or more expressions. |\n",
    "| `avg`    | Computes the average of the given columns.        |\n",
    "| `mean`   | Alias for `avg()`.                                |\n",
    "| `max`    | Maximum value for each column.                    |\n",
    "| `min`    | Minimum value for each column.                    |\n",
    "| `sum`    | Sum of values for each column.                    |\n",
    "| `count`  | Count of rows for each group.                     |\n",
    "| `pivot` | Performs a pivot (like SQL `PIVOT`) on the specified column, turning its values into new columns. |\n",
    "| `applyInPandas` | Apply a function to each group as a Pandas DataFrame and return a new DataFrame.             |\n",
    "| `apply`         | Apply a user-defined function to each group (returns an RDD, not a DataFrame â€” less common). |\n",
    "\n",
    "Check more details in the [`groupby`](spark/groupby.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8df74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines an example data frame. It constructs and shows the `GroupedData` object based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f096b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [group], value: [group: string, value: bigint], type: GroupBy]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (\"a\", 3),\n",
    "        (\"a\", 2),\n",
    "        (\"c\", 4),\n",
    "        (\"c\", 7)\n",
    "    ],\n",
    "    schema=['group', 'value']\n",
    ")\n",
    "\n",
    "grouped_expression = test_df.groupBy('group')\n",
    "grouped_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79d4af",
   "metadata": {},
   "source": [
    "The following code shows how to use the `agg` function to compute the aggregations based on the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cecfb36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+----------+\n",
      "|group|sum(value)|avg(value)|min(value)|max(value)|\n",
      "+-----+----------+----------+----------+----------+\n",
      "|    a|         5|       2.5|         2|         3|\n",
      "|    c|        11|       5.5|         4|         7|\n",
      "+-----+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "grouped_expression.agg(\n",
    "    sum('value'),\n",
    "    avg('value'),\n",
    "    min('value'),\n",
    "    max('value')\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
