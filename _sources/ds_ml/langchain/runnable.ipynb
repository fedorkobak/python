{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09e0c2f-835f-4214-b152-fb8fd750721e",
   "metadata": {},
   "source": [
    "# Runnable\n",
    "\n",
    "A Runnable is a LangChain concept representing an object with `invoke`, `stream` and `batch` methods. Runnables can sometimes can be combined.\n",
    "\n",
    "The base object for runnables in langchain is `langchain_core.runnables.base.Runnable`.\n",
    "\n",
    "Check the [Runnable](https://reference.langchain.com/python/langchain_core/runnables/) api reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1897ac-ab31-4e99-96cd-0ab8d0131202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfce2c-a0c9-49f3-8634-ba7ad66ed97d",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "The `invoke` method triggers the request to LLM.\n",
    "\n",
    "In most cases, it returns an `AIMessage`, but in some special cases, it can return some special output. For example, structured output langchain object return `dict` or `Pydantic.BaseModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645b2eb-5f1b-4e86-a088-27b78eefe999",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the kind of object `langchain_core.language_models.BaseChatModel` heir returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327d4ccd-5cbe-4531-9658-04f8f04140ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2026-01-20T14:41:12.602107097Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5657624355, 'load_duration': 5473528405, 'prompt_eval_count': 12, 'prompt_eval_duration': 27060628, 'eval_count': 10, 'eval_duration': 140510213, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--019bdbda-2b7d-7e22-afef-f1d811475446-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1d904-727c-4b08-86bb-21a9bd1ad39b",
   "metadata": {},
   "source": [
    "## Stream\n",
    "\n",
    "The `stream` method enables the outptu from the model to be obtained incrementally. It returns an `AIMessageChunk` containing the sequential output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a862d6b-35da-4f8d-b293-b9600b63a02f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell illustrates the process of gradual generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05cc1aad-00f1-4668-bf77-9162c805b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The|capital|of|France|is|Paris|.|||"
     ]
    }
   ],
   "source": [
    "iterations = model.stream(\"What is the capital of France?\")\n",
    "for i in iterations:\n",
    "    sleep(0.2)\n",
    "    print(i.content.strip(), end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb18b3-e68e-42e3-b58d-a4e803133891",
   "metadata": {},
   "source": [
    "The message chunk example is shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e727e3cd-d475-47a9-a9e0-46f626da6271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='A', additional_kwargs={}, response_metadata={}, id='lc_run--019bdbe8-7ac6-7690-982c-81f9712d4061', tool_calls=[], invalid_tool_calls=[], tool_call_chunks=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations = model.stream(\"What is the capital of GB?\")\n",
    "next(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4361c-338e-47a8-a7ea-6ffb04fd0066",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "You can modify the behaviour of your Runnable using a callback. Extend the `langchain_core.callbacks.BaseCallbackHandler` with special methods and specify that Runnable have to use the callback handler during the infvocation in the \"callbacks\" attribute of the `config`.\n",
    "\n",
    "The following table shows some of the methods that can be defined in the callback handler.\n",
    "\n",
    "|      Method      |           Triggered When...          |                  Useful For...                 |\n",
    "| :--------------: | :----------------------------------: | :--------------------------------------------: |\n",
    "|  on_chain_start  |      A Chain (or Agent) begins.      |              Logging user inputs.              |\n",
    "|   on_llm_start   |  The LLM API is about to be called.  | Inspecting the exact prompt sent to the model. |\n",
    "|   on_llm_end     |  The LLM API returned the result.    | Logging the information about model outputs.   |\n",
    "| on_llm_new_token | A token streams in (streaming only). |      Real-time UI updates (typing effect).     |\n",
    "|   on_tool_start  |    An Agent decides to use a tool.   |     Debugging which tools are being picked.    |\n",
    "|  on_chain_error  |         An exception happens.        | Capturing stack traces or alerting developers. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5850547-e718-4842-8b11-2cb9ef3a4f4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the `MyCustomTracer`, which extends the behaviour of the runnable associated with invoking LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7408fa-92a7-4541-bb54-0bf8701d2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "\n",
    "class MyCustomTracer(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"\\n[MY-TRACER] LLM Started with prompt: {prompts[0][:50]}...\")\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        content = response.generations[0][0].text\n",
    "        print(f\"[MY-TRACER] LLM Finished. Output: {content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc36af5-bc3c-4655-998c-1016d6b28b12",
   "metadata": {},
   "source": [
    "The following cell illustrates the invocation of the model with attached callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb66a91-88fe-4d81-9bf4-80019834aba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MY-TRACER] LLM Started with prompt: Human: Hello, I'm Fedor...\n",
      "[MY-TRACER] LLM Finished. Output: Nice to meet you, Fedor! Is there something I can ...\n"
     ]
    }
   ],
   "source": [
    "ans = model.invoke(\n",
    "    \"Hello, I'm Fedor\",\n",
    "    config={\"callbacks\": [MyCustomTracer()]}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
