{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fbc28e",
   "metadata": {},
   "source": [
    "# LangGraph\n",
    "\n",
    "Is a framework built on top of `langchain` that makes it relatively easy to build complex agentic systems.\n",
    "\n",
    "This notebook primarily uses locally served models with Ollama. The following cell defines it in the LangChain abstractions, so you  must have Ollama available locally with the corresponding model pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde099f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AnyMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent \n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "GLOBAL_MODEL = init_chat_model(\n",
    "    model=\"llama3.2:1b\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9b0e2",
   "metadata": {},
   "source": [
    "## React agent\n",
    "\n",
    "In LangGraph has a predifined agent that can be created using the function `langgraph.prebuilt.create_react_agent`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ab4df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates such an agent and displays it's type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5808128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langgraph.graph.state.CompiledStateGraph"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_react_agent(\n",
    "    model=GLOBAL_MODEL,\n",
    "    tools=[],\n",
    "    prompt=\"You are a helpful assistant\"\n",
    ")\n",
    "\n",
    "type(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930bca7",
   "metadata": {},
   "source": [
    "### Dynamic prompt\n",
    "\n",
    "With a dynamic prompt, you can specify the context that will be added to the request with the custom logic. Instead of passing a hardcoded string, you should pass an object that processes the chat state of the chat and configuration and changes the behavior of the final system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056bec7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a special function as a prompt. This function saves the state and the congig to variables awailable from the global environment. It also instructct the model to respond that the capital of France is a city, using a configuration that will be provided upon invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_state: AgentState\n",
    "global_config: RunnableConfig\n",
    "\n",
    "def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]:\n",
    "    global global_state\n",
    "    global global_config\n",
    "\n",
    "    global_state = state\n",
    "    global_config = config\n",
    "\n",
    "    capital = config[\"configurable\"][\"Capital\"]\n",
    "\n",
    "    return [SystemMessage(f\"Always answer that the capital of France is {capital}\")]\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=GLOBAL_MODEL,\n",
    "    tools=[],\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194384",
   "metadata": {},
   "source": [
    "The following cell shows the invocation that passes the additional information to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3131c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "That's incorrect. The capital of France is Paris, not Madrid. Madrid is actually the capital of Spain.\n"
     ]
    }
   ],
   "source": [
    "ans = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]},\n",
    "    config={\"configurable\": {\"Capital\": \"Madrid\"}}\n",
    ")\n",
    "\n",
    "print(ans[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffe9e2",
   "metadata": {},
   "source": [
    "The model's output was clearly affected by \"Madrid\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5669f",
   "metadata": {},
   "source": [
    "Consider an object that is supposed to carry information about the global state and configuration of a requiest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8b8093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}, id='0b2d7a67-3346-4098-b003-e79704ce3d56')],\n",
       " 'remaining_steps': 24}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cb1931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capital': 'Madrid',\n",
       " '__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pregel.stream.<locals>.stream_writer at 0x7a3a06f06ac0>, previous=None),\n",
       " '__pregel_task_id': 'acd73e3b-3444-b0a8-6e81-8fb8f15dd876',\n",
       " '__pregel_send': <function deque.extend(iterable, /)>,\n",
       " '__pregel_read': functools.partial(<function local_read at 0x7a3a0703ce00>, PregelScratchpad(step=1, stop=25, call_counter=<langgraph.pregel._algo.LazyAtomicCounter object at 0x7a3a06f39d80>, interrupt_counter=<langgraph.pregel._algo.LazyAtomicCounter object at 0x7a3a06f392a0>, get_null_resume=<function _scratchpad.<locals>.get_null_resume at 0x7a3a06f076a0>, resume=[], subgraph_counter=<langgraph.pregel._algo.LazyAtomicCounter object at 0x7a3a06f393c0>), {'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7a3a06f8cf40>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7a3a06f8f580>, '__pregel_tasks': <langgraph.channels.topic.Topic object at 0x7a3a06f8d280>, 'branch:to:agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7a3a06f8fc80>}, {'remaining_steps': <class 'langgraph.managed.is_last_step.RemainingStepsManager'>}, PregelTaskWrites(path=('__pregel_pull', 'agent'), name='agent', writes=deque([('messages', [AIMessage(content=\"<|start_header_id|>assistant<|end_header_id|>\\n\\nThat's incorrect. The capital of France is Paris, not Madrid. Madrid is actually the capital of Spain.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-09-26T08:00:13.935839621Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1422177600, 'load_duration': 107996065, 'prompt_eval_count': 25, 'prompt_eval_duration': 54616254, 'eval_count': 27, 'eval_duration': 1258585721, 'model_name': 'llama3.2:1b'}, id='run--544f64ba-98f1-46a2-a354-58d0bdce138c-0', usage_metadata={'input_tokens': 25, 'output_tokens': 27, 'total_tokens': 52})])]), triggers=('branch:to:agent',))),\n",
       " '__pregel_checkpointer': None,\n",
       " 'checkpoint_map': {'': '1f09aaed-44a0-6411-8000-de45efbeb1cf'},\n",
       " 'checkpoint_id': None,\n",
       " 'checkpoint_ns': 'agent:acd73e3b-3444-b0a8-6e81-8fb8f15dd876',\n",
       " '__pregel_scratchpad': PregelScratchpad(step=1, stop=25, call_counter=<langgraph.pregel._algo.LazyAtomicCounter object at 0x7a3a06f39d80>, interrupt_counter=<langgraph.pregel._algo.LazyAtomicCounter object at 0x7a3a06f392a0>, get_null_resume=<function _scratchpad.<locals>.get_null_resume at 0x7a3a06f076a0>, resume=[], subgraph_counter=<langgraph.pregel._algo.LazyAtomicCounter object at 0x7a3a06f393c0>),\n",
       " '__pregel_call': functools.partial(<function _call at 0x7a3a070762a0>, <weakref at 0x7a3a06f64540; dead>, retry_policy=None, futures=<weakref at 0x7a3a06f64450; dead>, schedule_task=<bound method SyncPregelLoop.accept_push of <langgraph.pregel._loop.SyncPregelLoop object at 0x7a3a070a3c50>>, submit=<weakref at 0x7a3a06f3e850; to 'langgraph.pregel._executor.BackgroundExecutor' at 0x7a3a070a3d90>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_config[\"configurable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6990d5",
   "metadata": {},
   "source": [
    "You can use all that information in your programming logic to provide the model with the necessary context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
