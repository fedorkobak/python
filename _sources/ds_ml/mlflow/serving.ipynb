{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d0f54",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "MLFlow allows to serve models that are registered. This page looks at the relevant tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5192e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from multiprocessing import Process\n",
    "\n",
    "mlflow_path = \"/tmp/mlflow_serving\"\n",
    "\n",
    "!rm -rf $mlflow_path\n",
    "mlflow.set_tracking_uri(\"file://\" + mlflow_path)\n",
    "mlflow.set_registry_uri(\"file://\" + mlflow_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22d83b",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "The `mlflow` command-line interface allows you to start an http server that deploys the specified model. The following table shows parameters for the `mlflow models serve` command that allows to run the server.\n",
    "\n",
    "| Option                  | Description                                                                                    |\n",
    "| ----------------------- | -----------------------------------------------------------------------------------------------|\n",
    "| `-m, --model-uri <URI>` | Path or URI of the model to serve (local path, S3, GCS, DBFS, registry URI).                   |\n",
    "| `-p, --port <PORT>`     | Port to serve the model on (default: `5000`).                                                  |\n",
    "| `-h, --host <HOST>`     | Host address to bind (default: `127.0.0.1`). Use `0.0.0.0` to make it accessible externally.   |\n",
    "| `--no-conda`            | Prevents creation of a new conda environment; runs in the current environment.                 |\n",
    "| `--env-manager`         | Controls how the serving environment is created (default: `conda`).                            |\n",
    "| `--enable-mlserver`     | Use **MLServer** backend instead of the default gunicorn/waitress server (for better scaling). |\n",
    "| `--workers <N>`         | Number of worker processes to handle requests (only for `gunicorn` on Unix).                   |\n",
    "| `--install-mlflow`      | Reinstalls MLflow in the serving environment (useful if it’s missing).                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27004e38",
   "metadata": {},
   "source": [
    "## Docker\n",
    "\n",
    "Use `mlflow models build-docker` interface to pack the model as a Docker image. The following table shows the important arguments:\n",
    "\n",
    "| Option                    | Description                                                                      |\n",
    "| ------------------------- | -------------------------------------------------------------------------------- |\n",
    "| `-m, --model-uri <URI>`   | Path or URI of the model to include in the Docker image.                         |\n",
    "| `-n, --name <IMAGE_NAME>` | Name of the resulting Docker image.                                              |\n",
    "| `-b, --build <flavor>`    | Choose which model flavor to build (`python_function`, `crate`, etc.).           |\n",
    "| `--enable-mlserver`       | Use **MLServer** as the serving backend instead of the default.                  |\n",
    "| `--install-mlflow`        | Ensures MLflow is installed in the image (sometimes required for compatibility). |\n",
    "| `--env-manager` | Specifies how dependencies should be managed inside the image.                             |\n",
    "| `--platform <PLATFORM>`   | Target platform for multi-arch builds (e.g., `linux/amd64`, `linux/arm64`).      |\n",
    "| `--no-cache`              | Do not use Docker’s build cache.                                                 |\n",
    "| `--build-arg KEY=VALUE`   | Pass custom build arguments to `docker build`.                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb923c00",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "To run the model from python, use `mlflow.models.flavor_backend_registry.get_flavor_backend`, which returns a special backend object that can start a server via the `serve` method.\n",
    "\n",
    "**Note.** As the server holds the Python process that it run, to run another Jupyter cell, run the server in a child process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388605c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell registers the simple model object in the mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d987438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'model'.\n",
      "Created version '1' of model 'model'.\n"
     ]
    }
   ],
   "source": [
    "@mlflow.pyfunc.utils.pyfunc\n",
    "def model(model_input: list[float]) -> list[float]:\n",
    "    return [x * 2 for x in model_input]\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"model\",\n",
    "        python_model=model,\n",
    "        registered_model_name=\"model\",\n",
    "        pip_requirements=[]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852b96a",
   "metadata": {},
   "source": [
    "The next code shows how to start the server as a separate Python process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1a37ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/24 16:15:56 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2025/10/24 16:15:56 INFO mlflow.pyfunc.backend: === Running command 'exec uvicorn --host localhost --port 1234 --workers 1 mlflow.pyfunc.scoring_server.app:app'\n",
      "INFO:     Started server process [362307]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:1234 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "model_uri = \"models:/model/1\"\n",
    "\n",
    "def run_model_serve():\n",
    "    backend = mlflow.models.flavor_backend_registry.get_flavor_backend(\n",
    "        model_uri=model_uri,\n",
    "        env_manager=\"local\"\n",
    "    )\n",
    "\n",
    "    backend.serve(\n",
    "        model_uri=model_uri,\n",
    "        port=1234,\n",
    "        host=\"localhost\",\n",
    "        timeout=60,\n",
    "        enable_mlserver=False\n",
    "    )\n",
    "\n",
    "process = Process(target=run_model_serve)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6edcfe",
   "metadata": {},
   "source": [
    "Invocation of the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2de61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50100 - \"POST /invocations HTTP/1.1\" 200 OK\n",
      "{\"predictions\": [2.0, 4.0, 6.0]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:1234/invocations\"\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\"inputs\": [1.0, 2.0, 3.0]}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7215e9f2",
   "metadata": {},
   "source": [
    "As expected, the API returns the inputs multiplicated by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935e1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [362307]\n"
     ]
    }
   ],
   "source": [
    "process.terminate()\n",
    "process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
