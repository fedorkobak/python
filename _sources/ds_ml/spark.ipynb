{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90eb301c",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "This page considers the python SDK for Spark. For more information, check out the[PySpark Overveiew](https://spark.apache.org/docs/latest/api/python/index.html) tutorial on the official website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a579cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/23 11:20:27 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/23 11:20:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/23 11:20:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_session = SparkSession.builder.appName('Temp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f34cb2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Some configuration is required to start experimenting with Spark in local mode:\n",
    "\n",
    "- `pip3 install pyspark`: for spark instalation.\n",
    "- Install java: `openjdk-17-jdk` package in `apt`. Set path to the jdk to the `$JAVA_HOME` variable. In ubuntu case `export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575ef73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you have completed the specified configuration correctly, you will be able to run the script below, which creates a local `SparkContext` - way to experiment with spark without any clusters.\n",
    "\n",
    "**Spark Context**: is a low-level API for manipulating with computational resources provided by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/19 09:04:44 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/09/19 09:04:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/19 09:04:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setMaster(\"local\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3bf76",
   "metadata": {},
   "source": [
    "### Session\n",
    "\n",
    "**Spark Session**: is built on top of the SparkContext tool to implement the way users interact with SparkSQL.\n",
    "\n",
    "The following list shows the different functions that allow manipulation of the session lifecycle:\n",
    "\n",
    "- The `SparkSession.builder.getOrCreate()` creates the session.\n",
    "- The `SparkSession.getActiveSession()` to get the active session, it will return `None` if there is no active session.\n",
    "- The `stop` method allows you to stop the current session. **Note.** Spark does not allow to keep two sessions on the same JVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a4cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell illustrates an example of how to create a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c509cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Temp\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176d1d",
   "metadata": {},
   "source": [
    "After that, `SparkSession.getActiveSession` returns an object representing the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd849d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = SparkSession.getActiveSession()\n",
    "type(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c80aa8",
   "metadata": {},
   "source": [
    "After calling `stop` from the Spark session the `getActiveSession` returns just `None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7572fa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()\n",
    "SparkSession.getActiveSession() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c7510",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "\n",
    "Spark SQL contains a DataFrame objects that provide a way to interact with tabular data.\n",
    "\n",
    "You can define a data frame: \n",
    "\n",
    "- Directly from your code using the `createDataFrame` method of the session object.\n",
    "- Using some special methods to read from external sources stored in the `read` attribute of the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e04fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the Spark dataset, which is formatted so that each row is a tuple whose values correspond to each column. And shows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbc857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Cathy| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 35)]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e085c",
   "metadata": {},
   "source": [
    "The following cell shows an alternative way to define the same data frame. Each row here is represented as a dictionary, and the values are specified under the keys, which correesponds to the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a830849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25|Alice|\n",
      "| 30|  Bob|\n",
      "| 35|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        {\"name\": \"Alice\", \"age\": 25},\n",
    "        {\"name\": \"Bob\", \"age\": 30},\n",
    "        {\"name\": \"Cathy\", \"age\": 35}\n",
    "    ]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9290d",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "\n",
    "Spark is a typical tool for building ETL pipelines, which include cyclical improvements through the process of saving and loading data. More over spark have special tools for data versioning.\n",
    "\n",
    "For more details check:\n",
    "- A comprehensive [Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html) guide in the corresponding page. \n",
    "- [Data Sources](spark/data_sources.ipynb) for more practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ba425",
   "metadata": {},
   "source": [
    "The Spark data frame contains the `write` interface, for saving data into the storage. The following table lists important methods of the interface.\n",
    "\n",
    "| Method | Description | Primary Use Case | Key Features |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`save()`** | Writes the DataFrame to a file system path. You specify the format (e.g., Parquet, CSV, JSON). | Simple, file-based persistence of data. | Creates **unmanaged** data; Spark does not track its metadata. Dropping the table (if you create one) does not delete the files. |\n",
    "| **`csv()`, `json()`, `parquet()`** | Writes the DataFrame to a file system path, in a corresponding format |  Simple, file-based persistence of data. | Creates **unmanaged** data; Spark does not track its metadata. Dropping the table (if you create one) does not delete the files. |\n",
    "| **`parquet()`** | A specific and highly-recommended way to save data in the Parquet format. | High-performance, schema-aware storage for analytics. | **Columnar storage**, automatic schema preservation, and efficient compression. This is the **default** for `save()`. |\n",
    "| **`saveAsTable()`** | Creates a **managed table** in the Hive Metastore. | Creating a named table for easy querying with Spark SQL. | Spark manages both the data and its metadata. Dropping the table deletes both the catalog entry and the data files. |\n",
    "| **`jdbc()`** | Writes the DataFrame to a relational database using a JDBC connection. | Storing data in a traditional database for transactional or reporting purposes. | Requires a JDBC driver and connection string. Allows you to specify table names, modes, and connection properties. |\n",
    "| **`format('...').save()`** | A more generic way to save data, explicitly specifying the format and path. | When using a format that isn't a dedicated method (e.g., Avro, ORC). | Gives you full control over the data source format. Also used for setting format-specific options. |\n",
    "| **`partitionBy()`** | A method to partition the data on disk based on one or more columns. | Optimizing queries that frequently filter on specific columns. | Creates subdirectories for each unique value of the specified partition column(s), significantly speeding up read operations. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a0cb6",
   "metadata": {},
   "source": [
    "The Spark data frame implements the `read` interface, which has the following specific methods:\n",
    "\n",
    "| Method | Description | Primary Use Case | Key Options |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`load()`** | A generic method to read data from a source. You must explicitly specify the format. | Reading from a data source when you need full control over the format and options. | `format()`, `path()`, and any format-specific options (e.g., `header`). It defaults to Parquet if no format is specified. |\n",
    "| **`parquet()`** | A dedicated, highly-optimized method for reading Parquet files. | Loading high-performance data that was previously saved by Spark. | This method automatically infers the schema from the Parquet file's metadata, so it requires fewer options. |\n",
    "| **`csv()`** | Loads data from CSV files. | Reading human-readable, simple-structured data where the schema is not embedded. | **`header=True`** (to use the first row as column names) and **`inferSchema=True`** (to automatically detect data types). |\n",
    "| **`json()`** | Loads data from line-delimited JSON files. | Reading semi-structured data from web logs, APIs, or data dumps. | The method automatically infers the schema, but you can provide a schema to avoid inference. |\n",
    "| **`jdbc()`** | Connects to and reads data from a relational database table. | Loading data from a traditional database for ETL or analysis. | Requires a `url`, `table`, and `driver` string. You can also specify `partitionColumn`, `lowerBound`, and `upperBound` for parallel reads. |\n",
    "| **`table()`** | Reads a managed table from the Spark/Hive Metastore. | Reading a table that was previously created using `df.write.saveAsTable()`. | This is the easiest way to load data since you only need the table name. Spark handles locating the data and its schema automatically. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1ee29",
   "metadata": {},
   "source": [
    "It's also important to know that Spark's most native data sources its *spark SQL catalog*, which in different configuratoins can be different. Possible options are:\n",
    "\n",
    "- Just folder that stores all necesarry files.\n",
    "- Hive storage.\n",
    "- Delta Lake.\n",
    "- Apahce Iceberg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea257ea",
   "metadata": {},
   "source": [
    "## Columns\n",
    "\n",
    "Data frame consists of a set of columns. There are two concepts important to know for refering the columns:\n",
    "\n",
    "- There are corresponding attibute of the data frame.\n",
    "- The `pyspark.sql.functions.col` allows you to define a reference to a column, when applied to a particular dataset, will be interpreted as a specific column in that dataset.\n",
    "\n",
    "The following table shows the typical use cases in which you may be required to reference a column.\n",
    "\n",
    "| Category         | Method / Operator                  | Example                          | Description |\n",
    "|------------------|------------------------------------|----------------------------------|-------------|\n",
    "| **Comparison**   | `==`, `!=`, `>`, `<`, `>=`, `<=`   | `col(\"age\") > 18`               | Compares column values. Returns a boolean column. |\n",
    "| **Boolean**      | `&`, `\\|`, `~`                     | `(col(\"age\") > 18) & (col(\"vip\") == True)` | Logical AND (`&`), OR (`\\|`), and NOT (`~`). |\n",
    "| **Arithmetic**   | `+`, `-`, `*`, `/`, `%`           | `(col(\"price\") * col(\"qty\"))`   | Arithmetic operations between columns or with literals. |\n",
    "| **Aliasing**     | `.alias(name)`                    | `col(\"age\").alias(\"user_age\")`  | Renames the column in the resulting DataFrame. |\n",
    "| **Casting**      | `.cast(dataType)`                 | `col(\"age\").cast(\"string\")`     | Changes the column type. |\n",
    "| **Null Handling**| `.isNull()`, `.isNotNull()`       | `col(\"name\").isNotNull()`       | Tests for `NULL` values. |\n",
    "| **String Ops**   | `.contains()`, `.startswith()`, `.endswith()` | `col(\"name\").contains(\"Al\")` | String matching and filtering. |\n",
    "| **Math**         | (via `pyspark.sql.functions`)      | `sqrt(col(\"value\"))`            | Use functions like `abs`, `log`, `sqrt`, `exp`, `pow`. |\n",
    "| **Aggregation**  | (via `pyspark.sql.functions`)      | `sum(col(\"value\"))`             | Use `avg`, `min`, `max`, `sum`, `count`. |\n",
    "| **Conditional**  | (via `when`)                      | `when(col(\"age\") > 18, \"adult\")`| Build conditional expressions. |\n",
    "| **Window Ops**   | (via `over`)                      | `row_number().over(windowSpec)` | Used for ranking, lead/lag, etc. |\n",
    "| **Collection**   | `.getItem(index)`                 | `col(\"array_col\").getItem(0)`   | Access element of array column. |\n",
    "| **Struct Access**| `.getField(name)`                 | `col(\"struct_col\").getField(\"x\")` | Access field of struct column. |\n",
    "\n",
    "\n",
    "Spark uses these references to the columns when performing operations like: `withColumn` and `filter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76726723",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the data frame that will be used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db655689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|      8|     20|\n",
      "|      9|     43|\n",
      "|     15|     88|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (8, 20),\n",
    "        (9, 43),\n",
    "        (15, 88)\n",
    "    ],\n",
    "    schema=[\"column1\", \"column2\"]\n",
    ")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911232d",
   "metadata": {},
   "source": [
    "The following cell apply the `filter` with the condition specified using a direct reference to the `test_df.column1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ebcd675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.classic.column.Column'>\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|     15|     88|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condition = (test_df.column1 > 10)\n",
    "print(type(condition))\n",
    "test_df.filter(condition=condition).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5eab8c",
   "metadata": {},
   "source": [
    "Alternatively, the next cell specifies the `calculation` using the abstract `column2`. However, the `withColumn` function of the `test_df` interprets it as a reference to `column2` it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|column1|column2|result|\n",
      "+-------+-------+------+\n",
      "|      8|     20|    28|\n",
      "|      9|     43|    51|\n",
      "|     15|     88|    96|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculation = col(\"column2\") + 8\n",
    "test_df.withColumn(\"result\", calculation).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f1216",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "This section consdiers general data transformation accessible in PySpark:\n",
    "\n",
    "- The `withColumn`: allows to specify the result column, and a computation that would be used to produce values.\n",
    "- The `selectExr`: allows you to specify the operations under the columns using SQL syntax to produce the new ones.\n",
    "- The `na` attribute gathers methods that specify transformation how to handle missing values: `na.fill`, `na.drop` and `na.replace`.\n",
    "- The `replace` method allows to specify which values to replace.\n",
    "\n",
    "Check [Transformations](spark/transformations.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f35fab",
   "metadata": {},
   "source": [
    "## Group by\n",
    "\n",
    "The data frame contains the `groupBy` method method, which returns a special `GroupedData` object. This object contains a set of tools for building an aggregations over the data:\n",
    "\n",
    "| Method             | Description                                       |\n",
    "| ------------------ | ------------------------------------------------- |\n",
    "| `agg` | General aggregation with one or more expressions. |\n",
    "| `avg`    | Computes the average of the given columns.        |\n",
    "| `mean`   | Alias for `avg()`.                                |\n",
    "| `max`    | Maximum value for each column.                    |\n",
    "| `min`    | Minimum value for each column.                    |\n",
    "| `sum`    | Sum of values for each column.                    |\n",
    "| `count`  | Count of rows for each group.                     |\n",
    "| `pivot` | Performs a pivot (like SQL `PIVOT`) on the specified column, turning its values into new columns. |\n",
    "| `applyInPandas` | Apply a function to each group as a Pandas DataFrame and return a new DataFrame.             |\n",
    "| `apply`         | Apply a user-defined function to each group (returns an RDD, not a DataFrame — less common). |\n",
    "\n",
    "Check more details in the [`groupby`](spark/groupby.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8df74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines an example data frame. It constructs and shows the `GroupedData` object based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f096b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [group], value: [group: string, value: bigint], type: GroupBy]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = spark_session.createDataFrame(\n",
    "    data=[\n",
    "        (\"a\", 3),\n",
    "        (\"a\", 2),\n",
    "        (\"c\", 4),\n",
    "        (\"c\", 7)\n",
    "    ],\n",
    "    schema=['group', 'value']\n",
    ")\n",
    "\n",
    "grouped_expression = test_df.groupBy('group')\n",
    "grouped_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79d4af",
   "metadata": {},
   "source": [
    "The following code shows how to use the `agg` function to compute the aggregations based on the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cecfb36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+----------+\n",
      "|group|sum(value)|avg(value)|min(value)|max(value)|\n",
      "+-----+----------+----------+----------+----------+\n",
      "|    a|         5|       2.5|         2|         3|\n",
      "|    c|        11|       5.5|         4|         7|\n",
      "+-----+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "grouped_expression.agg(\n",
    "    sum('value'),\n",
    "    avg('value'),\n",
    "    min('value'),\n",
    "    max('value')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269b72b",
   "metadata": {},
   "source": [
    "## ML\n",
    "\n",
    "There is a `pyspark.ml` package that contains a set of tools that implements a typical ml transformations. The following table provides an overwiew of its capabilities:\n",
    "\n",
    "| Module / Class          | Description                                                                 |\n",
    "|------------------------|-----------------------------------------------------------------------------|\n",
    "| **Pipeline**           | Combines multiple stages (transformers + estimators) into a single workflow.|\n",
    "| **Transformer**        | An object that transforms a DataFrame (e.g., feature scaling).              |\n",
    "| **Estimator**          | An object that fits a model from data and returns a Transformer.            |\n",
    "| **Evaluator**          | Computes metrics to evaluate model performance.                            |\n",
    "| **tuning**             | Tools for hyperparameter tuning (`CrossValidator`, `TrainValidationSplit`).|\n",
    "| **feature**            | Feature engineering utilities (e.g., `StringIndexer`, `VectorAssembler`).   |\n",
    "| **classification**     | Classification algorithms (e.g., `LogisticRegression`, `RandomForest`).    |\n",
    "| **regression**         | Regression algorithms (e.g., `LinearRegression`, `GBTRegressor`).          |\n",
    "| **clustering**         | Clustering algorithms (e.g., `KMeans`, `GaussianMixture`).                 |\n",
    "| **recommendation**     | Collaborative filtering (e.g., `ALS` for matrix factorization).            |\n",
    "| **stat**               | Statistical functions (e.g., `ChiSquareTest`, `Correlation`).              |\n",
    "\n",
    "For more details check:\n",
    "- [Machine Learning Library Guide](https://spark.apache.org/docs/latest/ml-guide.html) in spark documentation.\n",
    "- [ML](spark/ml.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f8dce",
   "metadata": {},
   "source": [
    "## Declarative Pipelines\n",
    "\n",
    "Spark Declarative Pipelines is facility to build a data pipelines in spark. Check more in the [Spark Declarative Pipelines Programming Guide](https://spark.apache.org/docs/4.1.0-preview1/declarative-pipelines-programming-guide.html).\n",
    "\n",
    "You can create different spark structures based on the outputs of the functions wrapped in the corresponding decorator.\n",
    "\n",
    "| Decorcator          | Description                           |\n",
    "|---------------------|---------------------------------------|\n",
    "| `materialized_view` | Creates a materilized view            |\n",
    "| `temporary_view`    | Creates a temporary view              |\n",
    "| `table`             | Creates a streaming table             |\n",
    "| `appned_flow`       | Flows to appending data to the target |\n",
    "\n",
    "The `spark-pipelines` command is used to manage the pipelines:\n",
    "\n",
    "- The `spark-pipelines init --name <name>` to initialise the project.\n",
    "- The `spark-pipelines run` to run the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ed0ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the process of creating and running Spark's declarative pipes.\n",
    "\n",
    "The following cell creates a folder and initializes the project within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c87b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/01 09:15:01 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/10/01 09:15:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline project 'my_first_pipeline' created successfully. To run your pipeline:\n",
      "cd 'my_first_pipeline'\n",
      "spark-pipelines run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 09:15:04 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/10/01 09:15:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bbe0540-0ad6-418d-88af-185649eb6aef\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf /tmp/spark_declarative_pipe\n",
    "mkdir /tmp/spark_declarative_pipe\n",
    "cd /tmp/spark_declarative_pipe\n",
    "spark-pipelines init --name my_first_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42eeeb",
   "metadata": {},
   "source": [
    "The following cell represents the structure of the generated project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f444247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/spark_declarative_pipe\u001b[0m\n",
      "└── \u001b[01;34mmy_first_pipeline\u001b[0m\n",
      "    ├── pipeline.yml\n",
      "    └── \u001b[01;34mtransformations\u001b[0m\n",
      "        ├── example_python_materialized_view.py\n",
      "        └── example_sql_materialized_view.sql\n",
      "\n",
      "3 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/spark_declarative_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa72ee1",
   "metadata": {},
   "source": [
    "The `pipeline.yml` defines the behaviour of the pipes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758db6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: my_first_pipeline\n",
      "libraries:\n",
      "  - glob:\n",
      "      include: transformations/**\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/spark_declarative_pipe/my_first_pipeline/pipeline.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d98cc",
   "metadata": {},
   "source": [
    "THe following cell represents an example of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960d08cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark import pipelines as dp\n",
      "from pyspark.sql import DataFrame, SparkSession\n",
      "\n",
      "spark = SparkSession.active()\n",
      "\n",
      "@dp.materialized_view\n",
      "def example_python_materialized_view() -> DataFrame:\n",
      "    return spark.range(10)\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/spark_declarative_pipe/my_first_pipeline/transformations/example_python_materialized_view.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e8e66",
   "metadata": {},
   "source": [
    "This is a script generates a table with numbers from 0 to 9. With name corresponding to the name of the wrapped function: `example_python_materialized_view`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dada3",
   "metadata": {},
   "source": [
    "The next code runs the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e83a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/01 09:21:29 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/10/01 09:21:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-01 09:21:31: Loading pipeline spec from /tmp/spark_declarative_pipe/my_first_pipeline/pipeline.yml...\n",
      "2025-10-01 09:21:31: Creating Spark session...\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/01 09:21:33 WARN Utils: Your hostname, user-ThinkPad-E16-Gen-2, resolves to a loopback address: 127.0.1.1; using 10.202.22.210 instead (on interface enp0s31f6)\n",
      "25/10/01 09:21:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/01 09:21:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/user/.virtualenvironments/python/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/connect/conf.py:64: UserWarning: Failed to set spark.sql.catalogImplementation to Some(in-memory) due to [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.sql.catalogImplementation\".\n",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110\n",
      "2025-10-01 09:21:37: Creating dataflow graph...\n",
      "2025-10-01 09:21:38: Registering graph elements...\n",
      "2025-10-01 09:21:38: Loading definitions. Root directory: '/tmp/spark_declarative_pipe/my_first_pipeline'.\n",
      "2025-10-01 09:21:38: Found 2 files matching glob 'transformations/**/*'\n",
      "2025-10-01 09:21:38: Registering SQL file /tmp/spark_declarative_pipe/my_first_pipeline/transformations/example_sql_materialized_view.sql...\n",
      "2025-10-01 09:21:38: Importing /tmp/spark_declarative_pipe/my_first_pipeline/transformations/example_python_materialized_view.py...\n",
      "2025-10-01 09:21:38: Starting run...\n",
      "2025-10-01 07:21:39: Flow spark_catalog.default.example_python_materialized_view is QUEUED.\n",
      "2025-10-01 07:21:39: Flow spark_catalog.default.example_sql_materialized_view is QUEUED.\n",
      "2025-10-01 07:21:39: Flow spark_catalog.default.example_python_materialized_view is PLANNING.\n",
      "2025-10-01 07:21:39: Flow spark_catalog.default.example_python_materialized_view is STARTING.\n",
      "2025-10-01 07:21:39: Flow spark_catalog.default.example_python_materialized_view is RUNNING.\n",
      "2025-10-01 07:21:41: Flow spark_catalog.default.example_python_materialized_view has COMPLETED.\n",
      "2025-10-01 07:21:42: Flow spark_catalog.default.example_sql_materialized_view is PLANNING.\n",
      "2025-10-01 07:21:42: Flow spark_catalog.default.example_sql_materialized_view is STARTING.\n",
      "2025-10-01 07:21:42: Flow spark_catalog.default.example_sql_materialized_view is RUNNING.\n",
      "2025-10-01 07:21:43: Flow spark_catalog.default.example_sql_materialized_view has COMPLETED.\n",
      "2025-10-01 07:21:44: Run is COMPLETED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/01 09:21:45 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/10/01 09:21:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-f82ca8cb-3e75-42fb-ba08-0d4b7b65a10f\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /tmp/spark_declarative_pipe/my_first_pipeline\n",
    "spark-pipelines run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe8134",
   "metadata": {},
   "source": [
    "As the result `spark-warehouse` folder is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c8c6946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/spark_declarative_pipe\u001b[0m\n",
      "└── \u001b[01;34mmy_first_pipeline\u001b[0m\n",
      "    ├── pipeline.yml\n",
      "    ├── \u001b[01;34mspark-warehouse\u001b[0m\n",
      "    │   ├── \u001b[01;34mexample_python_materialized_view\u001b[0m\n",
      "    │   │   ├── part-00000-183d4b9e-e121-4d6b-849c-a320e7e99683-c000.snappy.parquet\n",
      "    │   │   └── _SUCCESS\n",
      "    │   └── \u001b[01;34mexample_sql_materialized_view\u001b[0m\n",
      "    │       ├── part-00000-5ecdee8c-bcb8-4e9e-a7ef-38dd8f6d9dfd-c000.snappy.parquet\n",
      "    │       └── _SUCCESS\n",
      "    └── \u001b[01;34mtransformations\u001b[0m\n",
      "        ├── example_python_materialized_view.py\n",
      "        ├── example_sql_materialized_view.sql\n",
      "        └── \u001b[01;34m__pycache__\u001b[0m\n",
      "            └── example_python_materialized_view.cpython-313.pyc\n",
      "\n",
      "7 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/spark_declarative_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68cee5",
   "metadata": {},
   "source": [
    "The following cell load the table generated by the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "532dd61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4\n",
       "5   5\n",
       "6   6\n",
       "7   7\n",
       "8   8\n",
       "9   9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet('/tmp/spark_declarative_pipe/my_first_pipeline/spark-warehouse/example_python_materialized_view')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
