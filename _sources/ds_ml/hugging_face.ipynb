{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f10b03",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "This page discusses various aspects of the Hugging Face infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bc600",
   "metadata": {},
   "source": [
    "## Hub\n",
    "\n",
    "The `huggingface_hub` package allows you to interact with the huggingface infrastructure, which offers different tools for versioning and inferencing of the machine learning models.\n",
    "\n",
    "| Functionality             | Usage Reference                                   | Notes |\n",
    "|----------------------------|---------------------------------------------------|-------|\n",
    "| **Authentication**         | `huggingface_hub.login`                           | Log in with your token. |\n",
    "|                            | `huggingface_hub.whoami`                          | Check current user. |\n",
    "| **Repo Management**        | `huggingface_hub.HfApi.create_repo`               | Create repo for models/datasets/spaces. |\n",
    "|                            | `huggingface_hub.HfApi.delete_repo`               | Delete repo. |\n",
    "| **File Upload / Download** | `huggingface_hub.upload_file`                     | Upload a single file. |\n",
    "|                            | `huggingface_hub.download_file`                   | Download a single file. |\n",
    "| **Snapshot Download**      | `huggingface_hub.snapshot_download`               | Download entire repo (cached locally). |\n",
    "| **Commits & Revisions**    | `huggingface_hub.HfApi.create_commit`             | Commit files with git-like semantics. |\n",
    "|                            | `huggingface_hub.CommitOperationAdd`              | Add file in a commit. |\n",
    "| **Search / Listing**       | `huggingface_hub.HfApi.list_models`               | Search models. |\n",
    "|                            | `huggingface_hub.HfApi.list_datasets`             | Search datasets. |\n",
    "| **Model / Dataset Info**   | `huggingface_hub.HfApi.model_info`                | Get model metadata. |\n",
    "|                            | `huggingface_hub.HfApi.dataset_info`              | Get dataset details. |\n",
    "| **Inference API**          | `huggingface_hub.InferenceClient`                 | Run inference via HF servers. |\n",
    "| **Spaces Management**      | `huggingface_hub.HfApi.restart_space`             | Restart a Space. |\n",
    "|                            | `huggingface_hub.HfApi.request_space_hardware`    | Request hardware upgrade. |\n",
    "| **Utilities**              | `huggingface_hub.hf_hub_url`                      | Get raw file URL. |\n",
    "|                            | `huggingface_hub.scan_cache_dir`                  | Inspect local HF cache. |\n",
    "\n",
    "Check more:\n",
    "- [Hub python library](https://huggingface.co/docs/huggingface_hub/index) page.\n",
    "- [Hub](hugging_face/hub.ipynb) page for more detailed description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5942",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "`transformers` is python package that allows you to use pre-trained machine learning models that belong to the transformers architecture.\n",
    "\n",
    "| Component                   | Description                                                                                 |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Models**                  | Pretrained architectures for tasks like classification, generation, or embeddings.          |\n",
    "| **Tokenizers**              | Convert text into numerical input for models; handle batching, padding, truncation.         |\n",
    "| **Pipelines**               | High-level API combining tokenizer + model for a specific task (e.g., `summarization`).     |\n",
    "| **Configurations**          | Define model hyperparameters and architecture settings (e.g., `BertConfig`).                |\n",
    "| **Trainer**                 | High-level training API handling loops, evaluation, logging, and checkpointing.             |\n",
    "| **Schedulers & Optimizers** | Learning rate schedulers and optimizer integrations for training models.                    |\n",
    "| **Data Utilities**          | Helpers for preprocessing and batching (e.g., `DataCollator`, `BatchEncoding`).             |\n",
    "| **Hub Integration**         | Download/upload pretrained models from Hugging Face Hub (`from_pretrained`, `push_to_hub`). |\n",
    "\n",
    "Check more details in the [transformers](hugging_face/transformers.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6c121",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "The Datasets is package from Hugging Face's infrastructure that manages data and implements tools for loading and prcessing data, regardless of its modality.\n",
    "\n",
    "You can define your own datasets, as well as load ready datatsets form the [datasets section of hugging face hub](https://huggingface.co/datasets).\n",
    "\n",
    "The core elements of the package are:\n",
    "\n",
    "- Primitives to keep data:\n",
    "    - `Dataset` is a map of features, each of which is a array of elements.\n",
    "    - `DatasetDict` is a map of datasets.\n",
    "- `load_dataset`: method for loading datasets.\n",
    "- Modalities: package build to be able to process all kinds of data, so it implements subpackages:\n",
    "    - `Audio`: for working with audio data.\n",
    "    - `Image`: for working with image data.\n",
    "    - `Text`: for working with text data.\n",
    "    - `Video`: for working with video data.\n",
    "- Processing methods:\n",
    "    - `map`: method for applying a function to each element of a dataset.\n",
    "    - `filter`: method for filtering a dataset.\n",
    "    - `train_test_split`: method for splitting a dataset into training and testing sets.\n",
    "    - `sort`: method for sorting a dataset.\n",
    "    - `shuffle`: method for shuffling a dataset.\n",
    "- Other utilities:\n",
    "    - `arrow_writer`: class for writing data to an Arrow file.\n",
    "    - `Csv`: class for working with CSV files.\n",
    "    - `Json`: class for working with JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504a996",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the type of object you encounter when you start using the `datasets` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e2f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9144c",
   "metadata": {},
   "source": [
    "The following cell displays the load of the `lhoestq/demo1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a9e0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_dataset = load_dataset(\"lhoestq/demo1\")\n",
    "demo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2c299",
   "metadata": {},
   "source": [
    "At the top level, it is separated into test/train using `DatasetDict` abstraction of the datasets. Access the specific section of data by using the corresponding key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a960be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779b92f",
   "metadata": {},
   "source": [
    "Consider the following example with audio domain dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0888110b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "    num_rows: 563\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050ef26",
   "metadata": {},
   "source": [
    "It isn't separated into test/train; `load_dataset` returns dataset directly. The following cell shows the features avaialble in the dataset under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b420f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': Value('string'),\n",
       " 'audio': Audio(sampling_rate=8000, decode=True, stream_index=None),\n",
       " 'transcription': Value('string'),\n",
       " 'english_transcription': Value('string'),\n",
       " 'intent_class': ClassLabel(names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan', 'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill']),\n",
       " 'lang_id': ClassLabel(names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a194d",
   "metadata": {},
   "source": [
    "The `audio` feature has a  `Audio` datatype due to the specifics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6b3a5",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "A package that implements different tokenization approaches and related tools.\n",
    "\n",
    "| Component          | Description                                                                       |\n",
    "| ------------------ | --------------------------------------------------------------------------------- |\n",
    "| **PreTokenizers**  | Split text into initial units (words, punctuation, subwords) before encoding.     |\n",
    "| **Models**         | Define the algorithm for tokenization (BPE, WordPiece, SentencePiece, Unigram).   |\n",
    "| **Normalizers**    | Clean and standardize text (lowercasing, accent stripping, punctuation handling). |\n",
    "| **Trainers**       | Learn tokenization vocabulary from a dataset.                                     |\n",
    "| **Decoders**       | Convert token IDs back to readable text.                                          |\n",
    "| **Processors**     | Post-process tokenized output (e.g., adding special tokens like `[CLS]`).         |\n",
    "| **Batch Encoding** | Handle batch tokenization with padding, truncation, and attention masks.          |\n",
    "\n",
    "Check:\n",
    "\n",
    "- [Documentation](https://huggingface.co/docs/tokenizers/en/index) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492d4ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the most essential components that are typically used with the `tokenizers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9c11e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06dd30",
   "metadata": {},
   "source": [
    "Pre-tokenization is the initial separation of the text into smaller units, providing an upper bound on the number of tokens you expect to receive. The main algorithm is often statistical, so the output is not limited by any strict rule. A pre-tokenizer, in contrast, uses a deterministic algorithm, so its results are predictable. The following cell shows the application of the simplest whitespace pre-tokenizer to the preceding sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e63613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Some', (0, 4)), ('test', (5, 9)), ('text', (10, 14))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokinizer = Whitespace()\n",
    "pretokinizer.pre_tokenize_str(\"Some test text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00499d2",
   "metadata": {},
   "source": [
    "The `tokenizers.Tokenizer` class, is tool for interacting with a tokenizer. It takes a model that defines the exact approach to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f275cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pretokinizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0ffc4",
   "metadata": {},
   "source": [
    "The trainer class is another component of the whole system, and it defines some parameters. The following cell shows the training of the tokenizer defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(vocab_size=20)\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    [\n",
    "        \"some super check\",\n",
    "        \"super some check\"\n",
    "    ],\n",
    "    trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81352ea3",
   "metadata": {},
   "source": [
    "The following cell shows the vocabulary of the final tokenizer. Each token has an ID that will be used after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3c45e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some': 18,\n",
       " 'per': 16,\n",
       " 'ck': 11,\n",
       " 'h': 2,\n",
       " 'u': 9,\n",
       " 'me': 14,\n",
       " 'o': 5,\n",
       " 'er': 12,\n",
       " 'p': 6,\n",
       " 'r': 7,\n",
       " 'c': 0,\n",
       " 'k': 3,\n",
       " 'ch': 10,\n",
       " 's': 8,\n",
       " 'e': 1,\n",
       " 'check': 19,\n",
       " 'ome': 15,\n",
       " 'm': 4,\n",
       " 'su': 17,\n",
       " 'eck': 13}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c581acb",
   "metadata": {},
   "source": [
    "Here is a result of a transformation for a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "102f44dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'r', 'some', 'check']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"start some check\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de8a35",
   "metadata": {},
   "source": [
    "## Smolagents\n",
    "\n",
    "The `smalagents` package is a tooling for building agents. The following table lists core components that you're supposed to use:\n",
    "\n",
    "| Component | Description|\n",
    "|-----------|------------|\n",
    "| `CodeAgent` | A class that incapsulates the agent's logic |\n",
    "| `Tool`      | A base class for tools. There are some built in tools like: `ApiWebSearch`, `VisitWebpageTool`, `PythonInterpreterTool` etc. |\n",
    "| `@tool`     | A decorator that wraps functions that are supposed to be used as tools |\n",
    "| `Model`     | A base class for the interfaces that provide different ways to access the decision-making model: `InferenceClientModel`, `TransformersModel`, etc. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13911231",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the following example. Suppose you want to make model awailable to perform a really specific transformation, with which the model is unfamiliar. In this case, we'll consider the \"Kobak transformation\".\n",
    "\n",
    "The following cell asks the raw model to perform the \"Kobak transformation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56099fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kobak transformation is not a standard text transformation technique that I'm familiar with. It's possible that you might be referring to a specific context or a custom transformation method. Could you please provide more details or clarify the definition of the Kobak transformation? This will help me give you an accurate and helpful response.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "output = client.chat_completion(\n",
    "    [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Apply a Kobak transformation on the text 'Hello, World!'\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(output.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee913ad",
   "metadata": {},
   "source": [
    "The model indicates that it is not familiar with the type of the required transformation. We can implement the transformation using python code and provide the code to the model as a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49d513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Perforam a kobak transformation on the text 'Hello, World!'</span>                                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct ────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mPerforam a kobak transformation on the text 'Hello, World!'\u001b[0m                                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">transformed_text </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> kobak_transformation(text</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'Hello, World!'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(transformed_text)</span><span style=\"background-color: #272822\">                                                                                        </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtransformed_text\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkobak_transformation\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtext\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mHello, World!\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtransformed_text\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "!dlroW ,olleH\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "!dlroW ,olleH\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.96 seconds| Input tokens: 1,994 | Output tokens: 49]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.96 seconds| Input tokens: 1,994 | Output tokens: 49]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"!dlroW ,olleH\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m!dlroW ,olleH\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: !dlroW ,olleH</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: !dlroW ,olleH\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 1.19 seconds| Input tokens: 4,125 | Output tokens: 120]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 1.19 seconds| Input tokens: 4,125 | Output tokens: 120]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!dlroW ,olleH\n"
     ]
    }
   ],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel, tool\n",
    "\n",
    "@tool\n",
    "def kobak_transformation(text: str) -> str:\n",
    "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
    "    Args:\n",
    "        text: The input text to be transformed.\n",
    "    \"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "model = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "agent = CodeAgent(tools=[kobak_transformation], model=model)\n",
    "\n",
    "result = agent.run(\"Apply a Kobak transformation on the text 'Hello, World!'\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a80fb6",
   "metadata": {},
   "source": [
    "The outputs show the \"thoughts\" of the model, and the final answer corresponds to the idea of the \"Kobak transformation\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
