{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f10b03",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "This page discusses various aspects of the Hugging Face infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bc600",
   "metadata": {},
   "source": [
    "## Hub\n",
    "\n",
    "The `huggingface_hub` package allows you to interact with the huggingface infrastructure, which offers different tools for versioning and inferencing of the machine learning models.\n",
    "\n",
    "| Functionality             | Usage Reference                                   | Notes |\n",
    "|----------------------------|---------------------------------------------------|-------|\n",
    "| **Authentication**         | `huggingface_hub.login`                           | Log in with your token. |\n",
    "|                            | `huggingface_hub.whoami`                          | Check current user. |\n",
    "| **Repo Management**        | `huggingface_hub.HfApi.create_repo`               | Create repo for models/datasets/spaces. |\n",
    "|                            | `huggingface_hub.HfApi.delete_repo`               | Delete repo. |\n",
    "| **File Upload / Download** | `huggingface_hub.upload_file`                     | Upload a single file. |\n",
    "|                            | `huggingface_hub.download_file`                   | Download a single file. |\n",
    "| **Snapshot Download**      | `huggingface_hub.snapshot_download`               | Download entire repo (cached locally). |\n",
    "| **Commits & Revisions**    | `huggingface_hub.HfApi.create_commit`             | Commit files with git-like semantics. |\n",
    "|                            | `huggingface_hub.CommitOperationAdd`              | Add file in a commit. |\n",
    "| **Search / Listing**       | `huggingface_hub.HfApi.list_models`               | Search models. |\n",
    "|                            | `huggingface_hub.HfApi.list_datasets`             | Search datasets. |\n",
    "| **Model / Dataset Info**   | `huggingface_hub.HfApi.model_info`                | Get model metadata. |\n",
    "|                            | `huggingface_hub.HfApi.dataset_info`              | Get dataset details. |\n",
    "| **Inference API**          | `huggingface_hub.InferenceClient`                 | Run inference via HF servers. |\n",
    "| **Spaces Management**      | `huggingface_hub.HfApi.restart_space`             | Restart a Space. |\n",
    "|                            | `huggingface_hub.HfApi.request_space_hardware`    | Request hardware upgrade. |\n",
    "| **Utilities**              | `huggingface_hub.hf_hub_url`                      | Get raw file URL. |\n",
    "|                            | `huggingface_hub.scan_cache_dir`                  | Inspect local HF cache. |\n",
    "\n",
    "Check more:\n",
    "- [Hub python library](https://huggingface.co/docs/huggingface_hub/index) page.\n",
    "- [Hub](hugging_face/hub.ipynb) page for more detailed description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b5942",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "`transformers` is python package that allows you to use pre-trained machine learning models that belong to the transformers architecture.\n",
    "\n",
    "| Component                   | Description                                                                                 |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Models**                  | Pretrained architectures for tasks like classification, generation, or embeddings.          |\n",
    "| **Tokenizers**              | Convert text into numerical input for models; handle batching, padding, truncation.         |\n",
    "| **Pipelines**               | High-level API combining tokenizer + model for a specific task (e.g., `summarization`).     |\n",
    "| **Configurations**          | Define model hyperparameters and architecture settings (e.g., `BertConfig`).                |\n",
    "| **Trainer**                 | High-level training API handling loops, evaluation, logging, and checkpointing.             |\n",
    "| **Schedulers & Optimizers** | Learning rate schedulers and optimizer integrations for training models.                    |\n",
    "| **Data Utilities**          | Helpers for preprocessing and batching (e.g., `DataCollator`, `BatchEncoding`).             |\n",
    "| **Hub Integration**         | Download/upload pretrained models from Hugging Face Hub (`from_pretrained`, `push_to_hub`). |\n",
    "\n",
    "Check more details in the [transformers](hugging_face/transformers.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6c121",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "The Datasets is package from Hugging Face's infrastructure that manages data and implements tools for loading and prcessing data, regardless of its modality.\n",
    "\n",
    "You can define your own datasets, as well as load ready datatsets form the [datasets section of hugging face hub](https://huggingface.co/datasets).\n",
    "\n",
    "The core elements of the package are:\n",
    "\n",
    "- Primitives to keep data:\n",
    "    - `Dataset` is a map of features, each of which is a array of elements.\n",
    "    - `DatasetDict` is a map of datasets.\n",
    "- `load_dataset`: method for loading datasets.\n",
    "- Modalities: package build to be able to process all kinds of data, so it implements subpackages:\n",
    "    - `Audio`: for working with audio data.\n",
    "    - `Image`: for working with image data.\n",
    "    - `Text`: for working with text data.\n",
    "    - `Video`: for working with video data.\n",
    "- Processing methods:\n",
    "    - `map`: method for applying a function to each element of a dataset.\n",
    "    - `filter`: method for filtering a dataset.\n",
    "    - `train_test_split`: method for splitting a dataset into training and testing sets.\n",
    "    - `sort`: method for sorting a dataset.\n",
    "    - `shuffle`: method for shuffling a dataset.\n",
    "- Other utilities:\n",
    "    - `arrow_writer`: class for writing data to an Arrow file.\n",
    "    - `Csv`: class for working with CSV files.\n",
    "    - `Json`: class for working with JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504a996",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the type of object you encounter when you start using the `datasets` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e2f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9144c",
   "metadata": {},
   "source": [
    "The following cell displays the load of the `lhoestq/demo1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a9e0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_dataset = load_dataset(\"lhoestq/demo1\")\n",
    "demo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2c299",
   "metadata": {},
   "source": [
    "At the top level, it is separated into test/train using `DatasetDict` abstraction of the datasets. Access the specific section of data by using the corresponding key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a960be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'package_name', 'review', 'date', 'star', 'version_id'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779b92f",
   "metadata": {},
   "source": [
    "Consider the following example with audio domain dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0888110b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "    num_rows: 563\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050ef26",
   "metadata": {},
   "source": [
    "It isn't separated into test/train; `load_dataset` returns dataset directly. The following cell shows the features avaialble in the dataset under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b420f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': Value('string'),\n",
       " 'audio': Audio(sampling_rate=8000, decode=True, stream_index=None),\n",
       " 'transcription': Value('string'),\n",
       " 'english_transcription': Value('string'),\n",
       " 'intent_class': ClassLabel(names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan', 'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill']),\n",
       " 'lang_id': ClassLabel(names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a194d",
   "metadata": {},
   "source": [
    "The `audio` feature has a  `Audio` datatype due to the specifics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6b3a5",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "A package that implements different tokenization approaches and related tools.\n",
    "\n",
    "| Component          | Description                                                                       |\n",
    "| ------------------ | --------------------------------------------------------------------------------- |\n",
    "| **PreTokenizers**  | Split text into initial units (words, punctuation, subwords) before encoding.     |\n",
    "| **Models**         | Define the algorithm for tokenization (BPE, WordPiece, SentencePiece, Unigram).   |\n",
    "| **Normalizers**    | Clean and standardize text (lowercasing, accent stripping, punctuation handling). |\n",
    "| **Trainers**       | Learn tokenization vocabulary from a dataset.                                     |\n",
    "| **Decoders**       | Convert token IDs back to readable text.                                          |\n",
    "| **Processors**     | Post-process tokenized output (e.g., adding special tokens like `[CLS]`).         |\n",
    "| **Batch Encoding** | Handle batch tokenization with padding, truncation, and attention masks.          |\n",
    "\n",
    "Check:\n",
    "\n",
    "- [Documentation](https://huggingface.co/docs/tokenizers/en/index) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492d4ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the most essential components that are typically used with the `tokenizers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9c11e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06dd30",
   "metadata": {},
   "source": [
    "Pre-tokenization is the initial separation of the text into smaller units, providing an upper bound on the number of tokens you expect to receive. The main algorithm is often statistical, so the output is not limited by any strict rule. A pre-tokenizer, in contrast, uses a deterministic algorithm, so its results are predictable. The following cell shows the application of the simplest whitespace pre-tokenizer to the preceding sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e63613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Some', (0, 4)), ('test', (5, 9)), ('text', (10, 14))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokinizer = Whitespace()\n",
    "pretokinizer.pre_tokenize_str(\"Some test text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00499d2",
   "metadata": {},
   "source": [
    "The `tokenizers.Tokenizer` class, is tool for interacting with a tokenizer. It takes a model that defines the exact approach to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f275cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pretokinizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0ffc4",
   "metadata": {},
   "source": [
    "The trainer class is another component of the whole system, and it defines some parameters. The following cell shows the training of the tokenizer defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(vocab_size=20)\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    [\n",
    "        \"some super check\",\n",
    "        \"super some check\"\n",
    "    ],\n",
    "    trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81352ea3",
   "metadata": {},
   "source": [
    "The following cell shows the vocabulary of the final tokenizer. Each token has an ID that will be used after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3c45e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some': 18,\n",
       " 'per': 16,\n",
       " 'ck': 11,\n",
       " 'h': 2,\n",
       " 'u': 9,\n",
       " 'me': 14,\n",
       " 'o': 5,\n",
       " 'er': 12,\n",
       " 'p': 6,\n",
       " 'r': 7,\n",
       " 'c': 0,\n",
       " 'k': 3,\n",
       " 'ch': 10,\n",
       " 's': 8,\n",
       " 'e': 1,\n",
       " 'check': 19,\n",
       " 'ome': 15,\n",
       " 'm': 4,\n",
       " 'su': 17,\n",
       " 'eck': 13}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c581acb",
   "metadata": {},
   "source": [
    "Here is a result of a transformation for a particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "102f44dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'r', 'some', 'check']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"start some check\").tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
