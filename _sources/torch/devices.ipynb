{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfG_8TnXo4ec"
      },
      "source": [
        "# Devices\n",
        "\n",
        "A significant aspect of working with PyTorch involves managing the devices on which your code runs. This page focuses on that topic.\n",
        "\n",
        "**Note:** To fully grasp the concepts discussed here, you'll need a GPU that supports CUDA. Consider using Google Colab for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oBqG16cNo-HG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils import benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rbV3C94qADI"
      },
      "source": [
        "To check the status of your GPU, use the `nvidia-smi` command as shown in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kWMhqPNpB1e",
        "outputId": "7e903a00-265f-4237-a5d8-ac47629c16d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Aug  7 11:41:57 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEcsvK3QrAnV"
      },
      "source": [
        "## Selecting device\n",
        "\n",
        "By default, all objects are created on the `cpu`. However, you can use the `to` method to move them to a different device.\n",
        "\n",
        "---\n",
        "\n",
        "By default, tensors created in PyTorch are allocated to the CPU device. The following code demonstrates this by creating a tensor and printing its device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qGMF3VzqTqn",
        "outputId": "3cf71180-0e0b-4a6b-af56-f2a6f6ad60c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cpu_tensor = torch.ones([3, 3])\n",
        "cpu_tensor.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYAjGVv7xDmY"
      },
      "source": [
        "The following code switches the device to cuda and displays the device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju4F5trNq2Zo",
        "outputId": "f2e94c50-119b-4819-bf16-ef7a7b25f586"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cuda_tensor = cpu_tensor.to(\"cuda\")\n",
        "cuda_tensor.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jaM__OvBvF8"
      },
      "source": [
        "Another option is to use `cuda` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BIOr5y5B1yd",
        "outputId": "ebda4095-d3fd-421c-8f54-7e2344fa7356"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cpu_tensor.cuda().device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Device availability\n",
        "\n",
        "It's a common scenario to debug your program on one machine but use cloud utilities like Colab for computationally heavy operations. To avoid potential errors caused by forgetting to switch the device, you can check the availability of the device and use it if available.\n",
        "\n",
        "To determine if a device is available at runtime, use `torch.cuda.is_available()` for CUDA devices or `torch.backends.mps.is_available()` for Apple processors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "The following cell shows how you can check if CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The full code that defines the `device` variable based on the runtime environment is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"using device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsM7XgRzA75"
      },
      "source": [
        "### Network\n",
        "\n",
        "PyTorch networks have a `to` method that conveniently applies a specified device to all of the network's tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4KgPMtc1nEr"
      },
      "source": [
        "---\n",
        "\n",
        "In the following code cell, we create a network and then print the devices of its tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXWvw7ZOy_fZ",
        "outputId": "e871fc74-f122-469c-c6ad-421035127446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "network = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3,3),\n",
        "    torch.nn.ReLU()\n",
        ")\n",
        "\n",
        "for param in network.parameters():\n",
        "  print(param.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y70mKd931-zb"
      },
      "source": [
        "After applying the `.to('cuda')` method to the network, we obtain a new object that represents the same network but with all its tensors now residing on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG8bpLza09CI",
        "outputId": "619b4d12-85b6-4c36-8175-448a2a5d2cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "cuda_network = network.to('cuda')\n",
        "\n",
        "for param in cuda_network.parameters():\n",
        "  print(param.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXo8Juni45rn"
      },
      "source": [
        "## Benchmark\n",
        "\n",
        "Let's compare the performance of a PyTorch network running on the device (likely a GPU) and on the CPU using `torch.utils.benchmark`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LuL8rMEACFp"
      },
      "source": [
        "---\n",
        "\n",
        "This code creates a large, inefficient neural network and attempts to use it with a large matrix, resulting in slow execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q71lnH6x6dLP",
        "outputId": "7d759a24-baa8-4665-fd4c-6c44d31a2377"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.benchmark.utils.common.Measurement object at 0x7bdd1334ed40>\n",
              "network(X)\n",
              "  1.73 s\n",
              "  1 measurement, 5 runs , 1 thread"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dimentionality = 1000\n",
        "network = torch.nn.Sequential(*[\n",
        "    torch.nn.Linear(dimentionality, dimentionality) for i in range(100)\n",
        "])\n",
        "X = torch.randn([dimentionality, dimentionality])\n",
        "\n",
        "benchmark.Timer(\n",
        "    stmt=\"network(X)\",\n",
        "    globals={\"network\": network, \"X\": X}\n",
        ").timeit(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaKtcQ_iAjB9"
      },
      "source": [
        "Now, we'll benchmark the same network but this time with its computations moved to the GPU to see how much faster it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0x_Ql0J7R2s",
        "outputId": "104e27e7-bee8-418b-f629-58795f6f3eff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.benchmark.utils.common.Measurement object at 0x7bdd1334d5a0>\n",
              "network(X)\n",
              "  65.91 ms\n",
              "  1 measurement, 5 runs , 1 thread"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "network = network.to('cuda')\n",
        "X = X.to('cuda')\n",
        "\n",
        "benchmark.Timer(\n",
        "    stmt=\"network(X)\", globals={\"network\": network, \"X\": X}\n",
        ").timeit(5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP1Htkw6OBblvXYzAtt56bO",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
