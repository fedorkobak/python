{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model\n",
    "\n",
    "Saving and loading PyTorch models is crucial because any model you build needs to be transferred and deployed in some way. Check the [official tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html). Here, we'll experiment with the options from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import itertools\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State dict\n",
    "\n",
    "The classical method to save and load a model's state dictionary follows these steps:\n",
    "\n",
    "- Retrieve the model's state dictionary with `nn.Module.state_dict()`.\n",
    "- Save the state dictionary with `torch.save`.\n",
    "- Load the state dictionary with `torch.load`.\n",
    "- Load the weights into the model using `nn.Module.load_state_dict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the following cell, we created a simple model and initialized it with a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('0.bias', tensor([3., 3., 3.])),\n",
       "             ('1.weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('1.bias', tensor([3., 3., 3.]))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 3),\n",
    "    nn.Linear(3, 3)\n",
    ")\n",
    "for p in model.parameters():\n",
    "    nn.init.constant_(p, 3)\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using `torch.save`, we save the state dictionary and discard the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=model.state_dict(), f=Path(\"/tmp\")/\"my_model\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with `torch.load`, we load the state dictionaryâ€”since all values were constant during saving, they remain as 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('0.bias', tensor([3., 3., 3.])),\n",
       "             ('1.weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('1.bias', tensor([3., 3., 3.]))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_dict = torch.load(Path(\"/tmp\")/\"my_model\", weights_only=False)\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By recreating the exact same model and loading the previously saved state dictionary into it, you can fully recreate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 3),\n",
    "    nn.Linear(3, 3)\n",
    ")\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully executed `nn.load_state_dict` returns a special string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save entire model\n",
    "\n",
    "By passing the entire model to the `torch.save` function, you'll save a serialized Torch model. Then, with just one line of code, you can restore the model using the `torch.load` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates creating a model, initializing its weights with a constant, and saving this model to disk by passing it as `obj` to the `torch.save` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 3),\n",
    "    nn.Linear(3, 3)\n",
    ")\n",
    "for p in model.parameters():\n",
    "    nn.init.constant_(p, 3)\n",
    "\n",
    "torch.save(model, Path(\"/tmp\")/\"model.pht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `torch.load`, the model can be restored. The following cell shows that the loaded model has weights identical to those initialized before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('0.bias', tensor([3., 3., 3.])),\n",
       "             ('1.weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('1.bias', tensor([3., 3., 3.]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(Path(\"/tmp\")/\"model.pht\", weights_only=False).state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving group\n",
    "\n",
    "A typical case where this approach requires two `torch` objects, such as a generator and discriminator in a GAN, is straightforward. It's not a problem to save a batch of models grouped in any Python collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As an example, consider two linear layers that we want to save as a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = nn.Linear(3, 3)\n",
    "val2 = nn.Linear(3, 3)\n",
    "\n",
    "for p in itertools.chain(val1.parameters(), val2.parameters()):\n",
    "    nn.init.constant_(p, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save models as dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    obj={\"val1\": val1, \"val2\": val2},\n",
    "    f=Path(\"/tmp\")/\"model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are no problems to load that dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val1': Linear(in_features=3, out_features=3, bias=True),\n",
       " 'val2': Linear(in_features=3, out_features=3, bias=True)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = torch.load(Path(\"/tmp\")/\"model.pth\", weights_only=False)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure, let's check that the extracted model has weights just as we initialized them before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[3., 3., 3.],\n",
       "                      [3., 3., 3.],\n",
       "                      [3., 3., 3.]])),\n",
       "             ('bias', tensor([3., 3., 3.]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[\"val1\"].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same trick will work with tuples as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=3, out_features=3, bias=True),\n",
       " Linear(in_features=3, out_features=3, bias=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(obj=(val1, val2), f=Path(\"/tmp\")/\"model.pth\")\n",
    "torch.load(Path(\"/tmp\")/\"model.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different devices\n",
    "\n",
    "Saved model remembers device it was saved in. So it's typical problem to load model that used GPU as divice during saving to cpu only machine.  There are special section [\"saving loading model across devices\"](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices) in pytorch tutorial.\n",
    "\n",
    "`map_location` parameter in all model loading utilities allows to specify at which device model have to be loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here is the code that stores a simple model that runs on the GPU, using serialisation and state dict approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 3),\n",
    "    nn.Linear(3, 3)\n",
    ")\n",
    "model.to(device=torch.device('cuda'))\n",
    "\n",
    "files_path = Path(\"saving_model_fiels\")\n",
    "files_path.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save(model, f=files_path/\"entire_model.pth\")\n",
    "torch.save(model.state_dict(), f=files_path/\"state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to load something stored on the GPU on the single-CPU machine, you'll get a corresponding error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243924/253386758.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(files_path/\"entire_model.pth\")\n"
     ]
    }
   ],
   "source": [
    "files_path = Path(\"saving_model_files\")\n",
    "try:\n",
    "    torch.load(files_path/\"entire_model.pth\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the device in the `map_location` argument, everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (1): Linear(in_features=3, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\n",
    "    files_path/\"entire_model.pth\", \n",
    "    map_location=torch.device('cpu'), \n",
    "    weights_only=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.5407, -0.0230, -0.4316],\n",
       "                      [-0.0598,  0.4279, -0.1093],\n",
       "                      [-0.1346,  0.4117, -0.0023]])),\n",
       "             ('0.bias', tensor([-0.4625,  0.4542, -0.1207])),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.4032, -0.2615, -0.4629],\n",
       "                      [-0.3916,  0.3262, -0.0923],\n",
       "                      [-0.4275, -0.5173,  0.2389]])),\n",
       "             ('1.bias', tensor([ 0.2098, -0.2383, -0.2560]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\n",
    "    files_path/\"state_dict.pth\", \n",
    "    map_location=torch.device('cpu'), \n",
    "    weights_only=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
