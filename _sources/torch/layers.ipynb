{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67beb034-0408-4f3e-ae5f-b9d134adba64",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "Layer in torch is some transformation with some inputs and some outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcbd104-4e79-406c-99b9-d6e2979b2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from math import prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e4a60-103f-4cf2-a6e6-32ac1dc99c10",
   "metadata": {},
   "source": [
    "To perform a layer transformation on the tensor `X`, simply use the syntax `layer(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bd924-b58f-459c-a344-793248e069c9",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22a9c3-620a-4e09-82be-0363292487de",
   "metadata": {},
   "source": [
    "The `torch.nn.Linear` layer performs the following operation:\n",
    "\n",
    "$$X_{n \\times l} \\cdot \\left(\\omega_{k \\times l}\\right)^T + b_k$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $l$: number of inputs\n",
    "- $k$: number of outputs\n",
    "- $n$: number of input samples\n",
    "- $X_{n \\times l}$: input tensor\n",
    "- $\\omega_{k \\times l}$: weight matrix of the layer\n",
    "- $b_k$: bias vector of the layer\n",
    "\n",
    "Find out more:\n",
    "\n",
    "- [Corresponding page](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) in the official documentation.\n",
    "- [Special page](layers/linear.ipynb) on this website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf498d04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a linear layer and the appropriate input for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90385899-ed31-4bd7-8601-d5ef8f966acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 5\n",
    "out_features = 3\n",
    "\n",
    "linear = nn.Linear(\n",
    "    in_features = in_features, \n",
    "    out_features = out_features\n",
    ")\n",
    "\n",
    "X = torch.rand(10 ,in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdcb4f4",
   "metadata": {},
   "source": [
    "The following cell applies the Torch layer to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc3ceabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1886, -0.7310, -0.0978],\n",
       "        [-0.3230, -0.8067,  0.0371],\n",
       "        [-0.3204, -0.9425,  0.0208],\n",
       "        [-0.2285, -0.7381, -0.0097],\n",
       "        [-0.3597, -0.7659, -0.1116],\n",
       "        [-0.2014, -0.8315, -0.2646],\n",
       "        [-0.2403, -0.7090,  0.0276],\n",
       "        [-0.3010, -0.9064, -0.3029],\n",
       "        [-0.1405, -0.6946, -0.1152],\n",
       "        [-0.3152, -0.5994,  0.0664]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f68921",
   "metadata": {},
   "source": [
    "And the same result using algebraic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a994bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1886, -0.7310, -0.0978],\n",
       "        [-0.3230, -0.8067,  0.0371],\n",
       "        [-0.3204, -0.9425,  0.0208],\n",
       "        [-0.2285, -0.7381, -0.0097],\n",
       "        [-0.3597, -0.7659, -0.1116],\n",
       "        [-0.2014, -0.8315, -0.2646],\n",
       "        [-0.2403, -0.7090,  0.0276],\n",
       "        [-0.3010, -0.9064, -0.3029],\n",
       "        [-0.1405, -0.6946, -0.1152],\n",
       "        [-0.3152, -0.5994,  0.0664]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@linear.weight.T + linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cfaf1",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A dropout layer randomly sets some components of the input tensor to zero with a given probability $p$. During training, the remaining non-zero components are scaled by a factor of $ \\frac{1}{1-p}$ to prevent signal attenuation. Formally, if we start with a tensor $x_i$, where $i \\in \\mathbb{N}^k$ represents the indices of the $k$-dimensional tensor, the output after applying dropout is given by:\n",
    "\n",
    "$$\n",
    "x'_i = x_i \\cdot p_i \\cdot \\frac{1}{1-p},\n",
    "$$\n",
    "\n",
    "where $p_i$ is sampled from a Bernoulli distribution with parameter $p$, i.e., $p_i \\sim \\text{Bernoulli}(p)$.\n",
    "\n",
    "Find out more in the [specific page](layers/dropout.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d31ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This example demonstrates the transformation of a tensor after passing through a dropout layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49ae0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n",
      "\n",
      "Dropout result:\n",
      "tensor([[ 1.4286,  2.8571,  0.0000],\n",
      "        [ 5.7143,  0.0000,  8.5714],\n",
      "        [10.0000,  0.0000, 12.8571]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.3)\n",
    "tensor = (torch.arange(3 * 3, dtype=float) + 1).reshape((3, 3))\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print()\n",
    "print(\"Dropout result:\")\n",
    "print(dropout_layer(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1a2c2",
   "metadata": {},
   "source": [
    "## Normalization layers\n",
    "\n",
    "Set whose main purpose is to normalize input data.\n",
    "\n",
    "|  Normlization type | Layers |\n",
    "|--------------------|--------|\n",
    "|    **Batch normalization** | `torch.nn.BatchNorm1d` |\n",
    "|                   | `torch.nn.BatchNorm2d`   |\n",
    "|                   | `torch.nn.BatchNorm3d`   |\n",
    "| **Layer normalization** | `torch.nn.LayerNorm`     |\n",
    "| **Instance normalization** | `torch.nn.InstanceNorm1d`|\n",
    "|                   | `torch.nn.InstanceNorm2d`|\n",
    "|                   | `torch.nn.InstanceNorm3d`|\n",
    "\n",
    "Find out more in [specific page](layers/normalisation_layers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2665600",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As an example, consider the batch normalization layer. The input data is generated in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a77b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.,  9.],\n",
       "        [10., 11., 12., 13., 14.],\n",
       "        [15., 16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.arange(20, dtype=torch.float).reshape(4, 5)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9890b",
   "metadata": {},
   "source": [
    "This cell applies batch normalization to the tensor under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d9564e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -1.3416, -1.3416, -1.3416, -1.3416],\n",
       "        [-0.4472, -0.4472, -0.4472, -0.4472, -0.4472],\n",
       "        [ 0.4472,  0.4472,  0.4472,  0.4472,  0.4472],\n",
       "        [ 1.3416,  1.3416,  1.3416,  1.3416,  1.3416]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BatchNorm1d(num_features=5)(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a2981",
   "metadata": {},
   "source": [
    "Thus, we obtained a matrix where the input is normalized by columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72975a47",
   "metadata": {},
   "source": [
    "## Convolutional\n",
    "\n",
    "Convolutional layers are implemented in Torch using the classes `torch.nn.Conv1d`, `torch.nn.Conv2d` and ``torch.nn.Conv3d``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c416d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the example of an `nn.Conv1d` layer. Suppose we want to perform convolutions with a two-dimensional kernel on a one-channel sequence, producing a single-channel output. The following cell defines and prints the parameters of the `Conv1d` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce0f2791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 2.]]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_conv = nn.Conv1d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=2\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    show_conv.weight = nn.Parameter(\n",
    "        torch.arange(\n",
    "            prod(show_conv.weight.shape), dtype=torch.float\n",
    "        ).reshape_as(show_conv.weight) + 1\n",
    "    )\n",
    "    show_conv.bias = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "print(\"Weight\")\n",
    "display(show_conv.weight)\n",
    "print(\"Bias\")\n",
    "display(show_conv.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af6544",
   "metadata": {},
   "source": [
    "Here is an example of data that can be processed using the layer declared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33df3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.,  7.,  8.,  9.]],\n",
       "\n",
       "        [[10., 11., 12., 13., 14.]],\n",
       "\n",
       "        [[15., 16., 17., 18., 19.]],\n",
       "\n",
       "        [[20., 21., 22., 23., 24.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_count = 5\n",
    "channels_count = 1\n",
    "sequesnce_lenth = 5\n",
    "\n",
    "data = (\n",
    "    torch.arange(\n",
    "        samples_count * channels_count * sequesnce_lenth,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    .reshape([samples_count, channels_count, sequesnce_lenth])\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca605391",
   "metadata": {},
   "source": [
    "Here are 5 samples from a series of 5 elements, each with one input channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e482d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.,  5.,  8., 11.]],\n",
       "\n",
       "        [[17., 20., 23., 26.]],\n",
       "\n",
       "        [[32., 35., 38., 41.]],\n",
       "\n",
       "        [[47., 50., 53., 56.]],\n",
       "\n",
       "        [[62., 65., 68., 71.]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_conv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e196610",
   "metadata": {},
   "source": [
    "Check if the computation for some element matches our expectation:\n",
    "\n",
    "$$x'_{2,3} = x_{2,3}w_{1} + x_{2,4}w_{2} + b = 7 \\times 1 + 8 \\times 2 + 0 = 23$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_{ij}$: $j$-th element of the sequence of the $i$-th sample.\n",
    "- $x'_{ij}$: $j$-th element of the output of the $i$-th sample.\n",
    "- $w_i$: $i$-th weight of the layer under consideration.\n",
    "- $b$: bias of the layer under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca6c7",
   "metadata": {},
   "source": [
    "## Inverse convolution\n",
    "\n",
    "Inverse convolution allows upsampling of an image and passing it through learnable parameters.\n",
    "\n",
    "\n",
    "We'll describe the idea visually:\n",
    "\n",
    "Suppose we have an input tensor `input` and a kernel `k`.\n",
    "\n",
    "![input](./layers_files/inv_conv_input.png)\n",
    "\n",
    "The kernel is multiplied by each pixel of the input, and the result affects the corresponding part of the output.\n",
    "\n",
    "![output](./layers_files/inv_conv_output.png)\n",
    "\n",
    "In torch, inverse convolution is implemented with the classes `torch.nn.ConvTranspose1d`, `torch.nn.ConvTranspose2d`, and `torch.nn.ConvTranspose3d`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8986a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example reproduces the scenario shown in the picture. The next cell defines the input exactly as in the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c5d0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 4.],\n",
       "         [0., 1.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(\n",
    "    [[[2, 4],\n",
    "    [0, 1]]],\n",
    "    dtype=torch.float\n",
    ")\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8804d9",
   "metadata": {},
   "source": [
    "The next cell creates a `torch.nn.ConvTranspose2d` layer and initializes it in the same way as the kernel from the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d694eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3., 1.],\n",
       "          [1., 5.]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_traspose = torch.nn.ConvTranspose2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=2,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "kernel = next(iter(conv_traspose.parameters()))\n",
    "kernel.data = torch.tensor(\n",
    "    [[3, 1],\n",
    "    [1, 5]],\n",
    "    dtype=torch.float\n",
    ").reshape_as(kernel.data)\n",
    "next(iter(conv_traspose.parameters())).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a90ea",
   "metadata": {},
   "source": [
    "The last cell shows the result of applying the layer to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9734ee5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6., 14.,  4.],\n",
       "         [ 2., 17., 21.],\n",
       "         [ 0.,  1.,  5.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_traspose(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d70b6",
   "metadata": {},
   "source": [
    "It's totaly same as showed in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c32b77",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Pooling layers aggregate different subsets of an array according to a specified function.\n",
    "\n",
    "Pooling layers are counted below:\n",
    "\n",
    "| Agregation | Layers|\n",
    "|------------|-------|\n",
    "| **Maximum** | `torch.nn.MaxPool1d`     |\n",
    "|                   | `torch.nn.MaxPool2d`     |\n",
    "|                   | `torch.nn.MaxPool3d`     |\n",
    "| **Average** | `torch.nn.AvgPool1d`     |\n",
    "|                   | `torch.nn.AvgPool2d`     |\n",
    "|                   | `torch.nn.AvgPool3d`     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce60f00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates the application of `torch.nn.MaxPooling` on a vector. Pooling is primarily designed for convolutional networks, so it is applied along the channels, which is the outermost dimension of the input. Therefore, an extra dimension is added to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09c9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], dtype=torch.float16)\n",
      "Output\n",
      "tensor([[2., 5., 8.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "input = torch.arange(10, dtype=torch.float16)[None, :]\n",
    "print(\"Input\")\n",
    "print(input)\n",
    "\n",
    "output = torch.nn.MaxPool1d(kernel_size=3)(input)\n",
    "print(\"Output\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e0009",
   "metadata": {},
   "source": [
    "As a result, the values were computed as follows:\n",
    "\n",
    "- $w_1' = \\max(w_1, w_2, w_3) = 2$.\n",
    "- $w_2' = \\max(w_3, w_4, w_5) = 5$.\n",
    "- $w_3' = \\max(w_6, w_7, w_8) = 8$.\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w'_i$ is the $i$-th element of the output.\n",
    "- $w_i$ is the $i$-th element of the input.\n",
    "\n",
    "**Note** that the last element was skipped because it couldn't form a complete kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402c28a",
   "metadata": {},
   "source": [
    "## Recurrent layers\n",
    "\n",
    "Recent layer realized in Torch with the `torch.nn.RNN` class. Find out more in the [special page](layers/recurent_layers.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad9f30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Latest layer implemented in Torch with the `torch.nn.RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cfa4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ad9f6",
   "metadata": {},
   "source": [
    "So, such input can be processed with `input_size=3` and an arbitrary dimensionality of the hidden state - for example, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47a8ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(input_size=3, hidden_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521ab2b",
   "metadata": {},
   "source": [
    "In practice, obtaining all intermediate hidden states of the input sequence is convenient for training the model. An instance of `torch.nn.RNN` returns both all hidden states and the final state. The corresponding dimensions are printed in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7778e305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 4])\n",
      "torch.Size([1, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "output, h_n = rnn(X)\n",
    "print(output.shape)\n",
    "print(h_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b60ca2",
   "metadata": {},
   "source": [
    "Actually, the second output of the layer's forward pass is the same as the last element in the sequence of intermediate hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dd3d901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True],\n",
       "         [True, True, True, True],\n",
       "         [True, True, True, True],\n",
       "         [True, True, True, True],\n",
       "         [True, True, True, True]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1] == h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d587689",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Activation functions essentially apply a specific function to each element of the input tensor. Everything is quite simple. The only interesting aspect here is the `inplace` parameter of the layer. It determines whether the layer should modify the input tensor in place or return a transformed tensor as a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d87d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As an example, consider `ReLU` with different options for this parameter. The following cell creates the activations we'll use and the tensor to which we'll apply the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69a6630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9608,  0.7092, -0.0559],\n",
       "        [ 0.8391, -1.7846, -2.2366],\n",
       "        [ 1.2297,  0.1288,  0.1447]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_activation = torch.nn.ReLU()\n",
    "inplace_activation = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "input = torch.randn(3, 3)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874b107",
   "metadata": {},
   "source": [
    "After applying the activation with `inplace=False`, the `input` tensor still retains its original generated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b499daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9608,  0.7092, -0.0559],\n",
       "        [ 0.8391, -1.7846, -2.2366],\n",
       "        [ 1.2297,  0.1288,  0.1447]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_activation(input)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315dcfb2",
   "metadata": {},
   "source": [
    "However, `inplace_activation` modified the input tensor according to the `ReLU` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5641f015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9608, 0.7092, 0.0000],\n",
       "        [0.8391, 0.0000, 0.0000],\n",
       "        [1.2297, 0.1288, 0.1447]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inplace_activation(input)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18effc33",
   "metadata": {},
   "source": [
    "## Flatten/unflaten\n",
    "\n",
    "The `torch.nn.Flatten` layer merges some dimensions into a single dimension. Conversely, `torch.nn.Unflatten` reshapes a specified axis with the specified dimensionality. Check more in the [specific page](layers/flatten_unflatten.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8a5cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example creates array that we'll use as example. You can consider it as 3 three dimentional samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11120953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.arange(81).reshape([3,3,3,3])\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9be53c",
   "metadata": {},
   "source": [
    "Now lets apply the default `torch.nn.Flatten` to the array from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01919a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.nn.Flatten()(input)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a506c",
   "metadata": {},
   "source": [
    "It seems intuitive to get the same array but with one-dimensional observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cda26c",
   "metadata": {},
   "source": [
    "Now we can revert everything with `torch.nn.Unflatten` â€” we specify the second dimensionality to be unflattened and transform it back into 3-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc94d247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.nn.Unflatten(dim=1, unflattened_size=(3,3,3))(x)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
