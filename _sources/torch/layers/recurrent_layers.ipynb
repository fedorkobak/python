{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent layers\n",
    "\n",
    "This page considers relasilation of the recurent layers in torch. Find out more at the [specific page](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) of the torch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalent realisation\n",
    "\n",
    "On the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) page of the PyTorch documentation, you can find a function that implements a transformation equivalent to `torch.nn.RNN`. To explore different parameters of the recurrent layer, it's convenient to have a function that we can modify to gain a better understanding. This section consider how basic version of this function can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows a modification of this function that takes only parts of the transformation as parameters, making it more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    x: torch.Tensor,\n",
    "    hidden_size: int,\n",
    "    weight_ih: list[torch.Tensor],\n",
    "    bias_ih: list[torch.Tensor],\n",
    "    weight_hh: list[torch.Tensor],\n",
    "    bias_hh: list[torch.Tensor],\n",
    "    h_0 : torch.Tensor | None = None,\n",
    "    num_layers: int = 1,\n",
    "    batch_first: bool = False\n",
    "):\n",
    "    if batch_first:\n",
    "        x = x.transpose(0, 1)\n",
    "    seq_len, batch_size, _ = x.size()\n",
    "    if h_0 is None:\n",
    "        h_0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "    h_t_minus_1 = h_0\n",
    "    h_t = h_0\n",
    "    output = []\n",
    "    for t in range(seq_len):\n",
    "        for layer in range(num_layers):\n",
    "            h_t[layer] = torch.tanh(\n",
    "                x[t] @ weight_ih[layer].T\n",
    "                + bias_ih[layer]\n",
    "                + h_t_minus_1[layer] @ weight_hh[layer].T\n",
    "                + bias_hh[layer]\n",
    "            )\n",
    "        output.append(h_t[-1].clone())\n",
    "        h_t_minus_1 = h_t\n",
    "    output = torch.stack(output)\n",
    "    if batch_first:\n",
    "        output = output.transpose(0, 1)\n",
    "    return output, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code creates `RNN` layer and typical input for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 4\n",
    "batch_size = 5\n",
    "\n",
    "rnn = RNN(input_size=2, hidden_size=3)\n",
    "x = torch.randn(sequence_len, batch_size, rnn.input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use custom `forward`, you must pass weights as lists (there may be more than one layer under `torch.nn.RNN`), and all other parameters of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_out = forward(\n",
    "    x=x,\n",
    "    hidden_size=rnn.hidden_size,\n",
    "    weight_ih=[rnn.weight_ih_l0],\n",
    "    bias_ih=[rnn.bias_ih_l0],\n",
    "    weight_hh=[rnn.weight_hh_l0],\n",
    "    bias_hh=[rnn.bias_hh_l0]\n",
    ")\n",
    "\n",
    "layer_out = rnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that the results of the custom realization and `torch.nn.RNN` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(\n",
    "    actual=function_out[0],\n",
    "    expected=layer_out[0]\n",
    ")\n",
    "torch.testing.assert_close(\n",
    "    actual=function_out[1],\n",
    "    expected=layer_out[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward input\n",
    "\n",
    "Forward of the `torch.nn.RNN` takes two input arrays, one is a sequences that have to be processed, other is the initial hidden states. Other feature of the `forward` is that it can procedure both batched and unbatched input. Find out more in the [corresponding page](recurrent_layers/forward_input.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider examples with parameters defined in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 3\n",
    "sequence_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbached input assumes a sequence of vectors as `input` and `hidden_size` dimmed vector for each recurrent of the layer (in particular in this case - 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(sequence_len, input_size)\n",
    "hidden = torch.randn(1, hidden_size)\n",
    "output, hidden = rnn(input, hidden)\n",
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batched input actually the same but also added dimentinality of the samples - which is second by defalut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 5, 3]), torch.Size([1, 5, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_number = 5\n",
    "input = torch.randn(sequence_len, samples_number, input_size)\n",
    "hidden = torch.randn(1, samples_number, hidden_size)\n",
    "output, hidden = rnn(input, hidden)\n",
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch dimention\n",
    "\n",
    "By default `torch.nn.RNN` is supposed to work on tensors with dimensionality $(L, N, H_{in})$, which can be considered as **sequence of batches**. But there is `batch_first` parameter which makes `torch.nn.RNN` layer to work with dimensionality $(N, L, H_{in})$, so it can be considered as **batch of sequences** - which actually is convenied in most of the cases.\n",
    "\n",
    "Here:\n",
    "\n",
    "- $L$: lenght of the seuqnce.\n",
    "- $N$: batch size.\n",
    "- $H_{in}$: dimentionality of the element of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the difference using the tensor generated in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.empty(5, 7, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default we got last state for 7 items in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(input_size=3, hidden_size=10)\n",
    "rnn(X)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the same code, with only difference in the `batch_first=True` argument, resulted in last state for 5 items in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(input_size=3, hidden_size=10, batch_first=True)\n",
    "rnn(X)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers number\n",
    "\n",
    "By specifying the `num_layers` parameter in `torch.nn.RNN`, you can define how many times the recurrent layer will be applied to the input data. This means the output of one recurrent layer will serve as the input to the next, sequentially. Find more details on the [dedicated page](recurrent_layers/layers_number.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Each recurrent has it's own weights, the following cell shows the set of parameters of `torch.nn.RNN` that have `num_layers=3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.1217, -0.2320],\n",
      "        [-0.4093,  0.0111],\n",
      "        [ 0.0011, -0.1270]], requires_grad=True)\n",
      "================================================================================\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.4822,  0.2226,  0.2734],\n",
      "        [-0.2779, -0.3565,  0.0812],\n",
      "        [ 0.2085,  0.4712,  0.3837]], requires_grad=True)\n",
      "================================================================================\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.3848, -0.3489, -0.3642], requires_grad=True)\n",
      "================================================================================\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.1991,  0.4458,  0.1032], requires_grad=True)\n",
      "================================================================================\n",
      "weight_ih_l1 Parameter containing:\n",
      "tensor([[-0.2957, -0.3953, -0.1050],\n",
      "        [ 0.5394,  0.3613, -0.2077],\n",
      "        [ 0.0979, -0.1050, -0.3406]], requires_grad=True)\n",
      "================================================================================\n",
      "weight_hh_l1 Parameter containing:\n",
      "tensor([[-0.2834, -0.3222,  0.3811],\n",
      "        [ 0.4013,  0.3117,  0.3779],\n",
      "        [-0.1798, -0.5078,  0.2733]], requires_grad=True)\n",
      "================================================================================\n",
      "bias_ih_l1 Parameter containing:\n",
      "tensor([ 0.1544, -0.5526,  0.1558], requires_grad=True)\n",
      "================================================================================\n",
      "bias_hh_l1 Parameter containing:\n",
      "tensor([0.4194, 0.0069, 0.3011], requires_grad=True)\n",
      "================================================================================\n",
      "weight_ih_l2 Parameter containing:\n",
      "tensor([[ 0.0569, -0.1900, -0.0903],\n",
      "        [-0.2319,  0.1846, -0.0532],\n",
      "        [-0.5023, -0.0299,  0.1895]], requires_grad=True)\n",
      "================================================================================\n",
      "weight_hh_l2 Parameter containing:\n",
      "tensor([[ 0.0914, -0.4255, -0.3047],\n",
      "        [ 0.5072,  0.4777,  0.3464],\n",
      "        [-0.5395,  0.4865, -0.0562]], requires_grad=True)\n",
      "================================================================================\n",
      "bias_ih_l2 Parameter containing:\n",
      "tensor([-0.0107,  0.2063, -0.0353], requires_grad=True)\n",
      "================================================================================\n",
      "bias_hh_l2 Parameter containing:\n",
      "tensor([ 0.4801,  0.2667, -0.4451], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=2, hidden_size=3, num_layers=3)\n",
    "\n",
    "for name, parameters in rnn.named_parameters():\n",
    "    print(\"=\"*80)\n",
    "    print(name, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a postfix like `l1` after each set of parameters, indicating the layer to which those weights belong.\n",
    "\n",
    "**Note**: `weight_ih_l0` has dimensionality `input_size` $\\times$ `hidden_size`, whereas all subsequent weights, such as `weight_ih_l1` and `weight_ih_l2`, have dimensionalities of `hidden_size` $\\times$ `hidden_size`. This is because all layers except the first consider the hidden states of the previous layer as the input sequence, and those sequences have elements of size `hidden_size`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
