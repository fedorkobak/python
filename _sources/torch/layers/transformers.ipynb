{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7814c50",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "This page discusses the layers of the Torch that implements transformers architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd05ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4b228",
   "metadata": {},
   "source": [
    "## Encoder layer\n",
    "\n",
    "The encoder layer is implemented using the `torch.nn.TransformerEncoderLayer`.\n",
    "\n",
    "The crucial parameters:\n",
    "\n",
    "- `d_model`: The dimentionality of the processed sequence unit.\n",
    "- `nhead`: Number of heads that process the sequence.\n",
    "\n",
    "\n",
    "The ouput of the layer has the same dimentionality as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcfbcd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the regular encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f7cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.nn.TransformerEncoderLayer(\n",
    "    d_model=5, nhead=1, batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e91a8a",
   "metadata": {},
   "source": [
    "Here is defined the kind of input it supposes to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34a725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(32, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e89577",
   "metadata": {},
   "source": [
    "A batch of 32 data units, each of which is a sequence of 10 elements from $\\mathbb{R}^5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b5f34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbfe1f",
   "metadata": {},
   "source": [
    "## Transformer encoder\n",
    "\n",
    "A transformer encoder is an object that stacks a specified number of the transformer layers together. The output of the each layer just becomes the input of the following layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268bca49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The next code shows how to initialize the transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e6c4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = torch.nn.TransformerEncoder(\n",
    "    encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "        d_model=5, nhead=1, batch_first=True\n",
    "    ),\n",
    "    num_layers=2,\n",
    "    enable_nested_tensor=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c88b0",
   "metadata": {},
   "source": [
    "Here is an example data shape that can be processed by the `TransformerEncoder` defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94817908",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(32, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8c511",
   "metadata": {},
   "source": [
    "Here is an example of the data that came through the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48356841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder(inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc3c40",
   "metadata": {},
   "source": [
    "### Nested tensor\n",
    "\n",
    "The `TransformerEncoder` has a parameter `enable_nested_tensor`. `True` value forses torch to use special datastucture - nested tensor which is optimised to work with seqences.\n",
    "\n",
    "**Note:** Nested tensors have requirements for the data they process. If the dimentionality of the data is not even, torch automatically sets the value of `enable_nested_tensor` to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f63313",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell attempts to define a `TransformerEncoder` that uses a nested tensor with `TransformerEncoderLayer` that have an odd dimentionality for the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "415ac764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedor/.virtualenvs/knowledge/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transformer_encoder = torch.nn.TransformerEncoder(\n",
    "    encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "        d_model=5, nhead=1, batch_first=True\n",
    "    ),\n",
    "    num_layers=2,\n",
    "    enable_nested_tensor=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
