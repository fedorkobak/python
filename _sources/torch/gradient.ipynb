{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686c74bd-4d60-42b5-9f00-006801892d41",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "\n",
    "Torch has tools that allow you to calculate gradients. This page created for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6adde5-b680-4ff4-b42a-d2ce2a5f0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f9ea1-7d1d-4d0a-89c3-f5650c4aceab",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "To compute derivative in torch you need to create `torch.tensor` with the property `requires_grad = True` so that torch will look for the gradient of any function this tensor is involved in.\n",
    "\n",
    "Then we need to define a function that depends on the tensor under consideration. And call the `backward` method from it - it will compute partial derivatives for each tensor on which it depends.\n",
    "\n",
    "After previous step you will have derivative values in `grad` field of tensor under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485da64a-0281-44fc-a380-44e8ae900c05",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Let's say we have function:\n",
    "\n",
    "$$y(\\omega)=\\sum_i^n 3\\omega_i.$$\n",
    "\n",
    "And we need to find the derivative of the function on the variables $\\omega_i, i\\in\\overline{1,n}$. Let's do it by hand at first:\n",
    "\n",
    "$$\\frac{dy}{d \\omega_i} = \\sum_j^n\\frac{d3\\omega_j}{d \\omega_i} = \\sum_j^n3\\frac{d\\omega_j}{d \\omega_i}.$$\n",
    "\n",
    "And that's considering the fact that:\n",
    "\n",
    "$$\\frac{d \\omega_i}{d \\omega_j} = \\begin{cases} 0 , i\\neq j; \\\\ 1 , i=j.\\end{cases}$$\n",
    "\n",
    "We got:\n",
    "\n",
    "$$\\frac{dy}{d \\omega_i} = 3.$$\n",
    "\n",
    "The implementation of this example in Torch is listed in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626d8d98-838f-467f-aa29-94188c61fea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "w = torch.rand(n, requires_grad=True)\n",
    "y = torch.sum(w*3)\n",
    "y.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d82c1-d2e5-4c4d-af4e-37e18d41ea38",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Now we have a slightly more complicated function:\n",
    "\n",
    "$$y(\\omega, \\gamma)=\\sum_i^n \\omega_i \\gamma_i.$$\n",
    "\n",
    "So derivatives by $\\omega_i$ and $\\gamma_i, i \\in \\overline{1,n}$ accordingly:\n",
    "\n",
    "$$\\frac{dy}{d \\omega_i} = \\sum_j^n\\frac{d\\omega_j\\gamma_j}{d \\omega_i} = \\sum_j^n\\gamma_j\\frac{d\\omega_j}{d \\omega_i}=\\gamma_i;$$\n",
    "$$\\frac{dy}{d \\gamma_i} = \\sum_j^n\\frac{d\\omega_j\\gamma_j}{d \\gamma_i} = \\sum_j^n\\omega_j\\frac{d\\gamma_j}{d \\gamma_i}=\\omega_i.$$\n",
    "\n",
    "And the implementation in the Torch for this case will look like the following cell. The main purpose of this example is to show that if the derivative contains a variable, its value is substituted into the expression. So in the example:\n",
    "\n",
    "- $\\omega=(1,2,3,4)$ - so derivatives of the $\\gamma_i$ take these values;\n",
    "- likewise $\\gamma=(5,6,7,8)$ - derivatvies of the $\\omega_i$ take these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e36335-e2b3-48ff-8fc6-891bcd6ba0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omega gradient value\n",
      "tensor([5., 6., 7., 8.])\n",
      "gamma gradient value\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1, 2, 3, 4], dtype=torch.float, requires_grad=True)\n",
    "g = torch.tensor([5, 6, 7, 8], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y = w @ g\n",
    "y.backward()\n",
    "print(\"omega gradient value\")\n",
    "print(w.grad)\n",
    "print(\"gamma gradient value\")\n",
    "print(g.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fdfc86",
   "metadata": {},
   "source": [
    "## Ignore gradients\n",
    "\n",
    "Sometimes it's useful to avoid computing gradients for tensors with `requires_grad=True`. To achieve this, you can use the `torch.no_grad()` context manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91db906",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider a simple example: we'll use a tensor with `requires_grad=True`, which is defined in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "538d45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(n, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a905b",
   "metadata": {},
   "source": [
    "If we apply any transformation, the `torch.backward` methods will work correctly, and we can view the gradients as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61976eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(w)\n",
    "y.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2700cb",
   "metadata": {},
   "source": [
    "However, if we wrap the operation in a `torch.no_grad()` context, calling the `backward` method will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57139580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = torch.sum(w)\n",
    "\n",
    "try: y.backward() \n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd7bb8",
   "metadata": {},
   "source": [
    "## Non-leaf grad\n",
    "\n",
    "If a tensor is not a leaf in the PyTorch computation graph (meaning it is a result of operations involving other tensors) PyTorch will not compute gradients for it using the `backward` method.\n",
    "\n",
    "---\n",
    "\n",
    "Consider an example with leaf tensors `A` and `B`, and a non-leaf tensor `temp`, which is the result of matrix multiplication between `A` and `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "044d6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([3,2], dtype=torch.float, requires_grad=True)\n",
    "B = torch.rand([2,1], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "temp = A @ B\n",
    "temp.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab064c",
   "metadata": {},
   "source": [
    "Attempting to access the gradient of the `temp` tensor will result in an warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4e0dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9368/463209734.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  temp.grad\n"
     ]
    }
   ],
   "source": [
    "temp.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e0a38",
   "metadata": {},
   "source": [
    "However, the gradients of the leaf tensors involved in the operations will be computed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f34882f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4047, 0.6939],\n",
      "        [0.4047, 0.6939],\n",
      "        [0.4047, 0.6939]])\n",
      "tensor([[1.1646],\n",
      "        [1.4113]])\n"
     ]
    }
   ],
   "source": [
    "print(A.grad)\n",
    "print(B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880df05",
   "metadata": {},
   "source": [
    "According to the instructions provided by the `torch` warning, we need to call the `retain_grad` method on non-leaf tensors if we want to retrieve their gradients after calling `backward`. The following cell demonstrates this with our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d9b28302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = A @ B\n",
    "temp.retain_grad()\n",
    "temp.sum().backward()\n",
    "temp.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2162130-6437-4056-940e-913a130939a6",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "The torch's ability to calculate derivatives is extremely useful for gradient descent. So here is a simple example of a gradient descent implementation using torch derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff75311-4f3d-4a43-ac24-8cdabb767546",
   "metadata": {},
   "source": [
    "Sample generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf646d6-c749-4abe-b113-c08902018ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "X = (torch.rand(n_objects, n_features) - 0.5) * 5\n",
    "Y = X @ w_true + torch.randn(n_objects) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e5ad0-5fce-4f4d-afcd-14e97416c7c8",
   "metadata": {},
   "source": [
    "The implementation of the algorithm is shown in the following cell.\n",
    "\n",
    "At each iteration:\n",
    "- The predictions for the current weights are computed;\n",
    "- The **MSE** for the current prediction is calculated;\n",
    "- A gradient of **MSE** on the weights is taken with line `MSE.backward()`;\n",
    "- In the weights space, a step is made in the direction of the antigradient. We need to wrap this operation with `torch.no_grad()` so that this calculation isn't used for gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b98042a-5c36-4e02-ab20-05ae36b27a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-2\n",
    "w = torch.rand(X.shape[1], requires_grad = True)\n",
    "for i in range(5000):\n",
    "    y_pred = torch.matmul(X,w)\n",
    "    MSE = torch.mean((y_pred - Y)**2)\n",
    "    MSE.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "\n",
    "    w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf7fde-e9d7-4124-beaa-0df973413444",
   "metadata": {},
   "source": [
    "So let's compare the real coefficients and the result of the approximation - it's pretty close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5beb64e2-4539-40b1-98a1-9779c4ef7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real coefs: [-0.1252637803554535, 0.5651034116744995]\n",
      "Approximation coefs: [-0.1096344143152237, 0.5543152689933777]\n"
     ]
    }
   ],
   "source": [
    "print(\"Real coefs:\", w_true.tolist())\n",
    "print(\"Approximation coefs:\", w.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
