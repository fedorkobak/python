{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow\n",
    "\n",
    "Airflow is a tool that allows to schedule a set of processes commonly used in ETL pipelines and sometimes in ML automation. This section considers typical ways to use Airflow.\n",
    "\n",
    "We typically are typically run airflow in the Docker container to make sure that we are working in a clean environment. Use following command to run the container.\n",
    "\n",
    "Build the docker image described in the `airflow_files/dockerfile` and run it with the `standalone` command.\n",
    "\n",
    "```bash\n",
    "docker build -f packages/airflow_files/dockerfile .\n",
    "docker run -d --rm --name airflow -p 8080:8080 -v ./:/knowledge airflow standalone\n",
    "```\n",
    "\n",
    "Image that is used as an example configured in such way to create default user with login `user` and password `user` that will be used as credentials for the airflow server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration file\n",
    "\n",
    "The global configuration of the airflow is stored in the special file: `airflow.cfg`. Different installations put this file in different places (as usual). Typical locations are `/opt/airflow/airflow.cfg` and `~/airflow/airflow.cfg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Any way use following command to find the location of the `airflow.cfg` on your disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/airflow/airflow.cfg\n"
     ]
    }
   ],
   "source": [
    "!find / -name airflow.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a dag\n",
    "\n",
    "This section shows the minimal actions needed to create an airflow dag. It is based on the [Fundamental conceptse official tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html), but shows only the minimal command to add a dag and verify that it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First you need to identify the folder containing dags. This folder is specified by the `dogs_folder` parameter of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dags_folder = /opt/airflow/dags\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/airflow/airflow.cfg | grep dags_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAG must be implemented by the special DAG file. This is the file that contains `airflow.models.DAG` object. It takes a lot of settings that determine it's behavior but in general it needs only `dag_id` to be specified. The following cell defines a dag with id `tutorial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /opt/airflow/dags/tutorial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/airflow/dags/tutorial.py\n",
    "from airflow.models.dag import DAG\n",
    "\n",
    "with DAG(\"tutorial\") as dag:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command causes airflow to add DAG to its databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: sqlite:////opt/airflow/airflow.db\n",
      "Performing upgrade to the metadata database sqlite:////opt/airflow/airflow.db\n",
      "[\u001b[34m2025-03-22T14:14:21.698+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m207} INFO\u001b[0m - Context impl \u001b[1mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.700+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m210} INFO\u001b[0m - Will assume \u001b[1mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.704+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m207} INFO\u001b[0m - Context impl \u001b[1mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.704+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m210} INFO\u001b[0m - Will assume \u001b[1mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.705+0000\u001b[0m] {\u001b[34mdb.py:\u001b[0m1675} INFO\u001b[0m - Creating tables\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "Database migrating done!\n"
     ]
    }
   ],
   "source": [
    "!airflow db migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `airflow dags list` you can show DAGs that are seen by the airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdag_id  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mfileloc                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mowners\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mis_paused\u001b[0m\n",
      "=========+===============================+========+==========\n",
      "tutorial | /opt/airflow/dags/tutorial.py |        | True     \n",
      "\u001b[2;3m                                                             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result there is a dag we have added below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators & tasks\n",
    "\n",
    "**Operators** in Airflow are templates that define a common way of performing a task.  \n",
    "\n",
    "A **task** is a concrete instance of an operator.\n",
    "\n",
    "Here is the list of the common airflow operators:\n",
    "\n",
    "| Operator          | Description |\n",
    "|------------------|-------------|\n",
    "| `PythonOperator`  | Executes a Python function. |\n",
    "| `BashOperator`    | Runs a Bash command or script. |\n",
    "| `PostgresOperator` | Executes a SQL query in a PostgreSQL database. |\n",
    "| `MySqlOperator`   | Executes a SQL query in a MySQL database. |\n",
    "| `SqliteOperator`  | Executes a SQL query in a SQLite database. |\n",
    "| `MsSqlOperator`   | Executes a SQL query in an MS SQL Server database. |\n",
    "| `SnowflakeOperator` | Executes a query in Snowflake. |\n",
    "| `BigQueryOperator` | Executes a SQL query in Google BigQuery. |\n",
    "| `HiveOperator`    | Executes a HiveQL query in Apache Hive. |\n",
    "| `S3FileTransformOperator` | Transfers and transforms files in AWS S3. |\n",
    "| `EmailOperator`   | Sends an email. |\n",
    "| `HttpOperator`    | Sends an HTTP request. |\n",
    "| `DummyOperator` (Deprecated: Use `EmptyOperator`) | A placeholder operator that does nothing. |\n",
    "| `EmptyOperator`   | A lightweight no-op operator, replacing `DummyOperator`. |\n",
    "| `BranchPythonOperator` | Chooses the next task dynamically based on a Python function. |\n",
    "| `TriggerDagRunOperator` | Triggers another DAG. |\n",
    "| `KubernetesPodOperator` | Runs a task inside a Kubernetes pod. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As example consider basic tools to handle the tasks in mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines DAG with `bash` task and loads it to the airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/airflow/dags/tutorial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/airflow/dags/tutorial.py\n",
    "from airflow.models.dag import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(\"tutorial\") as dag:\n",
    "    BashOperator(\n",
    "        task_id=\"hello_world\",\n",
    "        bash_command=\"echo 'Hello world'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db migrate &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the tasks of the DAG with `airflow tasks list <name of the dug>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_world\n"
     ]
    }
   ],
   "source": [
    "!airflow tasks list tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tasks for testing purposes using `airflow tasks test <name of dag> <name of task>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-22T14:47:49.566+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from \u001b[1m/opt/airflow/dags\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.698+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2614} INFO\u001b[0m - Dependencies all met for dep_context=\u001b[1mnon-requeueable deps\u001b[22m ti=\u001b[1m<TaskInstance: tutorial.hello_world __airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__ [None]>\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.702+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2614} INFO\u001b[0m - Dependencies all met for dep_context=\u001b[1mrequeueable deps\u001b[22m ti=\u001b[1m<TaskInstance: tutorial.hello_world __airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__ [None]>\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.702+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2867} INFO\u001b[0m - Starting attempt 0 of 1\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.703+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2948} \u001b[33mWARNING\u001b[0m - \u001b[33mcannot record \u001b[1mqueued_duration\u001b[22m for task \u001b[1mhello_world\u001b[22m because previous state change time has not been saved\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.703+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2890} INFO\u001b[0m - Executing \u001b[1m<Task(BashOperator): hello_world>\u001b[22m on \u001b[1m2025-03-22 14:47:49.584873+00:00\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.022+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m3134} INFO\u001b[0m - Exporting env vars: \u001b[1mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tutorial' AIRFLOW_CTX_TASK_ID='hello_world' AIRFLOW_CTX_EXECUTION_DATE='2025-03-22T14:47:49.584873+00:00' AIRFLOW_CTX_DAG_RUN_ID='__airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__'\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.025+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m732} INFO\u001b[0m - ::endgroup::\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.040+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m78} INFO\u001b[0m - Tmp dir root location: \u001b[1m/tmp\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.041+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m88} INFO\u001b[0m - Running command: \u001b[1m['/usr/bin/bash', '-c', \"echo 'Hello world'\"]\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.048+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m99} INFO\u001b[0m - Output:\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.049+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m106} INFO\u001b[0m - \u001b[1mHello world\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.050+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m110} INFO\u001b[0m - Command exited with return code 0\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.065+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m341} INFO\u001b[0m - ::group::Post task execution logs\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.065+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m353} INFO\u001b[0m - \u001b[1m\u001b[22mMarking task as \u001b[1mSUCCESS\u001b[22m. dag_id=\u001b[1mtutorial\u001b[22m, task_id=\u001b[1mhello_world\u001b[22m, run_id=\u001b[1m__airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__\u001b[22m, execution_date=\u001b[1m20250322T144749\u001b[22m, start_date=\u001b[1m\u001b[22m, end_date=\u001b[1m20250322T144750\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow tasks test tutorial hello_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs contain a lot of logging information, but now only the line after \"Output\" is important. The next cell highlights it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-22T14:48:00.890+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m99} INFO\u001b[0m - Output:\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:48:00.891+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m106} INFO\u001b[0m - Hello world\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow tasks test tutorial hello_world | grep \"Output\" -A 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
