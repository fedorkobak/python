{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow\n",
    "\n",
    "Airflow is a tool that allows to schedule a set of processes commonly used in ETL pipelines and sometimes in ML automation. This section considers typical ways to use Airflow.\n",
    "\n",
    "We typically are typically run airflow in the Docker container to make sure that we are working in a clean environment. Use following command to run the container.\n",
    "\n",
    "Build the docker image described in the `airflow_files/dockerfile` and run it with the `standalone` command.\n",
    "\n",
    "```bash\n",
    "docker build -f packages/airflow_files/dockerfile .\n",
    "docker run -d --rm --name airflow -p 8080:8080 -v ./:/knowledge airflow standalone\n",
    "```\n",
    "\n",
    "Image that is used as an example configured in such way to create default user with login `user` and password `user` that will be used as credentials for the airflow server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration file\n",
    "\n",
    "The global configuration of the airflow is stored in the special file: `airflow.cfg`. Different installations put this file in different places (as usual). Typical locations are `/opt/airflow/airflow.cfg` and `~/airflow/airflow.cfg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Any way use following command to find the location of the `airflow.cfg` on your disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/airflow/airflow.cfg\n"
     ]
    }
   ],
   "source": [
    "!find / -name airflow.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG\n",
    "\n",
    "DAG stands for Direct Acyclic Graph. And it's a collection of tasks and determines relationships between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As a basic example, consider a procedure that adds dag to Airflow.\n",
    "\n",
    "First you need to identify the folder containing dags. This folder is specified by the `dogs_folder` parameter of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dags_folder = /opt/airflow/dags\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/airflow/airflow.cfg | grep dags_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAG must be implemented by the special DAG file. This is the file that contains `airflow.models.DAG` object. It takes a lot of settings that determine it's behavior but in general it needs only `dag_id` to be specified. The following cell defines a dag with id `tutorial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /opt/airflow/dags/tutorial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/airflow/dags/tutorial.py\n",
    "from airflow.models.dag import DAG\n",
    "\n",
    "with DAG(\"tutorial\") as dag:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command causes airflow to add DAG to its databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: sqlite:////opt/airflow/airflow.db\n",
      "Performing upgrade to the metadata database sqlite:////opt/airflow/airflow.db\n",
      "[\u001b[34m2025-03-22T14:14:21.698+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m207} INFO\u001b[0m - Context impl \u001b[1mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.700+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m210} INFO\u001b[0m - Will assume \u001b[1mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.704+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m207} INFO\u001b[0m - Context impl \u001b[1mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.704+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m210} INFO\u001b[0m - Will assume \u001b[1mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:14:21.705+0000\u001b[0m] {\u001b[34mdb.py:\u001b[0m1675} INFO\u001b[0m - Creating tables\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "Database migrating done!\n"
     ]
    }
   ],
   "source": [
    "!airflow db migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `airflow dags list` you can show DAGs that are seen by the airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdag_id  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mfileloc                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mowners\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mis_paused\u001b[0m\n",
      "=========+===============================+========+==========\n",
      "tutorial | /opt/airflow/dags/tutorial.py |        | True     \n",
      "\u001b[2;3m                                                             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result there is a dag we have added below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task flow\n",
    "\n",
    "There is an alternative way to define DAG - to use TaskFlow syntax. You need to cover the function that will arc as a DAG with `ariflow.decorators.dag` decorator and run result function.\n",
    "\n",
    "Check [Working with TaskFlow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines the DAG in the Task Flow syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/airflow/dags/tutorial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/airflow/dags/tutorial.py\n",
    "from airflow.decorators import dag\n",
    "\n",
    "@dag(\"taskflow_example\")\n",
    "def tutorial():\n",
    "    pass\n",
    "\n",
    "tutorial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell reloads the airflow database - as a result the `dag_id` corresponds to the parameter specified in the `ariflow.decorators.dag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdag_id          \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mfileloc                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mowners\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mis_paused\u001b[0m\n",
      "=================+===============================+========+==========\n",
      "taskflow_example | /opt/airflow/dags/tutorial.py |        | True     \n",
      "\u001b[2;3m                                                                     \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "airflow db migrate &> /dev/null\n",
    "airflow dags list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Task is a basic execution unit in Airflow. Task can be created from:\n",
    "\n",
    "- **Operator**: preset template for creating a task.\n",
    "- **Sensor**: special kind of operator that tiggers on some event.\n",
    "- **TaskFlow** syntax: you just have to wrap a normal python function into `airflow.decorators.task`. \n",
    "\n",
    "Here is the list of the common airflow operators:\n",
    "\n",
    "| Operator          | Description |\n",
    "|------------------|-------------|\n",
    "| `PythonOperator`  | Executes a Python function. |\n",
    "| `BashOperator`    | Runs a Bash command or script. |\n",
    "| `PostgresOperator` | Executes a SQL query in a PostgreSQL database. |\n",
    "| `MySqlOperator`   | Executes a SQL query in a MySQL database. |\n",
    "| `SqliteOperator`  | Executes a SQL query in a SQLite database. |\n",
    "| `MsSqlOperator`   | Executes a SQL query in an MS SQL Server database. |\n",
    "| `SnowflakeOperator` | Executes a query in Snowflake. |\n",
    "| `BigQueryOperator` | Executes a SQL query in Google BigQuery. |\n",
    "| `HiveOperator`    | Executes a HiveQL query in Apache Hive. |\n",
    "| `S3FileTransformOperator` | Transfers and transforms files in AWS S3. |\n",
    "| `EmailOperator`   | Sends an email. |\n",
    "| `HttpOperator`    | Sends an HTTP request. |\n",
    "| `DummyOperator` (Deprecated: Use `EmptyOperator`) | A placeholder operator that does nothing. |\n",
    "| `EmptyOperator`   | A lightweight no-op operator, replacing `DummyOperator`. |\n",
    "| `BranchPythonOperator` | Chooses the next task dynamically based on a Python function. |\n",
    "| `TriggerDagRunOperator` | Triggers another DAG. |\n",
    "| `KubernetesPodOperator` | Runs a task inside a Kubernetes pod. |\n",
    "\n",
    "Find out more:\n",
    "\n",
    "- In the official [Tasks](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html) page.\n",
    "- In the specific [page](airflow/tasks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As example consider basic tools to handle the tasks in mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines DAG with `bash` task and loads it to the airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/airflow/dags/tutorial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/airflow/dags/tutorial.py\n",
    "from airflow.models.dag import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(\"tutorial\") as dag:\n",
    "    BashOperator(\n",
    "        task_id=\"hello_world\",\n",
    "        bash_command=\"echo 'Hello world'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db migrate &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the tasks of the DAG with `airflow tasks list <name of the dug>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_world\n"
     ]
    }
   ],
   "source": [
    "!airflow tasks list tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tasks for testing purposes using `airflow tasks test <name of dag> <name of task>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-22T14:47:49.566+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from \u001b[1m/opt/airflow/dags\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.698+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2614} INFO\u001b[0m - Dependencies all met for dep_context=\u001b[1mnon-requeueable deps\u001b[22m ti=\u001b[1m<TaskInstance: tutorial.hello_world __airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__ [None]>\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.702+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2614} INFO\u001b[0m - Dependencies all met for dep_context=\u001b[1mrequeueable deps\u001b[22m ti=\u001b[1m<TaskInstance: tutorial.hello_world __airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__ [None]>\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.702+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2867} INFO\u001b[0m - Starting attempt 0 of 1\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.703+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2948} \u001b[33mWARNING\u001b[0m - \u001b[33mcannot record \u001b[1mqueued_duration\u001b[22m for task \u001b[1mhello_world\u001b[22m because previous state change time has not been saved\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:49.703+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2890} INFO\u001b[0m - Executing \u001b[1m<Task(BashOperator): hello_world>\u001b[22m on \u001b[1m2025-03-22 14:47:49.584873+00:00\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.022+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m3134} INFO\u001b[0m - Exporting env vars: \u001b[1mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tutorial' AIRFLOW_CTX_TASK_ID='hello_world' AIRFLOW_CTX_EXECUTION_DATE='2025-03-22T14:47:49.584873+00:00' AIRFLOW_CTX_DAG_RUN_ID='__airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__'\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.025+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m732} INFO\u001b[0m - ::endgroup::\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.040+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m78} INFO\u001b[0m - Tmp dir root location: \u001b[1m/tmp\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.041+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m88} INFO\u001b[0m - Running command: \u001b[1m['/usr/bin/bash', '-c', \"echo 'Hello world'\"]\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.048+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m99} INFO\u001b[0m - Output:\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.049+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m106} INFO\u001b[0m - \u001b[1mHello world\u001b[22m\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.050+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m110} INFO\u001b[0m - Command exited with return code 0\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.065+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m341} INFO\u001b[0m - ::group::Post task execution logs\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:47:50.065+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m353} INFO\u001b[0m - \u001b[1m\u001b[22mMarking task as \u001b[1mSUCCESS\u001b[22m. dag_id=\u001b[1mtutorial\u001b[22m, task_id=\u001b[1mhello_world\u001b[22m, run_id=\u001b[1m__airflow_temporary_run_2025-03-22T14:47:49.584901+00:00__\u001b[22m, execution_date=\u001b[1m20250322T144749\u001b[22m, start_date=\u001b[1m\u001b[22m, end_date=\u001b[1m20250322T144750\u001b[22m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow tasks test tutorial hello_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs contain a lot of logging information, but now only the line after \"Output\" is important. The next cell highlights it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-22T14:48:00.890+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m99} INFO\u001b[0m - Output:\u001b[0m\n",
      "[\u001b[34m2025-03-22T14:48:00.891+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m106} INFO\u001b[0m - Hello world\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow tasks test tutorial hello_world | grep \"Output\" -A 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "Airflow keeps all its data in a special database. It can be useful to be able to access this data from the formal interface. Different databases can be used, but the default and simpliest option is a `sqlite`.\n",
    "\n",
    "The following cell represents some of the popular tables that are available in the SQLite database.\n",
    "\n",
    "| Table Name       | Description                                                   |\n",
    "|------------------|---------------------------------------------------------------|\n",
    "| `dag`            | Stores metadata about DAGs.                                   |\n",
    "| `dag_run`        | Stores metadata about DAG runs (instances of DAG executions). |\n",
    "| `task_instance`  | Stores metadata about task instances (individual task executions). |\n",
    "| `log`            | Stores logs for task instances.                               |\n",
    "| `job`            | Stores metadata about jobs, such as the scheduler and backfill jobs. |\n",
    "| `xcom`           | Stores XCom (cross-communication) data for tasks.             |\n",
    "| `variable`       | Stores Airflow variables.                                     |\n",
    "| `connection`     | Stores Airflow connections.                                   |\n",
    "| `task_fail`      | Stores information about task failures.                       |\n",
    "| `sla_miss`       | Stores information about SLA misses.                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows how to access the `task_isntance` table from python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>dag_id</th>\n",
       "      <th>run_id</th>\n",
       "      <th>map_index</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>state</th>\n",
       "      <th>try_number</th>\n",
       "      <th>max_tries</th>\n",
       "      <th>...</th>\n",
       "      <th>executor</th>\n",
       "      <th>executor_config</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>rendered_map_index</th>\n",
       "      <th>external_executor_id</th>\n",
       "      <th>trigger_id</th>\n",
       "      <th>trigger_timeout</th>\n",
       "      <th>next_method</th>\n",
       "      <th>next_kwargs</th>\n",
       "      <th>task_display_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello_world</td>\n",
       "      <td>tutorial</td>\n",
       "      <td>backfill__2015-06-01T00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>2025-03-22 20:18:53.268787</td>\n",
       "      <td>2025-03-22 20:18:53.409019</td>\n",
       "      <td>0.140232</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>b'\\x80\\x05}\\x94.'</td>\n",
       "      <td>2025-03-22 20:18:53.420899</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hello_world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello_world</td>\n",
       "      <td>tutorial</td>\n",
       "      <td>backfill__2015-06-02T00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>2025-03-22 20:18:55.898364</td>\n",
       "      <td>2025-03-22 20:18:56.036250</td>\n",
       "      <td>0.137886</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>b'\\x80\\x05}\\x94.'</td>\n",
       "      <td>2025-03-22 20:18:56.047650</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hello_world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello_world</td>\n",
       "      <td>tutorial</td>\n",
       "      <td>backfill__2015-06-03T00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>2025-03-22 20:18:58.540839</td>\n",
       "      <td>2025-03-22 20:18:58.686317</td>\n",
       "      <td>0.145478</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>b'\\x80\\x05}\\x94.'</td>\n",
       "      <td>2025-03-22 20:18:58.693254</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hello_world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello_world</td>\n",
       "      <td>tutorial</td>\n",
       "      <td>backfill__2015-06-04T00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>2025-03-22 20:19:01.298076</td>\n",
       "      <td>2025-03-22 20:19:01.443630</td>\n",
       "      <td>0.145554</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>b'\\x80\\x05}\\x94.'</td>\n",
       "      <td>2025-03-22 20:19:01.452251</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hello_world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello_world</td>\n",
       "      <td>tutorial</td>\n",
       "      <td>backfill__2015-06-05T00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>2025-03-22 20:19:04.029111</td>\n",
       "      <td>2025-03-22 20:19:04.177918</td>\n",
       "      <td>0.148807</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>b'\\x80\\x05}\\x94.'</td>\n",
       "      <td>2025-03-22 20:19:04.185538</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hello_world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_id    dag_id                               run_id  map_index  \\\n",
       "0  hello_world  tutorial  backfill__2015-06-01T00:00:00+00:00         -1   \n",
       "1  hello_world  tutorial  backfill__2015-06-02T00:00:00+00:00         -1   \n",
       "2  hello_world  tutorial  backfill__2015-06-03T00:00:00+00:00         -1   \n",
       "3  hello_world  tutorial  backfill__2015-06-04T00:00:00+00:00         -1   \n",
       "4  hello_world  tutorial  backfill__2015-06-05T00:00:00+00:00         -1   \n",
       "\n",
       "                   start_date                    end_date  duration    state  \\\n",
       "0  2025-03-22 20:18:53.268787  2025-03-22 20:18:53.409019  0.140232  success   \n",
       "1  2025-03-22 20:18:55.898364  2025-03-22 20:18:56.036250  0.137886  success   \n",
       "2  2025-03-22 20:18:58.540839  2025-03-22 20:18:58.686317  0.145478  success   \n",
       "3  2025-03-22 20:19:01.298076  2025-03-22 20:19:01.443630  0.145554  success   \n",
       "4  2025-03-22 20:19:04.029111  2025-03-22 20:19:04.177918  0.148807  success   \n",
       "\n",
       "   try_number  max_tries  ... executor    executor_config  \\\n",
       "0           1          0  ...     None  b'\\x80\\x05}\\x94.'   \n",
       "1           1          0  ...     None  b'\\x80\\x05}\\x94.'   \n",
       "2           1          0  ...     None  b'\\x80\\x05}\\x94.'   \n",
       "3           1          0  ...     None  b'\\x80\\x05}\\x94.'   \n",
       "4           1          0  ...     None  b'\\x80\\x05}\\x94.'   \n",
       "\n",
       "                   updated_at rendered_map_index  external_executor_id  \\\n",
       "0  2025-03-22 20:18:53.420899               None                  None   \n",
       "1  2025-03-22 20:18:56.047650               None                  None   \n",
       "2  2025-03-22 20:18:58.693254               None                  None   \n",
       "3  2025-03-22 20:19:01.452251               None                  None   \n",
       "4  2025-03-22 20:19:04.185538               None                  None   \n",
       "\n",
       "  trigger_id  trigger_timeout next_method next_kwargs task_display_name  \n",
       "0       None             None        None        None       hello_world  \n",
       "1       None             None        None        None       hello_world  \n",
       "2       None             None        None        None       hello_world  \n",
       "3       None             None        None        None       hello_world  \n",
       "4       None             None        None        None       hello_world  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "con = sqlite3.connect(\"/opt/airflow/airflow.db\")\n",
    "pd.read_sql_query(\"SELECT * FROM task_instance LIMIT 5;\", con)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
