{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "Tasks are a basic execution unit in Airflow. This page focuses on details related to working with tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication\n",
    "\n",
    "In Airflow, setting up communication between tasks in a DAG is crucial. Airflow provides a special mechanism for this called XCom (short for 'Cross-Communication'). XCom is a system for passing small amounts of data between tasks by serializing Python objects and storing them in the metadata database. This allows tasks to share information and enables Airflow to track inter-task communication.\n",
    "\n",
    "Check more:\n",
    "\n",
    "- [XComs](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html) page.\n",
    "- [Passing arbitrary objects as argumemnts](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/taskflow.html#passing-arbitrary-objects-as-arguments) section of the TaskFlow description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider passing arguments in the modern Airflow syntax - task flow. You just have to work with tasks just like with python functions, all stuff associated related to XCom is implemented by the `task` decorator.\n",
    "\n",
    "The following cell takes the string from `echo` and saves it to the disk the `save` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/airflow/dags/tutorial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/airflow/dags/tutorial.py\n",
    "from pathlib import Path\n",
    "from airflow.decorators import dag, task\n",
    "import pendulum\n",
    "\n",
    "now = pendulum.now(\"UTC\")\n",
    "\n",
    "@dag(\"etl\")\n",
    "def etl(start_date=now, schedule=\"@daily\", catchup=False):\n",
    "    @task\n",
    "    def echo() -> str:\n",
    "        return \"message\"\n",
    "\n",
    "    @task\n",
    "    def save(temps: str) -> None:\n",
    "        Path(\"/tmp/airflow_data\").write_text(temps)\n",
    "\n",
    "    data = echo()\n",
    "    save(data) \n",
    "\n",
    "etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs dag and displays the contents of the `/tmp/airflow_data` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-23T13:00:57.099+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from /opt/airflow/dags\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:00:59.928+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from /opt/airflow/dags\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:00.220+0000\u001b[0m] {\u001b[34mexecutor_loader.py:\u001b[0m258} INFO\u001b[0m - Loaded executor: SequentialExecutor\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:00.287+0000\u001b[0m] {\u001b[34mbase_executor.py:\u001b[0m169} INFO\u001b[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl', 'echo', 'backfill__2022-01-01T00:00:00+00:00', '--depends-on-past', 'ignore', '--local', '--pool', 'default_pool', '--subdir', 'DAGS_FOLDER/tutorial.py', '--cfg-path', '/tmp/tmp99vnbjxg']\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:00.293+0000\u001b[0m] {\u001b[34msequential_executor.py:\u001b[0m84} INFO\u001b[0m - Executing command: ['airflow', 'tasks', 'run', 'etl', 'echo', 'backfill__2022-01-01T00:00:00+00:00', '--depends-on-past', 'ignore', '--local', '--pool', 'default_pool', '--subdir', 'DAGS_FOLDER/tutorial.py', '--cfg-path', '/tmp/tmp99vnbjxg']\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:02.332+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from /opt/airflow/dags/tutorial.py\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:02.564+0000\u001b[0m] {\u001b[34mtask_command.py:\u001b[0m467} INFO\u001b[0m - Running <TaskInstance: etl.echo backfill__2022-01-01T00:00:00+00:00 [queued]> on host db44a0789e04\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:03.312+0000\u001b[0m] {\u001b[34mbackfill_job_runner.py:\u001b[0m464} INFO\u001b[0m - [backfill progress] | finished run 0 of 1 | tasks waiting: 1 | succeeded: 1 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 1\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:04.373+0000\u001b[0m] {\u001b[34mbase_executor.py:\u001b[0m169} INFO\u001b[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl', 'save', 'backfill__2022-01-01T00:00:00+00:00', '--depends-on-past', 'ignore', '--local', '--pool', 'default_pool', '--subdir', 'DAGS_FOLDER/tutorial.py', '--cfg-path', '/tmp/tmpr556dona']\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:04.374+0000\u001b[0m] {\u001b[34msequential_executor.py:\u001b[0m84} INFO\u001b[0m - Executing command: ['airflow', 'tasks', 'run', 'etl', 'save', 'backfill__2022-01-01T00:00:00+00:00', '--depends-on-past', 'ignore', '--local', '--pool', 'default_pool', '--subdir', 'DAGS_FOLDER/tutorial.py', '--cfg-path', '/tmp/tmpr556dona']\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:06.432+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m588} INFO\u001b[0m - Filling up the DagBag from /opt/airflow/dags/tutorial.py\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:06.675+0000\u001b[0m] {\u001b[34mtask_command.py:\u001b[0m467} INFO\u001b[0m - Running <TaskInstance: etl.save backfill__2022-01-01T00:00:00+00:00 [queued]> on host db44a0789e04\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:07.410+0000\u001b[0m] {\u001b[34mbackfill_job_runner.py:\u001b[0m303} \u001b[33mWARNING\u001b[0m - \u001b[33mTaskInstanceKey(dag_id='etl', task_id='save', run_id='backfill__2022-01-01T00:00:00+00:00', try_number=3, map_index=-1) state success not in running=dict_values([<TaskInstance: etl.save backfill__2022-01-01T00:00:00+00:00 [queued]>])\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:07.415+0000\u001b[0m] {\u001b[34mdagrun.py:\u001b[0m854} INFO\u001b[0m - Marking run <DagRun etl @ 2022-01-01 00:00:00+00:00: backfill__2022-01-01T00:00:00+00:00, state:running, queued_at: 2025-03-23 13:00:57.326310+00:00. externally triggered: False> successful\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:07.415+0000\u001b[0m] {\u001b[34mdagrun.py:\u001b[0m905} INFO\u001b[0m - DagRun Finished: dag_id=etl, execution_date=2022-01-01 00:00:00+00:00, run_id=backfill__2022-01-01T00:00:00+00:00, run_start_date=2025-03-23 13:01:00.225884+00:00, run_end_date=2025-03-23 13:01:07.415813+00:00, run_duration=7.189929, state=success, external_trigger=False, run_type=backfill, data_interval_start=2022-01-01 00:00:00+00:00, data_interval_end=2022-01-02 00:00:00+00:00, dag_hash=None\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:07.416+0000\u001b[0m] {\u001b[34mbackfill_job_runner.py:\u001b[0m464} INFO\u001b[0m - [backfill progress] | finished run 1 of 1 | tasks waiting: 0 | succeeded: 2 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 0\u001b[0m\n",
      "[\u001b[34m2025-03-23T13:01:08.425+0000\u001b[0m] {\u001b[34mbackfill_job_runner.py:\u001b[0m1051} INFO\u001b[0m - Backfill done for DAG <DAG: etl>. Exiting.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "airflow db migrate &> /dev/null\n",
    "airflow tasks clear -s 2022-01-01 -e 2022-01-01 -y etl\n",
    "airflow dags backfill -s 2022-01-01 -e 2022-01-01 etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message"
     ]
    }
   ],
   "source": [
    "!cat /tmp/airflow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the exact string generated by the `echo` task was stored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
