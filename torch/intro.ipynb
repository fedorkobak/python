{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "PyTorch a great library that provides really convenient and flexible interfaces for building neural networks.\n",
    "\n",
    "For installation check [this page](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "Tensor is a generalisation of a matrix to the case of arbitrary dimensionality. Basic entity with wich torch operates is tensor. FInd out more in [specific page](torch/tensor.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example demonstrates how to create a specific tensor. In this tensor, the elements are denoted as $\\left[ijk\\right]$, where $i$ represents the layer index in the third dimension, $j$ denotes the row index, and $k$ indicates the column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[111, 112, 113, 114],\n",
       "         [121, 122, 123, 124],\n",
       "         [131, 132, 133, 134]],\n",
       "\n",
       "        [[211, 212, 213, 214],\n",
       "         [221, 222, 223, 224],\n",
       "         [231, 232, 233, 244]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    "    [\n",
    "        [111,112,113,114],\n",
    "        [121,122,123,124],\n",
    "        [131,132,133,134]\n",
    "    ],\n",
    "    [\n",
    "        [211,212,213,214],\n",
    "        [221,222,223,224],\n",
    "        [231,232,233,244]\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "A key feature of PyTorch that sets it apart from NumPy is its ability to automatically compute gradients for tensors involved in computations. You just need to call the `backward` method on the result of your computations. The tensors that participated in these computations will then have a `grad` attribute containing the gradients. Find out more on the [relevant page](torch/gradient.ipynb).\n",
    "\n",
    "---\n",
    "\n",
    "As example consider fuction:\n",
    "\n",
    "$$f(\\overline{X})=\\sum_i x_i^2, \\overline{X} = (x_1, x_2, x_3)$$\n",
    "\n",
    "Suppose we want to calculate the gradient of the $f$ on $x$ in point $(1,2,3)$:\n",
    "\n",
    "$$\\nabla f=(2x_1, 2x_2, 2x_3) \\Rightarrow \\nabla f(1,2,3)=(2,4,6)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same procedure with the torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3], dtype=torch.float, requires_grad=True)\n",
    "res = (X**2).sum()\n",
    "res.backward()\n",
    "X.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Torch implements common loss functions. The following table shows some of them:\n",
    "\n",
    "| Loss Function                         | Description                              |\n",
    "|--------------------------------------|------------------------------------------|\n",
    "| `torch.nn.functional.binary_cross_entropy` | Binary Cross Entropy                     |\n",
    "| `torch.nn.functional.binary_cross_entropy_with_logits` | Binary Cross Entropy with Logits        |\n",
    "| `torch.nn.functional.cross_entropy`       | Cross Entropy Loss                       |\n",
    "| `torch.nn.functional.hinge_embedding_loss` | Hinge Embedding Loss                     |\n",
    "| `torch.nn.functional.kl_div`              | Kullback-Leibler Divergence Loss         |\n",
    "| `torch.nn.functional.l1_loss`             | Mean Absolute Error Loss                |\n",
    "| `torch.nn.functional.mse_loss`            | Mean Squared Error Loss                  |\n",
    "| `torch.nn.functional.margin_ranking_loss` | Margin Ranking Loss                      |\n",
    "| `torch.nn.functional.multi_label_margin_loss` | Multi-Label Margin Loss                |\n",
    "| `torch.nn.functional.multi_label_soft_margin_loss` | Multi-Label Soft Margin Loss           |\n",
    "| `torch.nn.functional.smooth_l1_loss`      | Smooth L1 Loss                           |\n",
    "| `torch.nn.functional.triplet_margin_loss` | Triplet Margin Loss                      |\n",
    "| `torch.nn.functional.nll_loss`            | Negative Log Likelihood Loss            |\n",
    "| `torch.nn.functional.cosine_embedding_loss` | Cosine Embedding Loss                   |\n",
    "\n",
    "Find out more in the [particular page](torch/loss_functions.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The followgin cell shows applying `mse_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(\n",
    "    torch.tensor([1,2,3], dtype=torch.float),\n",
    "    torch.tensor([2,3,4], dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduction` parameter allows you to specify the type of aggregation to apply to the results of the function. The three commonly used values are `none`, `mean`, and `sum`.\n",
    "\n",
    "---\n",
    "\n",
    "The following cell demonstrates how different types of reduction are applied to the same inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduction - mean, res=1.0\n",
      "reduction - sum, res=3.0\n",
      "reduction - none, res=tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "tens1 = torch.tensor([1,2,3], dtype=torch.float)\n",
    "tens2 = torch.tensor([2,3,4], dtype=torch.float)\n",
    "\n",
    "for reduction in [\"mean\", \"sum\", \"none\"]:\n",
    "    res = F.mse_loss(tens1, tens2, reduction=reduction)\n",
    "    print(f\"reduction - {reduction}, res={res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "PyTorch provides a variety of tools for creating neural network layers. Find out more on the [relevant page](torch/layers.ipynb).\n",
    "\n",
    "**Note:** In theory, the term \"layer\" often refers to a combination of connections and activation functions. However, PyTorch has a more specific abstraction where there are dedicated layers for different functionalities. It's important to keep this in mind to avoid confusion.\n",
    "\n",
    "The following table categorizes layers and counts the layers corresponding to each category. All the objects mentioned in the table are accessible via `torch.nn`.\n",
    "\n",
    "| Category               | Layers                                                                                   | Description                                      |\n",
    "|------------------------|------------------------------------------------------------------------------------------|--------------------------------------------------|\n",
    "| **Linear Layer**      | `Linear`                                                                        | Fully connected layers for mapping inputs to outputs. |\n",
    "| **Convolutional Layers** | `Conv1d`, `Conv2d`, `Conv3d`                                | Layers for applying convolution operations on data.   |\n",
    "| **Inverse convolution** | `ConvTranspose1d`, `ConvTranspose2d`, `ConvTranspose3d`      | Layers that invert convolution. |\n",
    "| **Pooling Layers**     | `MaxPool1d`, `MaxPool2d`, `MaxPool3d`, `AvgPool1d`, `AvgPool2d`, `AvgPool3d` | Layers for downsampling data while preserving important features. |\n",
    "| **Normalization Layers** | `BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d`, `LayerNorm`, `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d` | Layers that normalize the input to improve training stability.  |\n",
    "| **Activation Functions** | `Sigmoid`, `ReLU`, `LeakyReLU`, `ELU`, `RReLU`, `SELU`, `Softmax`, `Softplus`, `Softshrink`, `Tanh`, `Hardtanh` | Non-linear functions applied to layer outputs to introduce non-linearity. |\n",
    "| **Recurrent Layers**    | `RNN`, `LSTM`, `GRU`                                        | Layers designed for processing sequential data.             |\n",
    "| **Padding Layers**      | `ZeroPad1d`, `ZeroPad2d`, `ZeroPad3d`, `ConstantPad1d`, `ConstantPad2d`, `ConstantPad3d`, `ReplicationPad1d`, `ReplicationPad2d`, `ReplicationPad3d`, `ReflectionPad1d`, `ReflectionPad2d`, `ReflectionPad3d`, `CircularPad1d`, `CircularPad2d`, `CircularPad3d` | Layers that modify the dimensions of data by adding padding.  |\n",
    "| **Other Layers**        | `Embedding`, `Dropout`, `Transformer`, `TransformerEncoder`, `TransformerDecoder`, `Flatten`, `Unflatten` | Miscellaneous layers including embedding, dropout, and transformer components. | \n",
    "\n",
    "This table now shows the names of the layers alongside their descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider typical features of such objets. As an example, let's take a linear layer without going into its peculiarities.\n",
    "\n",
    "---\n",
    "\n",
    "The following cell shows that you can apply layer to the operand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5347, -0.0643, -0.2821],\n",
       "        [ 0.2541,  0.2737, -0.2114],\n",
       "        [ 0.4634,  0.2516, -0.2575]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = torch.nn.Linear(10, 3)\n",
    "layer(torch.rand(3, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a layer as part of the computation, and it can participate in the `backward` pass to compute gradients. The following cell demonstrates how to obtain the gradient for the `weight` attribute of a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(torch.rand(3, 10)).sum().backward()\n",
    "layer.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing network\n",
    "\n",
    "Neural networks are built by composing layers. PyTorch provides powerful tools and concepts for building these compositions. This section will delve into these important concepts.\n",
    "\n",
    "The core of this is the `torch.nn.Module` class, which represents the network and handles its parameters and operations.\n",
    "\n",
    "Find out more in the [particular page](torch/managing_network.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now let's take a quick practical overview of the abilities of that class. The following cell defines the `ShowNN` as an descendant of the `torch.nn.Module`, it uses a `Linear` and `ReLU` layers inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowNN(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer1 = torch.nn.Linear(3,4)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer2(self.layer1(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell demonstrates that instances of `torch.nn.Module` are aware of the layers within them, and when used as callable objects, they apply the transformations described in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShowNN(\n",
      "  (layer1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (layer2): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.5974e-02, 5.8761e-01, 9.9243e-01, 5.0744e-01],\n",
       "        [2.9262e-01, 4.2199e-02, 3.4829e-01, 2.0401e-01],\n",
       "        [2.6856e-02, 8.9178e-01, 9.9853e-01, 7.6134e-01],\n",
       "        [1.1712e-04, 9.8912e-01, 1.0000e+00, 8.3337e-01]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = ShowNN()\n",
    "\n",
    "print(network.__str__())\n",
    "network(torch.normal(mean=0, std=10, size=[4,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "For tensors and the model you are using, you can select the device in which the tensor is to be used. Find out more in the [specific page](torch/devices.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example shows how to check the `device` for your tensor. By default it's cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.randn([5, 5]).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, most objects typically encapsulate tensors, allowing you to access their device through the tensors' devices. The following example demonstrates how to access the devices for the weights and biases of two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "example_sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,3),\n",
    "    torch.nn.Linear(3,3)\n",
    ")\n",
    "\n",
    "for param in example_sequential.parameters():\n",
    "    print(param.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "Reproducibility is fundamental when working with data, but there is inherent randomness in machine learning algorithms. To control this in PyTorch, the following tools are available:\n",
    "\n",
    "- Set seeds using `torch.manual_seed`, `torch.cuda.manual_seed`, and `torch.mps.manual_seed`.\n",
    "- Some objects, like `torch.utils.data.DataLoader`, require setting a `generator` object to control randomness.\n",
    "- Certain CUDA algorithms use non-deterministic approaches. By using `torch.use_deterministic_algorithms(True)`, you can ensure that PyTorch avoids non-deterministic operations where possible, and it will alert you when it can't.\n",
    "\n",
    "For more details, check the [official reproducibility guideline](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example generates a random tensor with the seed set to 10: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8173, -0.5556, -0.8267, -1.2970],\n",
       "        [-0.1974, -0.9643, -0.5133,  2.6278],\n",
       "        [-0.7465,  1.0051, -0.2568,  0.4765],\n",
       "        [-0.6652, -0.3627, -1.4504, -0.2496]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the same code again, the exact same tensor will be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8173, -0.5556, -0.8267, -1.2970],\n",
       "        [-0.1974, -0.9643, -0.5133,  2.6278],\n",
       "        [-0.7465,  1.0051, -0.2568,  0.4765],\n",
       "        [-0.6652, -0.3627, -1.4504, -0.2496]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data primitives\n",
    "\n",
    "Torch provides special tools for handling data: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. Learn more in the:\n",
    "\n",
    "- [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) tutorial on the official website.\n",
    "\n",
    "`Dataset` contains the data, while `DataLoader` allows you to split the data into batches.\n",
    "\n",
    "Check special [Data primitives]() page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines an example of a custom `Dataset`. This dataset contains 10 items, where each item returns a number from 0 to 9 as the `x` observation, and the square of the corresponding `x` as the `y` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = list(range(10))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = x ** 2\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of using that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_dataset = SimpleDataset()\n",
    "simple_dataset[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage here is that you can use it in combination with `DataLoader`. `DataLoader` requires a `Dataset` and acts as an iterable object that, at each iteration, returns a batch with `x` and `y` values. Following cell shows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3, 0, 5]), tensor([ 9,  0, 25])]\n",
      "[tensor([8, 2, 6]), tensor([64,  4, 36])]\n",
      "[tensor([4, 1, 9]), tensor([16,  1, 81])]\n",
      "[tensor([7]), tensor([49])]\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    simple_dataset, batch_size=3, shuffle=True\n",
    ")\n",
    "for batch in data_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get shuffled batches, but each `x` still corresponds to its square in `y`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
