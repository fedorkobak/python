{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear\n",
    "\n",
    "The `torch.nn.Linear` layer performs the following operation:\n",
    "\n",
    "$$X_{n \\times l} \\cdot \\left(\\omega_{k \\times l}\\right)^T + b_k$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $l$: number of inputs\n",
    "- $k$: number of outputs\n",
    "- $n$: number of input samples\n",
    "- $X_{n \\times l}$: input tensor\n",
    "- $\\omega_{k \\times l}$: weight matrix of the layer\n",
    "- $b_k$: bias vector of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to parameters\n",
    "\n",
    "This layer allows to get parameters using attributes `weights` and `bias`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Hereâ€™s an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(in_features=3, out_features=4)\n",
    "\n",
    "default_weights = torch.ones_like(linear_layer.weight)\n",
    "default_biases = torch.zeros_like(linear_layer.bias)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear_layer.weight.copy_(default_weights)\n",
    "    linear_layer.bias.copy_(default_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the process, we have the `weight` tensor initialized with ones and the `bias` tensor initialized with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More dimentions\n",
    "\n",
    "Unlike classical matrix multiplication, a Linear layer can operate on tensors with higher dimensionality.\n",
    "\n",
    "Suppose you have a tensor with dimensions $\\left(d_1, d_2, \\dots, d_{m-2}, d_{m-1}, d_m\\right)$. \n",
    "\n",
    "A Linear layer designed for such input will have $d_m$ input features and $k$ output features, with a weight matrix $\\omega \\in \\mathbb{R}^{k \\times d_m}$.\n",
    "\n",
    "The output will retain the shape $\\left(d_1, d_2, \\dots, d_{m-2}, d_{m-1}, k\\right)$, where each of the $\\prod_{i=1}^{m-2} d_i$ subtensors of size $d_{m-1} \\times d_m$ is independently multiplied with $\\omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For example, consider an input that takes shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6648,  0.0795, -0.3961,  0.0717],\n",
       "         [ 2.2550,  1.3696, -1.4603,  0.9347],\n",
       "         [ 0.2754, -0.6647, -0.0767,  0.2089],\n",
       "         [ 0.7514,  0.3045, -1.1518, -0.4475],\n",
       "         [-0.8777,  0.4888, -0.1978, -0.9798]],\n",
       "\n",
       "        [[-1.7346,  0.5344, -1.8987,  0.5710],\n",
       "         [ 0.5810, -0.0143,  0.7732, -0.3079],\n",
       "         [-0.6366,  0.5068, -1.8391,  1.4452],\n",
       "         [-1.1583,  0.9299,  0.6273, -1.8185],\n",
       "         [ 0.7702, -1.7367, -0.8410, -0.3621]],\n",
       "\n",
       "        [[ 0.2885, -0.1347,  0.8165, -0.4481],\n",
       "         [-0.1231,  0.8926, -0.1328,  0.8820],\n",
       "         [-0.9528,  1.1596, -0.3776, -0.5287],\n",
       "         [ 0.2178, -0.4286,  1.1390,  1.9489],\n",
       "         [-0.7107,  2.1834,  0.6254,  1.3248]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(3, 5, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenient to think of as 3 matrices, with $5 \\times 4$ dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer that can handle hundreds of such entries is created in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = Linear(\n",
    "    in_features=4, \n",
    "    out_features=2, \n",
    "    bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly applying the layer to the data yields $3$ matrices with $5 \\times 2$ dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1042, -0.1680],\n",
       "         [-0.8402, -0.3794],\n",
       "         [ 0.2441, -0.3321],\n",
       "         [-0.1327, -0.1373],\n",
       "         [-0.0276,  0.4840]],\n",
       "\n",
       "        [[ 0.1779, -0.0869],\n",
       "         [-0.1385,  0.1409],\n",
       "         [-0.0116, -0.4490],\n",
       "         [-0.2179,  1.0413],\n",
       "         [ 0.7147, -0.7942]],\n",
       "\n",
       "        [[-0.0410,  0.1841],\n",
       "         [-0.3835,  0.0896],\n",
       "         [-0.3047,  0.5820],\n",
       "         [-0.0099, -0.4006],\n",
       "         [-0.9295,  0.6689]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result could be achieved by taking the input matrices one by one and multiplying them by the weight of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1042, -0.1680],\n",
       "         [-0.8402, -0.3794],\n",
       "         [ 0.2441, -0.3321],\n",
       "         [-0.1327, -0.1373],\n",
       "         [-0.0276,  0.4840]],\n",
       "\n",
       "        [[ 0.1779, -0.0869],\n",
       "         [-0.1385,  0.1409],\n",
       "         [-0.0116, -0.4490],\n",
       "         [-0.2179,  1.0413],\n",
       "         [ 0.7147, -0.7942]],\n",
       "\n",
       "        [[-0.0410,  0.1841],\n",
       "         [-0.3835,  0.0896],\n",
       "         [-0.3047,  0.5820],\n",
       "         [-0.0099, -0.4006],\n",
       "         [-0.9295,  0.6689]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([x @ linear.weight.data.T for x in X])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
